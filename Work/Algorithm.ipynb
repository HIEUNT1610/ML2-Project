{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work on this notebook to implement the algorithm.\n",
    "Please check regularly for updates.\n",
    "\n",
    "Main tasks:\n",
    "1. Write the code for the algorithm\n",
    "2. Find better $\\alpha$ and $\\beta$ values for the model.\n",
    "3. Find stopping criteria for the model.\n",
    "4. Apply the new hyperparameters to the algorithm, and evaluate the results on a lexical similarity task, then on a sentiment analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries:\n",
    "import numpy as np\n",
    "#import gensim.models.word2vec as w2v\n",
    "#import gensim.downloader as api\n",
    "from copy import deepcopy\n",
    "import re\n",
    "import math\n",
    "import sys\n",
    "import gzip\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from lexicons_builder import build_lexicon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a read vectors function to read the vectors from the file\n",
    "def read_vectors_file(filename):\n",
    "    word_vectors = {}\n",
    "    if filename.endswith('.gz'): fileObject = gzip.open(filename, 'rt', encoding='utf-8' )\n",
    "    else: fileObject = open(filename, 'r', encoding='utf-8')    \n",
    "    \n",
    "    for line in fileObject:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            vector = [float(x) for x in parts[1:]]\n",
    "            word_vectors[word] = np.array(vector) # Convert to numpy array for later use in the code\n",
    "    return word_vectors\n",
    "\n",
    "# Read the vectors from the file as a dictionary\n",
    "vectors = read_vectors_file('vectors_datatxt_250_sg_w10_i5_c500_gensim_clean.gz')\n",
    "fr_vectors = read_vectors_file('vecs100-linear-frwiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The English vocabulary size is 125777\n",
      "The French vocabulary size is 150362\n",
      "The English vector size is 250\n",
      "The French vector size is 100\n"
     ]
    }
   ],
   "source": [
    "print(f\"The English vocabulary size is {len(vectors)}\")\n",
    "print(f\"The French vocabulary size is {len(fr_vectors)}\")\n",
    "print(f\"The English vector size is {len(vectors['dog'])}\")\n",
    "print(f\"The French vector size is {len(fr_vectors['le'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy corpus of 10 words:\n",
    "toy_corpus = ['frog', 'toad', 'berger', 'cat', 'cheetah', 'dog', 'feline', 'true', 'false', 'incorrect']\n",
    "\n",
    "# Vectorize the toy corpus:\n",
    "vec_toy_corpus = [vectors[word] for word in toy_corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the way when I used the gensim model. It's just too large or my laptop to run.\n",
    "# Load the word2vec model:\n",
    "#wv = api.load('word2vec-google-news-300')\n",
    "#wv = w2v.Word2Vec(vector_size=250, min_count=500, window=8, sample=1e-3, workers=8, sg=1, hs=0, negative=10, epochs=5)\n",
    "\n",
    "# Vectorize the toy corpus:\n",
    "#vec_toy_corpus = [wv.wv[word] for word in toy_corpus]\n",
    "#vec_toy_corpus = [wv[word] for word in toy_corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between cat and feline is: 0.5670\n"
     ]
    }
   ],
   "source": [
    "# Compare vectors using cosine similarity:\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Compute the cosine similarity matrix:\n",
    "similarity_matrix = np.zeros((len(vec_toy_corpus), len(vec_toy_corpus)))\n",
    "for i in range(len(vec_toy_corpus)):\n",
    "    for j in range(len(vec_toy_corpus)):\n",
    "        similarity_matrix[i][j] = cosine_similarity(vec_toy_corpus[i], vec_toy_corpus[j])\n",
    "print(f\"The similarity between cat and feline is: {similarity_matrix[3][6]:.4f}\")\n",
    "# Print the cosine similarity matrix:\n",
    "#for i in range(len(vec_toy_corpus)):\n",
    "#    for j in range(len(vec_toy_corpus)):\n",
    "#        print(f'Similarity between {toy_corpus[i]} and {toy_corpus[j]} is {similarity_matrix[i][j]:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implementing the algorithm and finding the suitable lexicons for English and French datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We implement the retrofitting algorithm of Faruqui et al. (2015):\n",
    "# Preprocessing the data:\n",
    "isNumber = re.compile(r'\\d+.*')\n",
    "def norm_word(word):\n",
    "  word = word.replace('[', '').replace(']', '').replace(\"'\", '').replace('\"', '').replace(':', '').replace(',', '').replace('.', '')\n",
    "  if isNumber.search(word.lower()):\n",
    "    return '---num---'\n",
    "  elif re.sub(r'\\W+', '', word) == '':\n",
    "    return '---punc---'\n",
    "  else:\n",
    "    return word.lower()\n",
    "\n",
    "# Read all the word vectors and normalize them:\n",
    "def read_word_vecs(filename):\n",
    "  wordVectors = {}\n",
    "  if filename.endswith('.gz'): fileObject = gzip.open(filename, 'r')\n",
    "  else: fileObject = open(filename, 'r')\n",
    "  \n",
    "  for line in fileObject:\n",
    "    line = line.strip().lower()\n",
    "    word = line.split()[0]\n",
    "    wordVectors[word] = np.zeros(len(line.split())-1, dtype=float)\n",
    "    for index, vecVal in enumerate(line.split()[1:]):\n",
    "      wordVectors[word][index] = float(vecVal)\n",
    "    ''' normalize weight vector '''\n",
    "    wordVectors[word] /= math.sqrt((wordVectors[word]**2).sum() + 1e-6)\n",
    "    \n",
    "  sys.stderr.write(\"Vectors read from: \"+filename+\" \\n\")\n",
    "  return wordVectors\n",
    "\n",
    "# Read lexicon as a dictionary:\n",
    "def read_lexicon(filename, encoding='utf-8'):\n",
    "  lexicon = {}\n",
    "  if filename.endswith('.gz'): fileObject = gzip.open(filename, 'r')\n",
    "  else: fileObject = open(filename, 'r', encoding=encoding)\n",
    "  \n",
    "  for line in fileObject:\n",
    "    words = line.lower().strip().split()\n",
    "    lexicon[norm_word(words[0])] = [norm_word(word) for word in words[1:]]\n",
    "  return lexicon\n",
    "\n",
    "# Retrofit word vectors to a lexicon:\n",
    "def retrofit(wordVecs, lexicon, numIters, alpha = 1, beta = 1): # wordVecs is a dictionary\n",
    "  newWordVecs = deepcopy(wordVecs)\n",
    "  wvVocab = set(newWordVecs.keys())\n",
    "  loopVocab = wvVocab.intersection(set(lexicon.keys()))\n",
    "  for it in range(numIters):\n",
    "    # loop through every node also in ontology (else just use data estimate)\n",
    "    for word in loopVocab:\n",
    "      wordNeighbours = set(lexicon[word]).intersection(wvVocab)\n",
    "      numNeighbours = len(wordNeighbours)\n",
    "      #no neighbours, pass - use data estimate\n",
    "      if numNeighbours == 0:\n",
    "        continue\n",
    "      # the weight of the old vector is alpha\n",
    "      newVec = alpha * wordVecs[word]\n",
    "      # loop over neighbours and add to new vector (currently with weight beta)\n",
    "      for ppWord in wordNeighbours:\n",
    "        newVec += beta * newWordVecs[ppWord] \n",
    "      newWordVecs[word] = newVec/(alpha + beta*numNeighbours) # 1/numNeighbours * numNeighbours = 1\n",
    "      # define a stopping mechanism using Euclidean distance\n",
    "      # if distance too small, use the original vector\n",
    "      if np.linalg.norm(newWordVecs[word] - wordVecs[word]) < 0.01:\n",
    "        newWordVecs[word] = wordVecs[word]\n",
    "  return newWordVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['frog', 'toad', 'berger', 'cat', 'cheetah', 'dog', 'feline', 'true', 'false', 'incorrect'])\n"
     ]
    }
   ],
   "source": [
    "# Transform the toy corpus into a wordVecs dictionary:\n",
    "toy_wordVecs = {}\n",
    "for i in range(len(vec_toy_corpus)):\n",
    "    toy_wordVecs[toy_corpus[i]] = vec_toy_corpus[i]\n",
    "print(toy_wordVecs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport xml.etree.ElementTree as ET\\n\\n# specify the path to the WOLF file\\nwolf_path = \\'wolf-1.0b4.xml\\'\\n\\n# parse the WOLF file\\ntree = ET.parse(wolf_path)\\nroot = tree.getroot()\\n\\nword_relations = {}\\n\\nsynsets = root.findall(\\'SYNSET\\')\\nfor synset in synsets:\\n    synset_id = synset.find(\\'ID\\').text\\n    synonym_element = synset.find(\\'SYNONYM\\')\\n    for literal in synonym_element.findall(\\'LITERAL\\'):\\n        word = literal.text\\n        if word != \\'_EMPTY_\\':\\n            if word not in word_relations:\\n                word_relations[word] = []\\n            for ilr in synset.findall(\\'ILR\\'):\\n                if ilr.get(\\'type\\') in [\\'hypernym\\', \\'hyponym\\']:\\n                    target_id = ilr.text\\n                    target_synset = root.find(f\"SYNSET[ID=\\'{target_id}\\']\")\\n                    target_synonym_element = target_synset.find(\\'SYNONYM\\')\\n                    for target_literal in target_synonym_element.findall(\\'LITERAL\\'):\\n                        word_relations[word].append(target_literal.text) if target_literal.text != \\'_EMPTY_\\' else None\\n\\n    \\ndef export_dict_to_file(dictionary, filename):\\n    with open(filename, \\'w\\', encoding = \\'utf-8\\') as f:\\n        for key, value in dictionary.items():\\n            f.write(f\"{key}: {value}\\n\")\\n\\nexport_dict_to_file(word_relations, \\'word_relations_lemma.txt\\')'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a French lexicon dictionary and add all hypernyms, hyponyms, meronyms, and holonyms\n",
    "# It took a lot of time so I saved the dictionary in a file.\n",
    "\n",
    "\"\"\"\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# specify the path to the WOLF file\n",
    "wolf_path = 'wolf-1.0b4.xml'\n",
    "\n",
    "# parse the WOLF file\n",
    "tree = ET.parse(wolf_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "word_relations = {}\n",
    "\n",
    "synsets = root.findall('SYNSET')\n",
    "for synset in synsets:\n",
    "    synset_id = synset.find('ID').text\n",
    "    synonym_element = synset.find('SYNONYM')\n",
    "    for literal in synonym_element.findall('LITERAL'):\n",
    "        word = literal.text\n",
    "        if word != '_EMPTY_':\n",
    "            if word not in word_relations:\n",
    "                word_relations[word] = []\n",
    "            for ilr in synset.findall('ILR'):\n",
    "                if ilr.get('type') in ['hypernym', 'hyponym']:\n",
    "                    target_id = ilr.text\n",
    "                    target_synset = root.find(f\"SYNSET[ID='{target_id}']\")\n",
    "                    target_synonym_element = target_synset.find('SYNONYM')\n",
    "                    for target_literal in target_synonym_element.findall('LITERAL'):\n",
    "                        word_relations[word].append(target_literal.text) if target_literal.text != '_EMPTY_' else None\n",
    "\n",
    "    \n",
    "def export_dict_to_file(dictionary, filename):\n",
    "    with open(filename, 'w', encoding = 'utf-8') as f:\n",
    "        for key, value in dictionary.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "export_dict_to_file(word_relations, 'word_relations_lemma.txt')\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the English lexicons from files:\n",
    "frame_lexicon = read_lexicon('framenet.txt')\n",
    "wn_lexicon = read_lexicon('wordnet-synonyms+.txt')\n",
    "ppdb_lexicon = read_lexicon('ppdb-xl.txt')\n",
    "\n",
    "# Read the French lexicon from file:\n",
    "fr_lexicon = read_lexicon('word_relations_lemma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shepherd', 'accompany', 'pursuer', 'lead', 'show', 'track', 'stalk', 'guided', 'walk', 'trail', 'tail', 'usher', 'pursuit', 'escort', 'conduct', 'follow', 'shadow', 'hound', 'guide', 'pursue', 'chase']\n",
      "['chase', 'chase_after', 'trail', 'tail', 'tag', 'give_chase', 'go_after', 'track', 'domestic_dog', 'canis_familiaris', 'andiron', 'firedog', 'dog-iron', 'pawl', 'detent', 'click', 'frank', 'frankfurter', 'hotdog', 'hot_dog', 'wiener', 'wienerwurst', 'weenie', 'cad', 'bounder', 'blackguard', 'hound', 'heel', 'frump', 'pursue', 'follow', 'domestic_animal', 'domesticated_animal', 'canine', 'canid', 'support', 'catch', 'stop', 'sausage', 'villain', 'scoundrel', 'chap', 'fellow', 'feller', 'fella', 'lad', 'gent', 'blighter', 'cuss', 'bloke', 'unpleasant_woman', 'disagreeable_woman', 'tree', 'run_down', 'quest', 'hound', 'hunt', 'trace', 'puppy', 'great_pyrenees', 'basenji', 'newfoundland', 'newfoundland_dog', 'lapdog', 'poodle', 'poodle_dog', 'leonberg', 'toy_dog', 'toy', 'spitz', 'pooch', 'doggie', 'doggy', 'barker', 'bow-wow', 'cur', 'mongrel', 'mutt', 'mexican_hairless', 'hunting_dog', 'working_dog', 'dalmatian', 'coach_dog', 'carriage_dog', 'pug', 'pug-dog', 'corgi', 'welsh_corgi', 'griffon', 'brussels_griffon', 'belgian_griffon', 'vienna_sausage', 'perisher']\n",
      "['dogs', 'chien', 'doggy', 'puppy', 'doggie', 'canine', 'hound', 'mutt', 'bitch', 'boy', 'cabot', 'lapdog', 'bloodhound', 'cane', 'psa', 'pup', 'pooch']\n",
      "['de', 'prairie', 'rodentia', 'rongeur']\n"
     ]
    }
   ],
   "source": [
    "# Test the lexicons:\n",
    "print(frame_lexicon['dog'])\n",
    "print(wn_lexicon['dog'])\n",
    "print(ppdb_lexicon['dog'])\n",
    "\n",
    "# Test the French lexicon:\n",
    "print(fr_lexicon['chien'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The frame similarity between cat and feline is: 0.5670\n",
      "The wn similarity between cat and feline is: 0.9337\n",
      "The ppdb similarity between cat and feline is: 0.5670\n",
      "Similarity is changed by 0.0000 between cat and feline\n",
      "Similarity is changed by 0.3668 between cat and feline\n",
      "Similarity is changed by 0.0000 between cat and feline\n",
      "There has been some improvements in the toy corpus, so the retrofit algorithm works! We can move on to find the better hyperparameters.\n",
      "It seems that wn_lexicon is the better lexicon, so we will use it for the rest of the assignment.\n"
     ]
    }
   ],
   "source": [
    "# Retrofit the word vectors using basic hyperparameters:\n",
    "frame_newWordVecs = retrofit(toy_wordVecs, frame_lexicon, numIters = 10, alpha = 1, beta = 2)\n",
    "wn_newWordVecs = retrofit(toy_wordVecs, wn_lexicon, numIters = 10, alpha = 1, beta = 2)\n",
    "ppdb_newWordVecs = retrofit(toy_wordVecs, ppdb_lexicon, numIters = 10, alpha = 1, beta = 2)\n",
    "\n",
    "# Similarity matrix after retrofitting:\n",
    "frame_retro_similarity_matrix = np.zeros((len(vec_toy_corpus), len(vec_toy_corpus)))\n",
    "wn_retro_similarity_matrix = np.zeros((len(vec_toy_corpus), len(vec_toy_corpus)))\n",
    "ppdb_retro_similarity_matrix = np.zeros((len(vec_toy_corpus), len(vec_toy_corpus)))\n",
    "\n",
    "for i in range(len(vec_toy_corpus)):\n",
    "    for j in range(len(vec_toy_corpus)):\n",
    "        frame_retro_similarity_matrix[i][j] = cosine_similarity(frame_newWordVecs[toy_corpus[i]], frame_newWordVecs[toy_corpus[j]])\n",
    "\n",
    "for i in range(len(vec_toy_corpus)):\n",
    "    for j in range(len(vec_toy_corpus)):\n",
    "        wn_retro_similarity_matrix[i][j] = cosine_similarity(wn_newWordVecs[toy_corpus[i]], wn_newWordVecs[toy_corpus[j]])\n",
    "        \n",
    "for i in range(len(vec_toy_corpus)):\n",
    "    for j in range(len(vec_toy_corpus)):\n",
    "        ppdb_retro_similarity_matrix[i][j] = cosine_similarity(ppdb_newWordVecs[toy_corpus[i]], ppdb_newWordVecs[toy_corpus[j]])        \n",
    "# Print the cosine similarity matrix:\n",
    "frame_retrofit_effect_matrix = frame_retro_similarity_matrix - similarity_matrix\n",
    "wn_retrofit_effect_matrix = wn_retro_similarity_matrix - similarity_matrix\n",
    "ppdb_retrofit_effect_matrix = ppdb_retro_similarity_matrix - similarity_matrix\n",
    "\n",
    "print(f\"The frame similarity between cat and feline is: {frame_retro_similarity_matrix[3][6]:.4f}\")\n",
    "print(f\"The wn similarity between cat and feline is: {wn_retro_similarity_matrix[3][6]:.4f}\")\n",
    "print(f\"The ppdb similarity between cat and feline is: {ppdb_retro_similarity_matrix[3][6]:.4f}\")\n",
    "\n",
    "print(f'Similarity is changed by {frame_retrofit_effect_matrix[3][6]:.4f} between {toy_corpus[3]} and {toy_corpus[6]}')\n",
    "print(f'Similarity is changed by {wn_retrofit_effect_matrix[3][6]:.4f} between {toy_corpus[3]} and {toy_corpus[6]}')\n",
    "print(f'Similarity is changed by {ppdb_retrofit_effect_matrix[3][6]:.4f} between {toy_corpus[3]} and {toy_corpus[6]}')\n",
    "\n",
    "print('There has been some improvements in the toy corpus, so the retrofit algorithm works! We can move on to find the better hyperparameters.')\n",
    "print(\"It seems that wn_lexicon is the better lexicon, so we will use it for the rest of the assignment.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implement the algorithm in matrix form."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to implement the retrofitting algorithm in the matrix form, we will assume that the input is the whole word embeddings matrix E(w), and the output is the whole word embeddings matrix E'(w). The retrofitting algorithm will be applied to the whole word embeddings matrix E(w) to get the whole word embeddings matrix E'(w).\n",
    "\n",
    "To do this, we will have to change the way the lexicon works. At the moment, it is a dictionary with words as keys and their neighbors as values, but to have better performance on a larger vocabulary, it should have indices instead of words. This way, we can use the indices to access the word embeddings matrix E(w) and E'(w)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word ids and indices:\n",
    "def build_word_index_mappings(word_vectors):\n",
    "    word_to_index = {}\n",
    "    index_to_word = []\n",
    "    for i, word in enumerate(word_vectors.keys()):\n",
    "        word_to_index[word] = i\n",
    "        index_to_word.append(word)\n",
    "    return word_to_index, index_to_word\n",
    "\n",
    "word_ids, word_indices = build_word_index_mappings(vectors)\n",
    "fr_word_ids, fr_word_indices = build_word_index_mappings(fr_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2122\n",
      "['chase', 'chase_after', 'trail', 'tail', 'tag', 'give_chase', 'go_after', 'track', 'domestic_dog', 'canis_familiaris', 'andiron', 'firedog', 'dog-iron', 'pawl', 'detent', 'click', 'frank', 'frankfurter', 'hotdog', 'hot_dog', 'wiener', 'wienerwurst', 'weenie', 'cad', 'bounder', 'blackguard', 'hound', 'heel', 'frump', 'pursue', 'follow', 'domestic_animal', 'domesticated_animal', 'canine', 'canid', 'support', 'catch', 'stop', 'sausage', 'villain', 'scoundrel', 'chap', 'fellow', 'feller', 'fella', 'lad', 'gent', 'blighter', 'cuss', 'bloke', 'unpleasant_woman', 'disagreeable_woman', 'tree', 'run_down', 'quest', 'hound', 'hunt', 'trace', 'puppy', 'great_pyrenees', 'basenji', 'newfoundland', 'newfoundland_dog', 'lapdog', 'poodle', 'poodle_dog', 'leonberg', 'toy_dog', 'toy', 'spitz', 'pooch', 'doggie', 'doggy', 'barker', 'bow-wow', 'cur', 'mongrel', 'mutt', 'mexican_hairless', 'hunting_dog', 'working_dog', 'dalmatian', 'coach_dog', 'carriage_dog', 'pug', 'pug-dog', 'corgi', 'welsh_corgi', 'griffon', 'brussels_griffon', 'belgian_griffon', 'vienna_sausage', 'perisher']\n",
      "93\n",
      "3137\n",
      "['de', 'prairie', 'rodentia', 'rongeur']\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the lexicon:\n",
    "print(word_ids['dog'])\n",
    "print(wn_lexicon['dog'])\n",
    "print(len(wn_lexicon['dog']))\n",
    "print(fr_word_ids['chien'])\n",
    "print(fr_lexicon['chien'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dictionary to store the lexicon with word IDs\n",
    "# This dictionary will have as keys the word IDs and as values the list of IDs of the words in the lexicon that are related to the key. The words not present in the vocabulary are ignored.\n",
    "# This is done to ensure that the IDs of the words in the lexicon are the same as the IDs of the words in the vocabulary.\n",
    "wn_lexicon_with_ids = {}\n",
    "\n",
    "# We iterate through the keys of the lexicon, and replace by the word IDs. Skip if not in the vocabulary.\n",
    "for word in wn_lexicon.keys():\n",
    "    if word not in word_ids:\n",
    "        continue\n",
    "    else:\n",
    "        wn_lexicon_with_ids[word_ids[word]] = [word_ids[w] for w in wn_lexicon[word] if w in word_ids.keys()]\n",
    "        \n",
    "# Do the same for french lexicon:\n",
    "fr_lexicon_with_ids = {}\n",
    "\n",
    "for word in fr_lexicon.keys():\n",
    "    if word not in fr_word_ids:\n",
    "        continue\n",
    "    else:\n",
    "        fr_lexicon_with_ids[fr_word_ids[word]] = [fr_word_ids[w] for w in fr_lexicon[word] if w in fr_word_ids.keys()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of neighbors for 'dog' is 53 after removing words not in the vocabulary.\n",
      "The number of neighbors for 'chien' is 3 after removing words not in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of neighbors for 'dog' is {len(wn_lexicon_with_ids[word_ids['dog']])} after removing words not in the vocabulary.\")\n",
    "print(f\"The number of neighbors for 'chien' is {len(fr_lexicon_with_ids[fr_word_ids['chien']])} after removing words not in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will not use the antonyms in this assignment, because it seems to lower the performance of the model in our implementation. But it is an option to improve the similarity.\n"
     ]
    }
   ],
   "source": [
    "# Create a new dictionary to store antonyms from WordNet:\n",
    "antonyms = {}\n",
    "\n",
    "# Get the antonyms from WordNet and store them in the dictionary\n",
    "for synset in wn.all_synsets():\n",
    "    for lemma in synset.lemmas():\n",
    "        for antonym in lemma.antonyms():\n",
    "            word = lemma.name()\n",
    "            antonym_word = antonym.name()\n",
    "            if word in word_ids and antonym_word in word_ids:\n",
    "                antonyms[word_ids[word]] = word_ids[antonym_word]\n",
    "print(\"We will not use the antonyms in this assignment, because it seems to lower the performance of the model in our implementation. But it is an option to improve the similarity.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3976, 3004, 4885, 2323, 628, 119837, 4064, 1675, 36730, 114003, 20248, 105882, 24027, 19729, 14793, 4721, 1550, 22357, 265, 3559, 959, 18002, 9909, 69569, 25414, 1666, 46171, 42210, 17849, 29517, 105981, 25817, 1947, 6109, 19729, 3446, 7013, 17798, 7860, 43170, 6316, 38975, 52235, 67663, 60179, 9590, 34751, 72560, 36218, 38783, 50015, 60126, 47213]\n",
      "wrong\n",
      "below\n"
     ]
    }
   ],
   "source": [
    "# Test new lexicons:\n",
    "print(wn_lexicon_with_ids.get(word_ids['dog'])) # dog\n",
    "print(word_indices[antonyms.get(word_ids['right'])])\n",
    "print(word_indices[antonyms.get(word_ids['above'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embedding matrix:\n",
    "embedding_matrix = np.zeros((len(word_indices), len(vectors['the'])))\n",
    "fr_embedding_matrix = np.zeros((len(fr_word_indices), len(fr_vectors['le'])))\n",
    "\n",
    "# Fill the embedding matrix with the word vectors from the word2vec model\n",
    "for i in range(len(word_indices)):\n",
    "    embedding_matrix[i] = vectors[word_indices[i]]   \n",
    "    \n",
    "for i in range(len(fr_word_indices)):\n",
    "    fr_embedding_matrix[i] = fr_vectors[fr_word_indices[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neighbor matrix from the new lexicon:\n",
    "# The value for each word is the sum of the embeddings of the words in the lexicon that are related to the word\n",
    "neighbor_matrix = np.zeros((len(word_indices), len(vectors['the']))) # It's a matrix of size (vocab_size, 250)\n",
    "fr_neighbor_matrix = np.zeros((len(fr_word_indices), len(fr_vectors['le']))) # It's a matrix of size (vocab_size, 100)\n",
    "\n",
    "# Fill the neighbor matrix:\n",
    "# For each word in the lexicon, we sum the embeddings of the words in the lexicon that are related to the word\n",
    "for key in wn_lexicon_with_ids.keys():\n",
    "    neighbor_matrix[key] = np.sum(embedding_matrix[wn_lexicon_with_ids[key]], axis = 0)\n",
    "    \n",
    "for key in fr_lexicon_with_ids.keys():\n",
    "    fr_neighbor_matrix[key] = np.sum(fr_embedding_matrix[fr_lexicon_with_ids[key]], axis = 0)\n",
    "\n",
    "# Create an array of number of neighbors for each word:\n",
    "num_neighbors = np.zeros(len(word_indices))\n",
    "for key in wn_lexicon_with_ids.keys():\n",
    "    num_neighbors[key] = len(wn_lexicon_with_ids[key])\n",
    "num_neighbors[num_neighbors == 0] = 1e-6 # To avoid division by 0\n",
    "\n",
    "fr_num_neighbors = np.zeros(len(fr_word_indices))\n",
    "for key in fr_lexicon_with_ids.keys():\n",
    "    fr_num_neighbors[key] = len(fr_lexicon_with_ids[key])\n",
    "fr_num_neighbors[fr_num_neighbors == 0] = 1e-6 # To avoid division by 0\n",
    "\n",
    "# Divide each row of the neighbor matrix by the number of neighbors for the corresponding word\n",
    "neighbor_matrix = neighbor_matrix / num_neighbors[:, None]\n",
    "fr_neighbor_matrix = fr_neighbor_matrix / fr_num_neighbors[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3976, 3004, 4885, 2323, 628, 119837, 4064, 1675, 36730, 114003, 20248, 105882, 24027, 19729, 14793, 4721, 1550, 22357, 265, 3559, 959, 18002, 9909, 69569, 25414, 1666, 46171, 42210, 17849, 29517, 105981, 25817, 1947, 6109, 19729, 3446, 7013, 17798, 7860, 43170, 6316, 38975, 52235, 67663, 60179, 9590, 34751, 72560, 36218, 38783, 50015, 60126, 47213]\n",
      "(53, 250)\n",
      "(250,)\n",
      "53.0\n",
      "\n",
      "\n",
      "[3, 15233, 40254]\n",
      "(3, 100)\n",
      "(100,)\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "# Test the neighbor matrix:\n",
    "print(wn_lexicon_with_ids[word_ids['dog']]) \n",
    "print(embedding_matrix[wn_lexicon_with_ids[word_ids['dog']]].shape) \n",
    "print(neighbor_matrix[word_ids['dog']].shape)\n",
    "print(num_neighbors[word_ids['dog']]) \n",
    "print('\\n')\n",
    "print(fr_lexicon_with_ids[fr_word_ids['chien']]) \n",
    "print(fr_embedding_matrix[fr_lexicon_with_ids[fr_word_ids['chien']]].shape)\n",
    "print(fr_neighbor_matrix[fr_word_ids['chien']].shape)\n",
    "print(fr_num_neighbors[fr_word_ids['chien']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an antonym matrix:\n",
    "antonym_matrix = np.zeros((len(word_indices), 250))\n",
    "\n",
    "# Fill the antonym matrix:\n",
    "#for key in antonyms.keys():\n",
    "    #antonym_matrix[key] = embedding_matrix[antonyms[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the retrofitting algorithm, using the previously found hyperparameters:\n",
    "def retrofit_matrix(embedding_matrix, neighbor_matrix, num_neighbors, alpha = 1, beta = 1, nb_iters = 10):\n",
    "    new_embedding_matrix = embedding_matrix\n",
    "    for i in np.arange(nb_iters):\n",
    "        new_embedding_matrix = (alpha * new_embedding_matrix * num_neighbors[:, None] + beta * (neighbor_matrix)) / (alpha + beta * num_neighbors[:, None]) # If we want to use the antonym matrix, we can subtract from the neighbor matrix\n",
    "        # Implement a stopping criterion: If the norm of the difference between the new and the old embedding matrix is less than 1e-2, stop the algorithm\n",
    "        if np.linalg.norm(new_embedding_matrix - embedding_matrix) < 1e-2:\n",
    "            break\n",
    "    return new_embedding_matrix\n",
    "\n",
    "# Run the algorithm on the embedding matrix using some basic hyperparameters:\n",
    "retrofitted_matrix = retrofit_matrix(embedding_matrix, neighbor_matrix, num_neighbors, alpha = 1, beta = 1, nb_iters = 10)\n",
    "fr_retrofitted_matrix = retrofit_matrix(fr_embedding_matrix, fr_neighbor_matrix, fr_num_neighbors, alpha = 1, beta = 1, nb_iters = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'diff_vec = np.zeros(250)\\nfor word in word_ids.keys():\\n    if word in wn_lexicon.keys():\\n        diff_vec += (retrofitted_matrix[word_ids[word]] - embedding_matrix[word_ids[word]])\\ndiff_vec = diff_vec / len(word_ids.keys())\\n\\n# Apply the difference vector on the words which were not present in the lexicon:\\nfor word in word_ids.keys():\\n    if word not in wn_lexicon.keys():\\n        retrofitted_matrix[word_ids[word]] = embedding_matrix[word_ids[word]] + diff_vec'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also improve the algorithm further by calculating a difference vector for all retrofitted vectors, and apply this difference vector on the words which were not present in the lexicon. This is done in the following code:\n",
    "# Calculate the difference vector:\n",
    "\"\"\"diff_vec = np.zeros(250)\n",
    "for word in word_ids.keys():\n",
    "    if word in wn_lexicon.keys():\n",
    "        diff_vec += (retrofitted_matrix[word_ids[word]] - embedding_matrix[word_ids[word]])\n",
    "diff_vec = diff_vec / len(word_ids.keys())\n",
    "\n",
    "# Apply the difference vector on the words which were not present in the lexicon:\n",
    "for word in word_ids.keys():\n",
    "    if word not in wn_lexicon.keys():\n",
    "        retrofitted_matrix[word_ids[word]] = embedding_matrix[word_ids[word]] + diff_vec\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluations.\n",
    "We will evaluate the new retrofitted word embeddings on a lexical similarity task, and a sentiment analysis task.\n",
    "## 3.1. Lexical similarity task.\n",
    "We will use the WordSim353 dataset to evaluate the performance of the retrofitting algorithm on a lexical similarity task. The WordSim353 dataset contains 353 pairs of words, and each pair is assigned a similarity score by human annotators. The similarity score ranges from 0 to 10, with 0 being not similar at all, and 10 being very similar. We will use the cosine similarity between the word embeddings of each pair of words to calculate the similarity score, and then compare it with the human similarity score to see how well the retrofitting algorithm performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the WordSim353 and RG65 datasets as pandas dataframes:\n",
    "# It is done with pandas for ease of manipulation, and also ease of calculation of the correlation between the human scores and the cosine similarities\n",
    "# Because the formats of two datasets are different, I read them separately\n",
    "def read_ws353():\n",
    "    with open('ws353.txt', 'r') as f:\n",
    "        data = f.read()\n",
    "    lines = data.split('\\n')\n",
    "    lines = lines[0:-1]\n",
    "    word1 = []\n",
    "    word2 = []\n",
    "    human_scores = []\n",
    "    for line in lines:\n",
    "        tokens = line.split('\\t')\n",
    "        word1.append(tokens[0])\n",
    "        word2.append(tokens[1])\n",
    "        human_scores.append(float(tokens[2]))\n",
    "    df = pd.DataFrame({'word1': word1, 'word2': word2, 'score': human_scores})\n",
    "\n",
    "    return df\n",
    "\n",
    "ws353_data = read_ws353()\n",
    "\n",
    "def read_rg65():\n",
    "    with open('rg65_french.txt', 'r', encoding = 'utf-8') as f:\n",
    "        data = f.read()\n",
    "    lines = data.split('\\n')\n",
    "    lines = lines[0:-1]\n",
    "    word1 = []\n",
    "    word2 = []\n",
    "    human_scores = []\n",
    "    for line in lines:\n",
    "        tokens = line.split(' ')\n",
    "        word1.append(tokens[0])\n",
    "        word2.append(tokens[1])\n",
    "        human_scores.append(float(tokens[2]))\n",
    "    df = pd.DataFrame({'word1': word1, 'word2': word2, 'score': human_scores})\n",
    "\n",
    "    return df\n",
    "\n",
    "rg65_data = read_rg65()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            word1     word2  score\n",
      "0            love       sex   6.77\n",
      "1           tiger       cat   7.35\n",
      "2           tiger     tiger  10.00\n",
      "3            book     paper   7.46\n",
      "4        computer  keyboard   7.62\n",
      "..            ...       ...    ...\n",
      "348        shower     flood   6.03\n",
      "349       weather  forecast   8.34\n",
      "350      disaster      area   6.25\n",
      "351      governor    office   6.34\n",
      "352  architecture   century   3.78\n",
      "\n",
      "[353 rows x 3 columns]\n",
      "         word1      word2  score\n",
      "0        corde    sourire   0.00\n",
      "1         midi    ficelle   0.00\n",
      "2          coq    périple   0.06\n",
      "3        fruit  fournaise   0.11\n",
      "4   autographe     rivage   0.00\n",
      "..         ...        ...    ...\n",
      "60     coussin   oreiller   3.00\n",
      "61   cimetière  cimetière   4.00\n",
      "62  automobile       auto   3.94\n",
      "63       joyau      bijou   3.22\n",
      "64        midi      dîner   2.17\n",
      "\n",
      "[65 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(ws353_data)\n",
    "print(rg65_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to compute the cosine similarity between two words:\n",
    "def emb_cosine_similarity(word1, word2, embedding_matrix, word_ids):\n",
    "    if word1 not in word_ids or word2 not in word_ids:\n",
    "        return None\n",
    "    word1_embedding = embedding_matrix[word_ids[word1]]\n",
    "    word2_embedding = embedding_matrix[word_ids[word2]]\n",
    "    return np.dot(word1_embedding, word2_embedding) / (np.linalg.norm(word1_embedding) * np.linalg.norm(word2_embedding))\n",
    "\n",
    "# Calculate the score for each pair of words in the dataset:\n",
    "ws353_data['original_score'] = ws353_data.apply(lambda row: emb_cosine_similarity(row['word1'], row['word2'], embedding_matrix, word_ids), axis = 1)\n",
    "ws353_data['retrofitted_score'] = ws353_data.apply(lambda row: emb_cosine_similarity(row['word1'], row['word2'], retrofitted_matrix, word_ids), axis = 1)\n",
    "\n",
    "# Calculate the score for each pair of words in the French dataset:\n",
    "rg65_data['original_score'] = rg65_data.apply(lambda row: emb_cosine_similarity(row['word1'], row['word2'], fr_embedding_matrix, fr_word_ids), axis = 1)\n",
    "rg65_data['retrofitted_score'] = rg65_data.apply(lambda row: emb_cosine_similarity(row['word1'], row['word2'], fr_retrofitted_matrix, fr_word_ids), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the scores for English original and retrofitted vectors against WS353 scores:\n",
      "                      score  original_score  retrofitted_score\n",
      "score              1.000000        0.690526           0.627718\n",
      "original_score     0.690526        1.000000           0.820189\n",
      "retrofitted_score  0.627718        0.820189           1.000000\n",
      "This is the scores for French original and retrofitted vectors against RG65 scores:\n",
      "                      score  original_score  retrofitted_score\n",
      "score              1.000000        0.704197           0.503364\n",
      "original_score     0.704197        1.000000           0.326257\n",
      "retrofitted_score  0.503364        0.326257           1.000000\n",
      "This is not a good result, because the correlations between the human scores and the retrofitted scores are much lower than the original vectors. \n",
      "We need to find better hyperparameters for the retrofitting algorithm.\n"
     ]
    }
   ],
   "source": [
    "# Because there are some words in the dataset that are not in the vocabulary, we need to remove them from the dataset before calculation of the correlation:\n",
    "ws353_data = ws353_data.dropna()\n",
    "rg65_data = rg65_data.dropna()\n",
    "\n",
    "# Calculate the Spearman correlation between the human scores and the model scores:\n",
    "print(\"This is the scores for English original and retrofitted vectors against WS353 scores:\")\n",
    "print(ws353_data[['score', 'original_score', 'retrofitted_score']].corr(method = 'spearman'))\n",
    "print(\"This is the scores for French original and retrofitted vectors against RG65 scores:\")\n",
    "print(rg65_data[['score', 'original_score', 'retrofitted_score']].corr(method = 'spearman'))\n",
    "print(\"This is not a good result, because the correlations between the human scores and the retrofitted scores are much lower than the original vectors. \\nWe need to find better hyperparameters for the retrofitting algorithm.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters are alpha = 2.1, beta = 3.1, and numIters = 1, with the Spearman correlation score of 0.7014897718483956 for the English set.\n"
     ]
    }
   ],
   "source": [
    "# We can use the lexical similarity task to test for better hyperparameters for the English data, using exhaustive search:\n",
    "# We iterate over the hyperparameters alpha, beta and the number of iterations:\n",
    "scores = []\n",
    "best_score = 0\n",
    "for alpha in np.arange(0.1, 5.1, 1):\n",
    "    for beta in np.arange(0.1, 5.1, 1):\n",
    "        for numIters in range(1, 5):\n",
    "            # For each set of hyperparameters, we retrofit the embedding matrix and calculate the correlation between the human scores and the model scores:\n",
    "            retrofitted_matrix = retrofit_matrix(embedding_matrix, neighbor_matrix, num_neighbors, alpha = alpha, beta = beta, nb_iters = numIters)\n",
    "            \n",
    "            # Because using emb_cosine_similarity() is too slow, we use the following method to calculate the cosine similarity:\n",
    "            word1_index = ws353_data['word1'].apply(lambda x: word_ids[x])\n",
    "            word2_index = ws353_data['word2'].apply(lambda x: word_ids[x])\n",
    "            word1_emb = retrofitted_matrix[word1_index]\n",
    "            word2_emb = retrofitted_matrix[word2_index]\n",
    "            ws353_data['retrofitted_score'] = np.sum(word1_emb * word2_emb, axis=1) / (np.linalg.norm(word1_emb, axis=1) * np.linalg.norm(word2_emb, axis=1))\n",
    "            \n",
    "            # Calculate the Spearman correlation between the human scores and the retrofitted scores:\n",
    "            spearman_score = ws353_data['score'].corr(ws353_data['retrofitted_score'], method = 'spearman')\n",
    "            \n",
    "            # Store the hyperparameters and the correlation score:\n",
    "            scores.append((alpha, beta, numIters, spearman_score))\n",
    "            \n",
    "            # Update the best hyperparameters and the best score so far:\n",
    "            if spearman_score > best_score:\n",
    "                best_score = spearman_score\n",
    "                best_alpha = alpha\n",
    "                best_beta = beta\n",
    "                best_numIters = numIters\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(f\"The best hyperparameters are alpha = {best_alpha}, beta = {best_beta}, and numIters = {best_numIters}, with the Spearman correlation score of {best_score} for the English set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters are alpha = 4.1, beta = 3.1, and numIters = 1, with the Spearman correlation score of 0.728031055603436 for the French set.\n"
     ]
    }
   ],
   "source": [
    "# We can use the lexical similarity task to test for better hyperparameters for the French data, using exhaustive search:\n",
    "# Same method as above, but for the French data:\n",
    "fr_scores = []\n",
    "fr_best_score = 0\n",
    "for alpha in np.arange(0.1, 5.1, 1):\n",
    "    for beta in np.arange(0.1, 5.1, 1):\n",
    "        for numIters in range(1, 5):\n",
    "            # Aply the retrofitting algorithm to the French data:\n",
    "            retrofitted_matrix = retrofit_matrix(fr_embedding_matrix, fr_neighbor_matrix, fr_num_neighbors, alpha = alpha, beta = beta, nb_iters = numIters)\n",
    "            \n",
    "            # Calculate the cosine similarity between the words in the dataset:\n",
    "            word1_index = rg65_data['word1'].apply(lambda x: fr_word_ids[x] if x in fr_word_ids else None)\n",
    "            word2_index = rg65_data['word2'].apply(lambda x: fr_word_ids[x] if x in fr_word_ids else None)\n",
    "            word1_emb = retrofitted_matrix[word1_index]\n",
    "            word2_emb = retrofitted_matrix[word2_index]\n",
    "            rg65_data['retrofitted_score'] = np.sum(word1_emb * word2_emb, axis=1) / (np.linalg.norm(word1_emb, axis=1) * np.linalg.norm(word2_emb, axis=1))\n",
    "            \n",
    "            \n",
    "            # Calculate the Spearman correlation between the human scores and the retrofitted scores:\n",
    "            spearman_score = rg65_data['score'].corr(rg65_data['retrofitted_score'], method = 'spearman')\n",
    "            fr_scores.append((alpha, beta, numIters, spearman_score))\n",
    "            if spearman_score > fr_best_score:\n",
    "                fr_best_score = spearman_score\n",
    "                fr_best_alpha = alpha\n",
    "                fr_best_beta = beta\n",
    "                fr_best_numIters = numIters\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(f\"The best hyperparameters are alpha = {fr_best_alpha}, beta = {fr_best_beta}, and numIters = {fr_best_numIters}, with the Spearman correlation score of {fr_best_score} for the French set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters are alpha = 2.1, beta = 3.1, and numIters = 1, with the Spearman correlation score of 0.7014897718483956 for the English set.\n",
      "The best hyperparameters are alpha = 4.1, beta = 3.1, and numIters = 1, with the Spearman correlation score of 0.728031055603436 for the French set.\n",
      "[(0.1, 0.1, 1, 0.7008698914673333), (0.1, 0.1, 2, 0.6968607783951691), (0.1, 0.1, 3, 0.6877194579559041), (0.1, 0.1, 4, 0.678658255938702), (0.1, 1.1, 1, 0.6572374307314311), (0.1, 1.1, 2, 0.41370918775106663), (0.1, 1.1, 3, 0.22262882591030553), (0.1, 1.1, 4, 0.20791368919189002), (0.1, 2.1, 1, 0.6212234020217932), (0.1, 2.1, 2, 0.26956720894623964), (0.1, 2.1, 3, 0.20901236492392733), (0.1, 2.1, 4, 0.2072842329038082), (0.1, 3.1, 1, 0.5886890983245912), (0.1, 3.1, 2, 0.2318245694211315), (0.1, 3.1, 3, 0.20746505794802642), (0.1, 3.1, 4, 0.20712926280854266), (0.1, 4.1, 1, 0.5631579746276502), (0.1, 4.1, 2, 0.22020069508706097), (0.1, 4.1, 3, 0.20724241810982308), (0.1, 4.1, 4, 0.20712926280854266), (1.1, 0.1, 1, 0.6933438669438073), (1.1, 0.1, 2, 0.6951916978016701), (1.1, 0.1, 3, 0.6957123079466304), (1.1, 0.1, 4, 0.695943406502701), (1.1, 1.1, 1, 0.7008698914673333), (1.1, 1.1, 2, 0.6968607783951691), (1.1, 1.1, 3, 0.6877194579559041), (1.1, 1.1, 4, 0.678658255938702), (1.1, 2.1, 1, 0.699941188084892), (1.1, 2.1, 2, 0.67967872843008), (1.1, 2.1, 3, 0.6440317761561895), (1.1, 2.1, 4, 0.5954372400012328), (1.1, 3.1, 1, 0.6967549646226037), (1.1, 3.1, 2, 0.6557006572532517), (1.1, 3.1, 3, 0.5784741591099897), (1.1, 3.1, 4, 0.45291422787650276), (1.1, 4.1, 1, 0.6922493407714785), (1.1, 4.1, 2, 0.6306981236237872), (1.1, 4.1, 3, 0.49689525162791454), (1.1, 4.1, 4, 0.3195989291463729), (2.1, 0.1, 1, 0.6923658476402158), (2.1, 0.1, 2, 0.692941519250538), (2.1, 0.1, 3, 0.6932706112551082), (2.1, 0.1, 4, 0.6937265840276868), (2.1, 1.1, 1, 0.6984069681819178), (2.1, 1.1, 2, 0.6993706236249803), (2.1, 1.1, 3, 0.698165974521927), (2.1, 1.1, 4, 0.695458227213713), (2.1, 2.1, 1, 0.7008698914673333), (2.1, 2.1, 2, 0.6968607783951691), (2.1, 2.1, 3, 0.6877194579559041), (2.1, 2.1, 4, 0.678658255938702), (2.1, 3.1, 1, 0.7014897718483956), (2.1, 3.1, 2, 0.6891454701095578), (2.1, 3.1, 3, 0.6701203772401156), (2.1, 3.1, 4, 0.6439446354023121), (2.1, 4.1, 1, 0.699788452367931), (2.1, 4.1, 2, 0.6790191080348869), (2.1, 4.1, 3, 0.6411467149696645), (2.1, 4.1, 4, 0.588838801670996), (3.1, 0.1, 1, 0.6924956011803299), (3.1, 0.1, 2, 0.6927334028713142), (3.1, 0.1, 3, 0.6931015964962141), (3.1, 0.1, 4, 0.6932176245696003), (3.1, 1.1, 1, 0.6968476913222423), (3.1, 1.1, 2, 0.6985751849485602), (3.1, 1.1, 3, 0.698854482236629), (3.1, 1.1, 4, 0.6971223602552516), (3.1, 2.1, 1, 0.6989435381719105), (3.1, 2.1, 2, 0.7005993720940323), (3.1, 2.1, 3, 0.6964469396134764), (3.1, 2.1, 4, 0.6924054280558963), (3.1, 3.1, 1, 0.7008698914673333), (3.1, 3.1, 2, 0.6968607783951691), (3.1, 3.1, 3, 0.6877194579559041), (3.1, 3.1, 4, 0.678658255938702), (3.1, 4.1, 1, 0.7014043866774715), (3.1, 4.1, 2, 0.6928764030828054), (3.1, 4.1, 3, 0.6764700015862959), (3.1, 4.1, 4, 0.6577641056175036), (4.1, 0.1, 1, 0.6919919084711043), (4.1, 0.1, 2, 0.6922415204474125), (4.1, 0.1, 3, 0.6927310088945594), (4.1, 0.1, 4, 0.6929683317901926), (4.1, 1.1, 1, 0.6958937713846499), (4.1, 1.1, 2, 0.6970824606426705), (4.1, 1.1, 3, 0.6974969378181645), (4.1, 1.1, 4, 0.6970727251372008), (4.1, 2.1, 1, 0.697949399424836), (4.1, 2.1, 2, 0.6995421919590794), (4.1, 2.1, 3, 0.6981718796645889), (4.1, 2.1, 4, 0.6957177342939415), (4.1, 3.1, 1, 0.6993147641673664), (4.1, 3.1, 2, 0.6999514023857129), (4.1, 3.1, 3, 0.6952314378158012), (4.1, 3.1, 4, 0.6902336123438737), (4.1, 4.1, 1, 0.7008698914673333), (4.1, 4.1, 2, 0.6968607783951691), (4.1, 4.1, 3, 0.6877194579559041), (4.1, 4.1, 4, 0.678658255938702)]\n",
      "[(0.1, 0.1, 1, 0.7233785696377824), (0.1, 0.1, 2, 0.6474216964552386), (0.1, 0.1, 3, 0.5926409695229315), (0.1, 0.1, 4, 0.5538860078817864), (0.1, 1.1, 1, 0.5637849813449928), (0.1, 1.1, 2, 0.4461986315174882), (0.1, 1.1, 3, 0.43704972869137615), (0.1, 1.1, 4, 0.434089207669294), (0.1, 2.1, 1, 0.5143443398623578), (0.1, 2.1, 2, 0.4395965825254215), (0.1, 2.1, 3, 0.43452168709244376), (0.1, 2.1, 4, 0.4348873056247339), (0.1, 3.1, 1, 0.48606465153526873), (0.1, 3.1, 2, 0.4365639510350956), (0.1, 3.1, 3, 0.43409441716696495), (0.1, 3.1, 4, 0.4348873056247339), (0.1, 4.1, 1, 0.4750065664261882), (0.1, 4.1, 2, 0.4370444837274842), (0.1, 4.1, 3, 0.43488208661173533), (0.1, 4.1, 4, 0.43488208661173533), (1.1, 0.1, 1, 0.7063109779074687), (1.1, 0.1, 2, 0.7065752708882824), (1.1, 0.1, 3, 0.7088422812453535), (1.1, 0.1, 4, 0.7084578504779504), (1.1, 1.1, 1, 0.7233785696377824), (1.1, 1.1, 2, 0.6474216964552386), (1.1, 1.1, 3, 0.5926409695229315), (1.1, 1.1, 4, 0.5538860078817864), (1.1, 2.1, 1, 0.6928560625205862), (1.1, 2.1, 2, 0.586826523945029), (1.1, 2.1, 3, 0.5260391383578638), (1.1, 2.1, 4, 0.4834158885429897), (1.1, 3.1, 1, 0.6599474930162743), (1.1, 3.1, 2, 0.5494957281567769), (1.1, 3.1, 3, 0.4697447334445324), (1.1, 3.1, 4, 0.44456482036336675), (1.1, 4.1, 1, 0.6315716969973345), (1.1, 4.1, 2, 0.5209277167541356), (1.1, 4.1, 3, 0.45919704084660135), (1.1, 4.1, 4, 0.4432006209698255), (2.1, 0.1, 1, 0.7044609270417723), (2.1, 0.1, 2, 0.7072960699268654), (2.1, 0.1, 3, 0.7042446873301975), (2.1, 0.1, 4, 0.703499861656995), (2.1, 1.1, 1, 0.721423731083092), (2.1, 1.1, 2, 0.7022504766567845), (2.1, 1.1, 3, 0.6568641638606758), (2.1, 1.1, 4, 0.6111488124790458), (2.1, 2.1, 1, 0.7233785696377824), (2.1, 2.1, 2, 0.6474294661527116), (2.1, 2.1, 3, 0.5926409695229315), (2.1, 2.1, 4, 0.5538860078817864), (2.1, 3.1, 1, 0.7050856195418775), (2.1, 3.1, 2, 0.6124222393960687), (2.1, 3.1, 3, 0.552931578140442), (2.1, 3.1, 4, 0.517509683067965), (2.1, 4.1, 1, 0.693288541943736), (2.1, 4.1, 2, 0.5842146318378677), (2.1, 4.1, 3, 0.5255168590400099), (2.1, 4.1, 4, 0.47190713056028133), (3.1, 0.1, 1, 0.7049018658794719), (3.1, 0.1, 2, 0.7055661522342662), (3.1, 0.1, 3, 0.705325885888072), (3.1, 0.1, 4, 0.7038122079070476), (3.1, 1.1, 1, 0.7182762419479463), (3.1, 1.1, 2, 0.7128462225239544), (3.1, 1.1, 3, 0.6864732159670868), (3.1, 1.1, 4, 0.6494159071286515), (3.1, 2.1, 1, 0.7250757795452457), (3.1, 2.1, 2, 0.6897326000200599), (3.1, 2.1, 3, 0.6287770279905587), (3.1, 2.1, 4, 0.5790178676937132), (3.1, 3.1, 1, 0.7233785696377824), (3.1, 3.1, 2, 0.6474216964552386), (3.1, 3.1, 3, 0.5926409695229315), (3.1, 3.1, 4, 0.5538860078817864), (3.1, 4.1, 1, 0.7086415614655536), (3.1, 4.1, 2, 0.6158266719306843), (3.1, 4.1, 3, 0.5644645011625344), (3.1, 4.1, 4, 0.5299554798008312), (4.1, 0.1, 1, 0.7042206606955781), (4.1, 0.1, 2, 0.7043732735742927), (4.1, 0.1, 3, 0.7072480166576266), (4.1, 0.1, 4, 0.7039083144455253), (4.1, 1.1, 1, 0.7151853889075043), (4.1, 1.1, 2, 0.7133353158393769), (4.1, 1.1, 3, 0.6961080120751265), (4.1, 1.1, 4, 0.6723613431902099), (4.1, 2.1, 1, 0.7232497553141688), (4.1, 2.1, 2, 0.7019381304067318), (4.1, 2.1, 3, 0.6581454737940724), (4.1, 2.1, 4, 0.6130155556802107), (4.1, 3.1, 1, 0.728031055603436), (4.1, 3.1, 2, 0.6804905121493765), (4.1, 3.1, 3, 0.6119824103915751), (4.1, 3.1, 4, 0.5748852865391708), (4.1, 4.1, 1, 0.7233785696377824), (4.1, 4.1, 2, 0.6474294661527116), (4.1, 4.1, 3, 0.5926409695229315), (4.1, 4.1, 4, 0.5538926550589497)]\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the results:\n",
    "print(f\"The best hyperparameters are alpha = {best_alpha}, beta = {best_beta}, and numIters = {best_numIters}, with the Spearman correlation score of {best_score} for the English set.\")\n",
    "print(f\"The best hyperparameters are alpha = {fr_best_alpha}, beta = {fr_best_beta}, and numIters = {fr_best_numIters}, with the Spearman correlation score of {fr_best_score} for the French set.\")\n",
    "print(scores)\n",
    "print(fr_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Spearman correlation score for the English set is 0.7014897718483956 with the following hyperparameters: alpha = 2.1, beta = 3.1, numIters = 1.\n",
      "                      score  original_score  retrofitted_score\n",
      "score              1.000000        0.690526           0.701490\n",
      "original_score     0.690526        1.000000           0.983356\n",
      "retrofitted_score  0.701490        0.983356           1.000000\n",
      "The best Spearman correlation score for the French set is 0.728031055603436 with the following hyperparameters: alpha = 4.1, beta = 3.1, numIters = 1.\n",
      "                      score  original_score  retrofitted_score\n",
      "score              1.000000        0.704197           0.728031\n",
      "original_score     0.704197        1.000000           0.953341\n",
      "retrofitted_score  0.728031        0.953341           1.000000\n"
     ]
    }
   ],
   "source": [
    "# Apply the best hyperparameters to the English data:\n",
    "retrofitted_matrix = retrofit_matrix(embedding_matrix, neighbor_matrix, num_neighbors, alpha = best_alpha, beta = best_beta, nb_iters = best_numIters)\n",
    "word1_index = ws353_data['word1'].apply(lambda x: word_ids[x])\n",
    "word2_index = ws353_data['word2'].apply(lambda x: word_ids[x])\n",
    "word1_emb = retrofitted_matrix[word1_index]\n",
    "word2_emb = retrofitted_matrix[word2_index]\n",
    "ws353_data['retrofitted_score'] = np.sum(word1_emb * word2_emb, axis=1) / (np.linalg.norm(word1_emb, axis=1) * np.linalg.norm(word2_emb, axis=1))\n",
    "spearman_score = ws353_data['score'].corr(ws353_data['retrofitted_score'], method = 'spearman')\n",
    "print(f\"The best Spearman correlation score for the English set is {spearman_score} with the following hyperparameters: alpha = {best_alpha}, beta = {best_beta}, numIters = {best_numIters}.\")\n",
    "print(ws353_data[['score', 'original_score', 'retrofitted_score']].corr(method = 'spearman'))\n",
    "\n",
    "# Apply the best hyperparameters to the French data:\n",
    "fr_retrofitted_matrix = retrofit_matrix(fr_embedding_matrix, fr_neighbor_matrix, fr_num_neighbors, alpha = fr_best_alpha, beta = fr_best_beta, nb_iters = fr_best_numIters)\n",
    "word1_index = rg65_data['word1'].apply(lambda x: fr_word_ids[x] if x in fr_word_ids else None)\n",
    "word2_index = rg65_data['word2'].apply(lambda x: fr_word_ids[x] if x in fr_word_ids else None)\n",
    "word1_emb = fr_retrofitted_matrix[word1_index]\n",
    "word2_emb = fr_retrofitted_matrix[word2_index]\n",
    "rg65_data['retrofitted_score'] = np.sum(word1_emb * word2_emb, axis=1) / (np.linalg.norm(word1_emb, axis=1) * np.linalg.norm(word2_emb, axis=1))\n",
    "fr_spearman_score = rg65_data['score'].corr(rg65_data['retrofitted_score'], method = 'spearman')\n",
    "print(f\"The best Spearman correlation score for the French set is {fr_spearman_score} with the following hyperparameters: alpha = {fr_best_alpha}, beta = {fr_best_beta}, numIters = {fr_best_numIters}.\")\n",
    "print(rg65_data[['score', 'original_score', 'retrofitted_score']].corr(method = 'spearman'))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Sentiment analysis task.\n",
    "We will use the Stanford Sentiment Treebank dataset to evaluate the performance of the retrofitting algorithm on a sentiment analysis task. The Stanford Sentiment Treebank dataset contains 11,855 sentences, and each sentence is assigned a sentiment score by human annotators. The sentiment score ranges between -1 and 1, with -1 as negative and 1 as positive. \n",
    "\n",
    "The two word embeddings will be used in a frozen way. We will train 2 neural network using the sentences vectorized by the two embeddings as input, and the sentiment scores as output. Then we will compare the predicted sentiment scores with original embeddings with the predicted sentiment scores with retrofitted embeddings to see how well the retrofitting algorithm performs. For this task, we will use accuracy as a metric to evaluate the performance of the retrofitting algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset from Stanford Sentiment Treebank:\n",
    "def read_sst(filename):\n",
    "    # Open the gzip file\n",
    "    with gzip.open(filename, 'rt', encoding='utf-8') as f:\n",
    "        # Read the data and split it into lines\n",
    "        data = f.read()\n",
    "        lines = data.split('\\n')\n",
    "        lines = lines[0:-1]\n",
    "        \n",
    "        # Initialize lists to store sentences and labels\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        \n",
    "        # Loop over each line\n",
    "        for line in lines:\n",
    "            # Skip the header line\n",
    "            if line.startswith('1') or line.startswith('-1'):\n",
    "                # Split the line into tokens using space as the delimiter\n",
    "                tokens = line.split(' ', 1)\n",
    "            \n",
    "                # Check if the tokens list contains at least two elements\n",
    "                if len(tokens) >= 2:\n",
    "                    # Append the label and sentence to the respective lists\n",
    "                    labels.append(int(tokens[0]))\n",
    "                    sentences.append(tokens[1])\n",
    "        \n",
    "        # Create a DataFrame from the sentences and labels\n",
    "        df = pd.DataFrame({'sentence': sentences, 'label': labels})\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Read the dataset\n",
    "dataset = read_sst('stanford_sentiment_analysis.tar.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to preprocess the sentences:\n",
    "def preprocess_sentence(sentence):\n",
    "    # Remove the HTML tags\n",
    "    sentence = re.sub(r'<[^>]+>', '', sentence)\n",
    "    \n",
    "    # Remove any non-alphanumeric characters (except spaces)\n",
    "    sentence = re.sub(r'[^a-zA-Z0-9 ]', '', sentence)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # Return the preprocessed sentence\n",
    "    return sentence\n",
    "\n",
    "# Preprocess the sentences\n",
    "dataset['sentence'] = dataset['sentence'].apply(preprocess_sentence)\n",
    "\n",
    "# Create a function to tokenize the sentences:\n",
    "def tokenize_sentence(sentence):\n",
    "    # Split the sentence into tokens\n",
    "    tokens = sentence.split(' ')\n",
    "    \n",
    "    # Return the list of tokens\n",
    "    return tokens\n",
    "\n",
    "# Tokenize the sentences\n",
    "dataset['tokens'] = dataset['sentence'].apply(tokenize_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the tokens using the embedding matrix:\n",
    "def vectorize_tokens(tokens, embedding_matrix):\n",
    "    # Initialize the vector\n",
    "    vectors = np.zeros((len(tokens), 250))\n",
    "    \n",
    "    # Loop over the tokens\n",
    "    for i in range(len(tokens)):\n",
    "        # Check if the token is in the vocabulary\n",
    "        if tokens[i] in word_ids:\n",
    "            # Get the index of the token in the vocabulary\n",
    "            index = word_ids[tokens[i]]\n",
    "            \n",
    "            # Get the vector of the token from the embedding matrix\n",
    "            vectors[i] = embedding_matrix[index]\n",
    "    \n",
    "    # Return the vector\n",
    "    return vectors\n",
    "\n",
    "# Vectorize the tokens\n",
    "dataset['original_vectors'] = dataset['tokens'].apply(lambda tokens: vectorize_tokens(tokens, embedding_matrix))\n",
    "dataset['retrofitted_vectors'] = dataset['tokens'].apply(lambda tokens: vectorize_tokens(tokens, retrofitted_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35, 250)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataset['tokens'][0]))\n",
    "dataset['original_vectors'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the vector for each sentence by averaging the vectors of the tokens:\n",
    "def calculate_sentence_vector(vectors):\n",
    "    # Calculate the average of all token vectors\n",
    "    vector = np.mean(vectors, axis = 0)\n",
    "    # Return the vector\n",
    "    return vector\n",
    "\n",
    "# Calculate the sentence vectors\n",
    "dataset['ori_sentence_vector'] = dataset['original_vectors'].apply(calculate_sentence_vector) \n",
    "dataset['retro_sentence_vector'] = dataset['retrofitted_vectors'].apply(calculate_sentence_vector)\n",
    "dataset['label'] = dataset['label'].apply(lambda x: 1 if x == 1 else 0) # Convert the labels to 0 and 1\n",
    "\n",
    "# Dropping the columns that are not needed:\n",
    "dataset = dataset.drop(['sentence', 'tokens', 'original_vectors', 'retrofitted_vectors'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example sets for the original and retrofitted datasets:\n",
    "dataset_original = [(dataset['ori_sentence_vector'][i], dataset['label'][i]) for i in range(len(dataset))]\n",
    "dataset_retrofitted = [(dataset['retro_sentence_vector'][i], dataset['label'][i]) for i in range(len(dataset))]\n",
    "\n",
    "# Split the dataset into train, dev, test sets:\n",
    "from sklearn.model_selection import train_test_split\n",
    "ori_train, ori_temp = train_test_split(dataset_original, test_size = 0.2, random_state = 0)\n",
    "ori_dev, ori_test = train_test_split(ori_temp, test_size = 0.5, random_state = 0)\n",
    "\n",
    "retro_train, retro_temp = train_test_split(dataset_retrofitted, test_size = 0.2, random_state = 0)\n",
    "retro_dev, retro_test = train_test_split(retro_temp, test_size = 0.5, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple one hidden layer neural network:\n",
    "class SentimentAnalysis(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, early_stop = False):\n",
    "        super(SentimentAnalysis, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_dim, hidden_dim) #[250, 100]\n",
    "        self.linear2 = torch.nn.Linear(hidden_dim, output_dim) #[100, 2]\n",
    "        self.sigmoid = torch.nn.Sigmoid() # Sigmoid for binary classification\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.early_stop = early_stop\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x) # [batch_size, 250] * [250, 100] = [batch_size, 100]\n",
    "        out = self.activation(out) # [batch_size, 100]\n",
    "        out = self.linear2(out) # [batch_size, 100] * [100, 2] = [batch_size, 2]\n",
    "        out = self.sigmoid(out) # [batch_size, 2]\n",
    "        return out\n",
    "\n",
    "# Create the model for original embeddings:\n",
    "model_original = SentimentAnalysis(250, 100, 2)\n",
    "# Create the model for retrofitted embeddings:\n",
    "model_retrofitted = SentimentAnalysis(250, 100, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer:\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer_original = torch.optim.Adam(model_original.parameters(), lr = 0.001)\n",
    "optimizer_retrofitted = torch.optim.Adam(model_retrofitted.parameters(), lr = 0.001)\n",
    "\n",
    "from random import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the original model...\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wazzat\\AppData\\Local\\Temp\\ipykernel_14424\\4129527228.py:17: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  sentence = torch.FloatTensor(sentence) # [batch_size, 250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on training set at epoch 0 : 153.400101\n",
      "Epoch: 1\n",
      "Loss on training set at epoch 1 : 133.660222\n",
      "Epoch: 2\n",
      "Loss on training set at epoch 2 : 127.935602\n",
      "Epoch: 3\n",
      "Loss on training set at epoch 3 : 125.240231\n",
      "Epoch: 4\n",
      "Loss on training set at epoch 4 : 124.072411\n",
      "Epoch: 5\n",
      "Loss on training set at epoch 5 : 122.993484\n",
      "Epoch: 6\n",
      "Loss on training set at epoch 6 : 122.440195\n",
      "Epoch: 7\n",
      "Loss on training set at epoch 7 : 122.158350\n",
      "Epoch: 8\n",
      "Loss on training set at epoch 8 : 121.619787\n",
      "Epoch: 9\n",
      "Loss on training set at epoch 9 : 121.064905\n",
      "Epoch: 10\n",
      "Loss on training set at epoch 10 : 121.126370\n",
      "Epoch: 11\n",
      "Loss on training set at epoch 11 : 120.509867\n",
      "Epoch: 12\n",
      "Loss on training set at epoch 12 : 120.093741\n",
      "Epoch: 13\n",
      "Loss on training set at epoch 13 : 120.068827\n",
      "Epoch: 14\n",
      "Loss on training set at epoch 14 : 120.008695\n",
      "Epoch: 15\n",
      "Loss on training set at epoch 15 : 120.037117\n",
      "Epoch: 16\n",
      "Loss on training set at epoch 16 : 119.592616\n",
      "Epoch: 17\n",
      "Loss on training set at epoch 17 : 119.329150\n",
      "Epoch: 18\n"
     ]
    }
   ],
   "source": [
    "# Training the original model:\n",
    "train_losses = []\n",
    "print('Training the original model...')\n",
    "for epoch in range(50):\n",
    "    print('Epoch:', epoch)\n",
    "    epoch_loss = 0\n",
    "    shuffle(ori_train)\n",
    "    i = 0\n",
    "    # Process the training data in batches of 32:\n",
    "    while i < len(ori_train):\n",
    "        batch = ori_train[i:i+32]\n",
    "        i += 32\n",
    "        # Get the sentence vectors and labels from the batch:\n",
    "        sentence, label = zip(*batch)\n",
    "        \n",
    "        # Step 1: Process the input data:\n",
    "        sentence = torch.FloatTensor(sentence) # [batch_size, 250]\n",
    "        label = torch.LongTensor(label) # [batch_size]\n",
    "        \n",
    "        # Step 2: Zero the gradients:\n",
    "        model_original.zero_grad()\n",
    "        \n",
    "        # Step 3: Run the forward propagation:\n",
    "        output = model_original(sentence)\n",
    "        \n",
    "        # Step 4: Compute loss and gradients:\n",
    "        loss = loss_function(output, label)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step 5: Update the weights:\n",
    "        optimizer_original.step()\n",
    "    # end of handling of this batch\n",
    "    print(\"Loss on training set at epoch %d : %f\" %(epoch, epoch_loss))\n",
    "    train_losses.append(epoch_loss)    \n",
    "    # Step 6: Early stopping:\n",
    "    with torch.no_grad():\n",
    "        if model_original.early_stop:\n",
    "            # forward propagation\n",
    "            # We convert the test examples to tensors and do the forward propagation\n",
    "            dev_sentence, dev_label = zip(*ori_dev)\n",
    "            dev_sentence = torch.tensor(dev_sentence, dtype=torch.float32) # [DEV_SIZE, CONTEXT_SIZE]\n",
    "            dev_label = torch.tensor(dev_label, dtype=torch.long)   # [DEV_SIZE]        \n",
    "            output = model_original(dev_sentence)    \n",
    "                \n",
    "            # total loss on the dev set\n",
    "            dev_loss = loss_function(output, dev_label)\n",
    "            print(\"Loss on dev set at epoch %d: %f\\n\" %(epoch, dev_loss))\n",
    "                \n",
    "            # prediction and accuracy on the dev set\n",
    "            pred_labels = torch.argmax(output, dim=1)\n",
    "            accuracy = torch.sum(pred_labels == dev_label).item() / len(dev_label)\n",
    "            print(\"Accuracy on dev, after epoch %d: %3.2f\\n\" % (epoch, accuracy * 100))\n",
    "                \n",
    "            # early stopping\n",
    "            # if first epoch: we record the dev loss, to be used for early stopping\n",
    "            if epoch == 0:\n",
    "                previous_dev_loss = dev_loss\n",
    "            elif dev_loss > previous_dev_loss:\n",
    "                print(\"Loss on dev has increased, we stop training!\")\n",
    "                break\n",
    "            else:\n",
    "                previous_dev_loss = dev_loss\n",
    "# Plot the training losses\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training losses over Epochs\")\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on test set after training: 0.509965\n",
      "\n",
      "Accuracy on test after training: 78.98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on test set after training:\n",
    "with torch.no_grad():\n",
    "    # forward propagation on test set\n",
    "    test_sentence, test_label = zip(*ori_test)\n",
    "    test_sentence = torch.tensor(test_sentence, dtype=torch.float32) # [DEV_SIZE, CONTEXT_SIZE]\n",
    "    test_label = torch.tensor(test_label, dtype=torch.long)   # [DEV_SIZE]\n",
    "    log_probs = model_original(test_sentence)\n",
    "    \n",
    "    # total loss on the test set\n",
    "    test_loss = loss_function(log_probs, test_label)\n",
    "    print(\"Loss on test set after training: %f\\n\" %(test_loss))\n",
    "            \n",
    "    # prediction and accuracy on the dev set\n",
    "    pred_labels = torch.argmax(log_probs, dim=1)\n",
    "    accuracy = torch.sum(pred_labels == test_label).item() / len(test_label)\n",
    "    print(\"Accuracy on test after training: %3.2f\\n\" % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the retrofitted model...\n",
      "Epoch: 0\n",
      "Loss on training set at epoch 0 : 157.617834\n",
      "Epoch: 1\n",
      "Loss on training set at epoch 1 : 140.571887\n",
      "Epoch: 2\n",
      "Loss on training set at epoch 2 : 134.518117\n",
      "Epoch: 3\n",
      "Loss on training set at epoch 3 : 132.188575\n",
      "Epoch: 4\n",
      "Loss on training set at epoch 4 : 130.670829\n",
      "Epoch: 5\n",
      "Loss on training set at epoch 5 : 129.765735\n",
      "Epoch: 6\n",
      "Loss on training set at epoch 6 : 128.907196\n",
      "Epoch: 7\n",
      "Loss on training set at epoch 7 : 128.734525\n",
      "Epoch: 8\n",
      "Loss on training set at epoch 8 : 127.816995\n",
      "Epoch: 9\n",
      "Loss on training set at epoch 9 : 127.621325\n",
      "Epoch: 10\n",
      "Loss on training set at epoch 10 : 127.291265\n",
      "Epoch: 11\n",
      "Loss on training set at epoch 11 : 127.098669\n",
      "Epoch: 12\n",
      "Loss on training set at epoch 12 : 127.002913\n",
      "Epoch: 13\n",
      "Loss on training set at epoch 13 : 126.495856\n",
      "Epoch: 14\n",
      "Loss on training set at epoch 14 : 126.327872\n",
      "Epoch: 15\n",
      "Loss on training set at epoch 15 : 126.018092\n",
      "Epoch: 16\n",
      "Loss on training set at epoch 16 : 125.771460\n",
      "Epoch: 17\n",
      "Loss on training set at epoch 17 : 125.722582\n",
      "Epoch: 18\n",
      "Loss on training set at epoch 18 : 125.448019\n",
      "Epoch: 19\n",
      "Loss on training set at epoch 19 : 125.215962\n",
      "Epoch: 20\n",
      "Loss on training set at epoch 20 : 125.121865\n",
      "Epoch: 21\n",
      "Loss on training set at epoch 21 : 125.226653\n",
      "Epoch: 22\n",
      "Loss on training set at epoch 22 : 124.777385\n",
      "Epoch: 23\n",
      "Loss on training set at epoch 23 : 124.616959\n",
      "Epoch: 24\n",
      "Loss on training set at epoch 24 : 124.364633\n",
      "Epoch: 25\n",
      "Loss on training set at epoch 25 : 124.415838\n",
      "Epoch: 26\n",
      "Loss on training set at epoch 26 : 124.972664\n",
      "Epoch: 27\n",
      "Loss on training set at epoch 27 : 124.274811\n",
      "Epoch: 28\n",
      "Loss on training set at epoch 28 : 124.017923\n",
      "Epoch: 29\n",
      "Loss on training set at epoch 29 : 123.910441\n",
      "Epoch: 30\n",
      "Loss on training set at epoch 30 : 123.745074\n",
      "Epoch: 31\n",
      "Loss on training set at epoch 31 : 123.915622\n",
      "Epoch: 32\n",
      "Loss on training set at epoch 32 : 123.587692\n",
      "Epoch: 33\n",
      "Loss on training set at epoch 33 : 123.375909\n",
      "Epoch: 34\n",
      "Loss on training set at epoch 34 : 123.663909\n",
      "Epoch: 35\n",
      "Loss on training set at epoch 35 : 123.326716\n",
      "Epoch: 36\n",
      "Loss on training set at epoch 36 : 123.470733\n",
      "Epoch: 37\n",
      "Loss on training set at epoch 37 : 123.164287\n",
      "Epoch: 38\n",
      "Loss on training set at epoch 38 : 122.973452\n",
      "Epoch: 39\n",
      "Loss on training set at epoch 39 : 123.132365\n",
      "Epoch: 40\n",
      "Loss on training set at epoch 40 : 122.673259\n",
      "Epoch: 41\n",
      "Loss on training set at epoch 41 : 123.120938\n",
      "Epoch: 42\n",
      "Loss on training set at epoch 42 : 122.603342\n",
      "Epoch: 43\n",
      "Loss on training set at epoch 43 : 122.738114\n",
      "Epoch: 44\n",
      "Loss on training set at epoch 44 : 122.745427\n",
      "Epoch: 45\n",
      "Loss on training set at epoch 45 : 122.673173\n",
      "Epoch: 46\n",
      "Loss on training set at epoch 46 : 122.109415\n",
      "Epoch: 47\n",
      "Loss on training set at epoch 47 : 122.110455\n",
      "Epoch: 48\n",
      "Loss on training set at epoch 48 : 122.025622\n",
      "Epoch: 49\n",
      "Loss on training set at epoch 49 : 122.232265\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training the retrofitted model:\n",
    "train_losses = []\n",
    "print('Training the retrofitted model...')\n",
    "\n",
    "for epoch in range(50):\n",
    "    print('Epoch:', epoch)\n",
    "    epoch_loss = 0\n",
    "    shuffle(retro_train)\n",
    "    i = 0\n",
    "    # Process the training data in batches of 32:\n",
    "    while i < len(retro_train):\n",
    "        batch = retro_train[i:i+32]\n",
    "        i += 32\n",
    "        # Get the sentence vectors and labels from the batch:\n",
    "        sentence, label = zip(*batch)\n",
    "        \n",
    "        # Step 1: Process the input data:\n",
    "        sentence = torch.FloatTensor(sentence) # [batch_size, 250]\n",
    "        label = torch.LongTensor(label) # [batch_size]\n",
    "        \n",
    "        # Step 2: Zero the gradients:\n",
    "        model_retrofitted.zero_grad()\n",
    "        \n",
    "        # Step 3: Run the forward propagation:\n",
    "        output = model_retrofitted(sentence)\n",
    "        \n",
    "        # Step 4: Compute loss and gradients:\n",
    "        loss = loss_function(output, label)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step 5: Update the weights:\n",
    "        optimizer_retrofitted.step()\n",
    "    # end of handling of this batch\n",
    "    print(\"Loss on training set at epoch %d : %f\" %(epoch, epoch_loss))\n",
    "    train_losses.append(epoch_loss)    \n",
    "        # Step 6: Early stopping:\n",
    "    with torch.no_grad():\n",
    "        if model_retrofitted.early_stop:\n",
    "            # forward propagation\n",
    "            # We convert the test examples to tensors and do the forward propagation\n",
    "            dev_sentence, dev_label = zip(*retro_dev)\n",
    "            dev_sentence = torch.tensor(dev_sentence, dtype=torch.float32) # [DEV_SIZE, CONTEXT_SIZE]\n",
    "            dev_label = torch.tensor(dev_label, dtype=torch.long)   # [DEV_SIZE]        \n",
    "            output = model_retrofitted(dev_sentence)    \n",
    "                \n",
    "            # total loss on the dev set\n",
    "            dev_loss = loss_function(output, dev_label)\n",
    "            print(\"Loss on dev set at epoch %d: %f\\n\" %(epoch, dev_loss))\n",
    "                \n",
    "            # prediction and accuracy on the dev set\n",
    "            pred_labels = torch.argmax(output, dim=1)\n",
    "            accuracy = torch.sum(pred_labels == dev_label).item() / len(dev_label)\n",
    "            print(\"Accuracy on dev, after epoch %d: %3.2f\\n\" % (epoch, accuracy * 100))\n",
    "                \n",
    "            # early stopping\n",
    "            # if first epoch: we record the dev loss, to be used for early stopping\n",
    "            if epoch == 0:\n",
    "                previous_dev_loss = dev_loss\n",
    "            elif dev_loss > previous_dev_loss:\n",
    "                print(\"Loss on dev has increased, we stop training!\")\n",
    "                break\n",
    "            else:\n",
    "                previous_dev_loss = dev_loss\n",
    "# Plot the training losses\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training losses over Epochs\")\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on test set after training: 0.543930\n",
      "\n",
      "Accuracy on test after training: 75.13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on test set after training:\n",
    "with torch.no_grad():\n",
    "    # forward propagation on test set\n",
    "    test_sentence, test_label = zip(*retro_test)\n",
    "    test_sentence = torch.tensor(test_sentence, dtype=torch.float32) # [DEV_SIZE, CONTEXT_SIZE]\n",
    "    test_label = torch.tensor(test_label, dtype=torch.long)   # [DEV_SIZE]\n",
    "    log_probs = model_retrofitted(test_sentence)\n",
    "    \n",
    "    # total loss on the test set\n",
    "    test_loss = loss_function(log_probs, test_label)\n",
    "    print(\"Loss on test set after training: %f\\n\" %(test_loss))\n",
    "            \n",
    "    # prediction and accuracy on the dev set\n",
    "    pred_labels = torch.argmax(log_probs, dim=1)\n",
    "    accuracy = torch.sum(pred_labels == test_label).item() / len(test_label)\n",
    "    print(\"Accuracy on test after training: %3.2f\\n\" % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
