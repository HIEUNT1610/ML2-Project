{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP PROJECT: \n",
    "## *Improving vector space representations using semantic resources*\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User manual:\n",
    "Our project was implemented wholly in the form of Jupyter notebook, and all the necessary files including the embeddings were correctly named and included in the same folder named \"Code\" as the notebook to ease the re-implementation process. \n",
    "\n",
    "Please modify the filepath for the English and French embeddings to run the project if necessary.\n",
    "\n",
    "The notebook has already been optimized in order to avoid the problem of \"hidden state\" due to non-sequential execution of cells.\n",
    "\n",
    "The results in the sentiment analysis task might slightly vary due to the randomness of the training process, but the results in the classification task should be approximately the same as the ones in the report due to the randomization of weights during training. Further support will be provided where necessary in the notebook, in the form of comments and markdown cells."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Installing/importing necessary libraries and importing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the necessary libraries. Uncomment the following line to install the libraries.\n",
    "#!pip install numpy pandas gzip nltk matplotlib torch sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries:\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import re\n",
    "import math\n",
    "import sys\n",
    "import gzip\n",
    "from nltk.corpus import wordnet as wn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a read vectors function to read the vectors from the file.\n",
    "# This function can also be used on other files with the same format.\n",
    "def read_vectors_file(filename):\n",
    "    word_vectors = {}\n",
    "    if filename.endswith('.gz'): fileObject = gzip.open(filename, 'rt', encoding='utf-8' )\n",
    "    else: fileObject = open(filename, 'r', encoding='utf-8')    \n",
    "    first_line = True\n",
    "    for line in fileObject:\n",
    "        line = line.strip()\n",
    "        if first_line: #skip the first line containing the size of the vocabulary and the vector size\n",
    "            first_line = False\n",
    "            continue\n",
    "        if line:\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            vector = [float(x) for x in parts[1:]]\n",
    "            word_vectors[word] = np.array(vector) # Convert to numpy array for later use in the code\n",
    "    return word_vectors\n",
    "\n",
    "# Read the vectors from the file as a dictionary. \n",
    "vectors = read_vectors_file('vectors_datatxt_250_sg_w10_i5_c500_gensim_clean.gz')\n",
    "fr_vectors = read_vectors_file('vecs100-linear-frwiki')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the imported data using the commands below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The English vocabulary size is 125776\n",
      "The French vocabulary size is 150361\n",
      "The English vector size is 250\n",
      "The French vector size is 100\n"
     ]
    }
   ],
   "source": [
    "print(f\"The English vocabulary size is {len(vectors)}\")\n",
    "print(f\"The French vocabulary size is {len(fr_vectors)}\")\n",
    "print(f\"The English vector size is {len(vectors['dog'])}\")\n",
    "print(f\"The French vector size is {len(fr_vectors['le'])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a small toy corpus for testing purpose. The corpus contains 10 simple words, which will be used to test the lexicons later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy corpus of 10 words:\n",
    "toy_corpus = ['frog', 'toad', 'berger', 'cat', 'cheetah', 'dog', 'feline', 'true', 'false', 'incorrect']\n",
    "\n",
    "# Vectorize the toy corpus:\n",
    "vec_toy_corpus = [vectors[word] for word in toy_corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use gensim pre-trained models for the embedding. It's just too large or my laptop to run so we chose not to use it.\n",
    "# Load the word2vec model:\n",
    "#wv = api.load('word2vec-google-news-300')\n",
    "#wv = w2v.Word2Vec(vector_size=250, min_count=500, window=8, sample=1e-3, workers=8, sg=1, hs=0, negative=10, epochs=5)\n",
    "\n",
    "# Vectorize the toy corpus:\n",
    "#vec_toy_corpus = [wv.wv[word] for word in toy_corpus]\n",
    "#vec_toy_corpus = [wv[word] for word in toy_corpus]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the toy corpus to look at the original similarity between vectorized words using the original embedding. The pair of words can be changed if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between cat and feline is: 0.5670\n"
     ]
    }
   ],
   "source": [
    "# Compare vectors using cosine similarity:\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Compute the cosine similarity matrix:\n",
    "similarity_matrix = np.zeros((len(vec_toy_corpus), len(vec_toy_corpus)))\n",
    "for i in range(len(vec_toy_corpus)):\n",
    "    for j in range(len(vec_toy_corpus)):\n",
    "        similarity_matrix[i][j] = cosine_similarity(vec_toy_corpus[i], vec_toy_corpus[j])\n",
    "print(f\"The similarity between cat and feline is: {similarity_matrix[3][6]:.4f}\")\n",
    "# Print the cosine similarity matrix:\n",
    "#for i in range(len(vec_toy_corpus)):\n",
    "#    for j in range(len(vec_toy_corpus)):\n",
    "#        print(f'Similarity between {toy_corpus[i]} and {toy_corpus[j]} is {similarity_matrix[i][j]:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementing the original algorithm and finding the suitable lexicons for English and French datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We implement the retrofitting algorithm of Faruqui et al. (2015):\n",
    "# Preprocessing the data:\n",
    "isNumber = re.compile(r'\\d+.*')\n",
    "def norm_word(word):\n",
    "  word = word.replace('[', '').replace(']', '').replace(\"'\", '').replace('\"', '').replace(':', '').replace(',', '').replace('.', '')\n",
    "  if isNumber.search(word.lower()):\n",
    "    return '---num---'\n",
    "  elif re.sub(r'\\W+', '', word) == '':\n",
    "    return '---punc---'\n",
    "  else:\n",
    "    return word.lower()\n",
    "\n",
    "# Read all the word vectors and normalize them:\n",
    "def read_word_vecs(filename):\n",
    "  wordVectors = {}\n",
    "  if filename.endswith('.gz'): fileObject = gzip.open(filename, 'r')\n",
    "  else: fileObject = open(filename, 'r')\n",
    "  \n",
    "  for line in fileObject:\n",
    "    line = line.strip().lower()\n",
    "    word = line.split()[0]\n",
    "    wordVectors[word] = np.zeros(len(line.split())-1, dtype=float)\n",
    "    for index, vecVal in enumerate(line.split()[1:]):\n",
    "      wordVectors[word][index] = float(vecVal)\n",
    "    ''' normalize weight vector '''\n",
    "    wordVectors[word] /= math.sqrt((wordVectors[word]**2).sum() + 1e-6)\n",
    "    \n",
    "  sys.stderr.write(\"Vectors read from: \"+filename+\" \\n\")\n",
    "  return wordVectors\n",
    "\n",
    "# Read lexicon as a dictionary:\n",
    "def read_lexicon(filename, encoding='utf-8'):\n",
    "  lexicon = {}\n",
    "  if filename.endswith('.gz'): fileObject = gzip.open(filename, 'r')\n",
    "  else: fileObject = open(filename, 'r', encoding=encoding)\n",
    "  \n",
    "  for line in fileObject:\n",
    "    words = line.lower().strip().split()\n",
    "    lexicon[norm_word(words[0])] = [norm_word(word) for word in words[1:]]\n",
    "  return lexicon\n",
    "\n",
    "# Retrofit word vectors to a lexicon:\n",
    "def retrofit(wordVecs, lexicon, numIters, alpha = 1, beta = 1): # wordVecs is a dictionary\n",
    "  newWordVecs = deepcopy(wordVecs)\n",
    "  wvVocab = set(newWordVecs.keys())\n",
    "  loopVocab = wvVocab.intersection(set(lexicon.keys()))\n",
    "  for it in range(numIters):\n",
    "    # loop through every node also in ontology (else just use data estimate)\n",
    "    for word in loopVocab:\n",
    "      wordNeighbours = set(lexicon[word]).intersection(wvVocab)\n",
    "      numNeighbours = len(wordNeighbours)\n",
    "      #no neighbours, pass - use data estimate\n",
    "      if numNeighbours == 0:\n",
    "        continue\n",
    "      # the weight of the old vector is alpha\n",
    "      newVec = alpha * wordVecs[word]\n",
    "      # loop over neighbours and add to new vector (currently with weight beta)\n",
    "      for ppWord in wordNeighbours:\n",
    "        newVec += beta * newWordVecs[ppWord] \n",
    "      newWordVecs[word] = newVec/(alpha + beta*numNeighbours) \n",
    "      # define a stopping mechanism using Euclidean distance\n",
    "      # if distance too small, use the original vector\n",
    "      if np.linalg.norm(newWordVecs[word] - wordVecs[word]) < 0.01:\n",
    "        newWordVecs[word] = wordVecs[word]\n",
    "  return newWordVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['frog', 'toad', 'berger', 'cat', 'cheetah', 'dog', 'feline', 'true', 'false', 'incorrect'])\n"
     ]
    }
   ],
   "source": [
    "# Transform the toy corpus into a wordVecs dictionary:\n",
    "toy_wordVecs = {}\n",
    "for i in range(len(vec_toy_corpus)):\n",
    "    toy_wordVecs[toy_corpus[i]] = vec_toy_corpus[i]\n",
    "print(toy_wordVecs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport xml.etree.ElementTree as ET\\n\\n# specify the path to the WOLF file\\nwolf_path = \\'wolf-1.0b4.xml\\'\\n\\n# parse the WOLF file\\ntree = ET.parse(wolf_path)\\nroot = tree.getroot()\\n\\nword_relations = {}\\n\\nsynsets = root.findall(\\'SYNSET\\')\\nfor synset in synsets:\\n    synset_id = synset.find(\\'ID\\').text\\n    synonym_element = synset.find(\\'SYNONYM\\')\\n    for literal in synonym_element.findall(\\'LITERAL\\'):\\n        word = literal.text\\n        if word != \\'_EMPTY_\\':\\n            if word not in word_relations:\\n                word_relations[word] = []\\n            for ilr in synset.findall(\\'ILR\\'):\\n                if ilr.get(\\'type\\') in [\\'hypernym\\', \\'hyponym\\']:\\n                    target_id = ilr.text\\n                    target_synset = root.find(f\"SYNSET[ID=\\'{target_id}\\']\")\\n                    target_synonym_element = target_synset.find(\\'SYNONYM\\')\\n                    for target_literal in target_synonym_element.findall(\\'LITERAL\\'):\\n                        word_relations[word].append(target_literal.text) if target_literal.text != \\'_EMPTY_\\' else None\\n\\n    \\ndef export_dict_to_file(dictionary, filename):\\n    with open(filename, \\'w\\', encoding = \\'utf-8\\') as f:\\n        for key, value in dictionary.items():\\n            f.write(f\"{key}: {value}\\n\")\\n\\nexport_dict_to_file(word_relations, \\'word_relations_lemma.txt\\')'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a French lexicon dictionary and add all synonyms, hypernyms, hyponyms\n",
    "# It took a lot of time so I saved the dictionary in a file named \"word_relations_lemma.txt\". For some reasons, my laptop can't read the 'fra' lexicon in NLTK WordNet.\n",
    "\n",
    "\"\"\"\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# specify the path to the WOLF file\n",
    "wolf_path = 'wolf-1.0b4.xml'\n",
    "\n",
    "# parse the WOLF file\n",
    "tree = ET.parse(wolf_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "word_relations = {}\n",
    "\n",
    "synsets = root.findall('SYNSET')\n",
    "for synset in synsets:\n",
    "    synset_id = synset.find('ID').text\n",
    "    synonym_element = synset.find('SYNONYM')\n",
    "    for literal in synonym_element.findall('LITERAL'):\n",
    "        word = literal.text\n",
    "        if word != '_EMPTY_':\n",
    "            if word not in word_relations:\n",
    "                word_relations[word] = []\n",
    "            for ilr in synset.findall('ILR'):\n",
    "                if ilr.get('type') in ['hypernym', 'hyponym']:\n",
    "                    target_id = ilr.text\n",
    "                    target_synset = root.find(f\"SYNSET[ID='{target_id}']\")\n",
    "                    target_synonym_element = target_synset.find('SYNONYM')\n",
    "                    for target_literal in target_synonym_element.findall('LITERAL'):\n",
    "                        word_relations[word].append(target_literal.text) if target_literal.text != '_EMPTY_' else None\n",
    "\n",
    "    \n",
    "def export_dict_to_file(dictionary, filename):\n",
    "    with open(filename, 'w', encoding = 'utf-8') as f:\n",
    "        for key, value in dictionary.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "export_dict_to_file(word_relations, 'word_relations_lemma.txt')\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the English lexicons from files:\n",
    "frame_lexicon = read_lexicon('framenet.txt')\n",
    "wn_lexicon = read_lexicon('wordnet-synonyms+.txt')\n",
    "ppdb_lexicon = read_lexicon('ppdb-xl.txt')\n",
    "\n",
    "# Read the French lexicon from file:\n",
    "fr_lexicon = read_lexicon('word_relations_lemma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shepherd', 'accompany', 'pursuer', 'lead', 'show', 'track', 'stalk', 'guided', 'walk', 'trail', 'tail', 'usher', 'pursuit', 'escort', 'conduct', 'follow', 'shadow', 'hound', 'guide', 'pursue', 'chase']\n",
      "['chase', 'chase_after', 'trail', 'tail', 'tag', 'give_chase', 'go_after', 'track', 'domestic_dog', 'canis_familiaris', 'andiron', 'firedog', 'dog-iron', 'pawl', 'detent', 'click', 'frank', 'frankfurter', 'hotdog', 'hot_dog', 'wiener', 'wienerwurst', 'weenie', 'cad', 'bounder', 'blackguard', 'hound', 'heel', 'frump', 'pursue', 'follow', 'domestic_animal', 'domesticated_animal', 'canine', 'canid', 'support', 'catch', 'stop', 'sausage', 'villain', 'scoundrel', 'chap', 'fellow', 'feller', 'fella', 'lad', 'gent', 'blighter', 'cuss', 'bloke', 'unpleasant_woman', 'disagreeable_woman', 'tree', 'run_down', 'quest', 'hound', 'hunt', 'trace', 'puppy', 'great_pyrenees', 'basenji', 'newfoundland', 'newfoundland_dog', 'lapdog', 'poodle', 'poodle_dog', 'leonberg', 'toy_dog', 'toy', 'spitz', 'pooch', 'doggie', 'doggy', 'barker', 'bow-wow', 'cur', 'mongrel', 'mutt', 'mexican_hairless', 'hunting_dog', 'working_dog', 'dalmatian', 'coach_dog', 'carriage_dog', 'pug', 'pug-dog', 'corgi', 'welsh_corgi', 'griffon', 'brussels_griffon', 'belgian_griffon', 'vienna_sausage', 'perisher']\n",
      "['dogs', 'chien', 'doggy', 'puppy', 'doggie', 'canine', 'hound', 'mutt', 'bitch', 'boy', 'cabot', 'lapdog', 'bloodhound', 'cane', 'psa', 'pup', 'pooch']\n",
      "['de', 'prairie', 'rodentia', 'rongeur']\n"
     ]
    }
   ],
   "source": [
    "# Test the lexicons:\n",
    "print(frame_lexicon['dog'])\n",
    "print(wn_lexicon['dog'])\n",
    "print(ppdb_lexicon['dog'])\n",
    "\n",
    "# Test the French lexicon:\n",
    "print(fr_lexicon['chien'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The frame similarity between cat and feline is: 0.5670\n",
      "The wn similarity between cat and feline is: 0.9337\n",
      "The ppdb similarity between cat and feline is: 0.5670\n",
      "Similarity is changed by 0.0000 between cat and feline\n",
      "Similarity is changed by 0.3668 between cat and feline\n",
      "Similarity is changed by 0.0000 between cat and feline\n",
      "There has been some improvements in the toy corpus, so the retrofit algorithm works! We can move on to find the better hyperparameters.\n",
      "It seems that wn_lexicon is the better lexicon, so we will use it for the rest of the assignment.\n"
     ]
    }
   ],
   "source": [
    "# Retrofit the word vectors using basic hyperparameters:\n",
    "frame_newWordVecs = retrofit(toy_wordVecs, frame_lexicon, numIters = 10, alpha = 1, beta = 2)\n",
    "wn_newWordVecs = retrofit(toy_wordVecs, wn_lexicon, numIters = 10, alpha = 1, beta = 2)\n",
    "ppdb_newWordVecs = retrofit(toy_wordVecs, ppdb_lexicon, numIters = 10, alpha = 1, beta = 2)\n",
    "\n",
    "# Similarity matrix after retrofitting:\n",
    "frame_retro_similarity_matrix = np.zeros((len(vec_toy_corpus), len(vec_toy_corpus)))\n",
    "wn_retro_similarity_matrix = np.zeros((len(vec_toy_corpus), len(vec_toy_corpus)))\n",
    "ppdb_retro_similarity_matrix = np.zeros((len(vec_toy_corpus), len(vec_toy_corpus)))\n",
    "\n",
    "for i in range(len(vec_toy_corpus)):\n",
    "    for j in range(len(vec_toy_corpus)):\n",
    "        frame_retro_similarity_matrix[i][j] = cosine_similarity(frame_newWordVecs[toy_corpus[i]], frame_newWordVecs[toy_corpus[j]])\n",
    "\n",
    "for i in range(len(vec_toy_corpus)):\n",
    "    for j in range(len(vec_toy_corpus)):\n",
    "        wn_retro_similarity_matrix[i][j] = cosine_similarity(wn_newWordVecs[toy_corpus[i]], wn_newWordVecs[toy_corpus[j]])\n",
    "        \n",
    "for i in range(len(vec_toy_corpus)):\n",
    "    for j in range(len(vec_toy_corpus)):\n",
    "        ppdb_retro_similarity_matrix[i][j] = cosine_similarity(ppdb_newWordVecs[toy_corpus[i]], ppdb_newWordVecs[toy_corpus[j]])        \n",
    "# Print the cosine similarity matrix:\n",
    "frame_retrofit_effect_matrix = frame_retro_similarity_matrix - similarity_matrix\n",
    "wn_retrofit_effect_matrix = wn_retro_similarity_matrix - similarity_matrix\n",
    "ppdb_retrofit_effect_matrix = ppdb_retro_similarity_matrix - similarity_matrix\n",
    "\n",
    "print(f\"The frame similarity between cat and feline is: {frame_retro_similarity_matrix[3][6]:.4f}\")\n",
    "print(f\"The wn similarity between cat and feline is: {wn_retro_similarity_matrix[3][6]:.4f}\")\n",
    "print(f\"The ppdb similarity between cat and feline is: {ppdb_retro_similarity_matrix[3][6]:.4f}\")\n",
    "\n",
    "print(f'Similarity is changed by {frame_retrofit_effect_matrix[3][6]:.4f} between {toy_corpus[3]} and {toy_corpus[6]}')\n",
    "print(f'Similarity is changed by {wn_retrofit_effect_matrix[3][6]:.4f} between {toy_corpus[3]} and {toy_corpus[6]}')\n",
    "print(f'Similarity is changed by {ppdb_retrofit_effect_matrix[3][6]:.4f} between {toy_corpus[3]} and {toy_corpus[6]}')\n",
    "\n",
    "print('There has been some improvements in the toy corpus, so the retrofit algorithm works! We can move on to find the better hyperparameters.')\n",
    "print(\"It seems that wn_lexicon is the better lexicon, so we will use it for the rest of the assignment.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implement the algorithm in matrix form."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to implement the retrofitting algorithm in the matrix form, we assumed that the input is the original word embeddings matrix E(w), and the output is the retrofitted word embeddings matrix E'(w). The retrofitting algorithm will be applied to E(w) to get E'(w).\n",
    "\n",
    "To do this, we will have to change the way the lexicon works. At the moment, it is a dictionary with words as keys and their neighbors as values, but to be used in matrix form, the words have to be encoded to access the word embeddings matrix E(w) and E'(w). We encoded the words using a word_ids dictionary and a word_indices list, using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word ids and indices:\n",
    "def build_word_index_mappings(word_vectors):\n",
    "    word_to_index = {}\n",
    "    index_to_word = []\n",
    "    for i, word in enumerate(word_vectors.keys()):\n",
    "        word_to_index[word] = i\n",
    "        index_to_word.append(word)\n",
    "    return word_to_index, index_to_word\n",
    "\n",
    "word_ids, word_indices = build_word_index_mappings(vectors)\n",
    "fr_word_ids, fr_word_indices = build_word_index_mappings(fr_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2121\n",
      "['chase', 'chase_after', 'trail', 'tail', 'tag', 'give_chase', 'go_after', 'track', 'domestic_dog', 'canis_familiaris', 'andiron', 'firedog', 'dog-iron', 'pawl', 'detent', 'click', 'frank', 'frankfurter', 'hotdog', 'hot_dog', 'wiener', 'wienerwurst', 'weenie', 'cad', 'bounder', 'blackguard', 'hound', 'heel', 'frump', 'pursue', 'follow', 'domestic_animal', 'domesticated_animal', 'canine', 'canid', 'support', 'catch', 'stop', 'sausage', 'villain', 'scoundrel', 'chap', 'fellow', 'feller', 'fella', 'lad', 'gent', 'blighter', 'cuss', 'bloke', 'unpleasant_woman', 'disagreeable_woman', 'tree', 'run_down', 'quest', 'hound', 'hunt', 'trace', 'puppy', 'great_pyrenees', 'basenji', 'newfoundland', 'newfoundland_dog', 'lapdog', 'poodle', 'poodle_dog', 'leonberg', 'toy_dog', 'toy', 'spitz', 'pooch', 'doggie', 'doggy', 'barker', 'bow-wow', 'cur', 'mongrel', 'mutt', 'mexican_hairless', 'hunting_dog', 'working_dog', 'dalmatian', 'coach_dog', 'carriage_dog', 'pug', 'pug-dog', 'corgi', 'welsh_corgi', 'griffon', 'brussels_griffon', 'belgian_griffon', 'vienna_sausage', 'perisher']\n",
      "93\n",
      "3136\n",
      "['de', 'prairie', 'rodentia', 'rongeur']\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the lexicon:\n",
    "print(word_ids['dog'])\n",
    "print(wn_lexicon['dog'])\n",
    "print(len(wn_lexicon['dog']))\n",
    "print(fr_word_ids['chien'])\n",
    "print(fr_lexicon['chien'])\n",
    "print(word_ids['('])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dictionary to store the lexicon with word IDs\n",
    "# This dictionary will have as keys the word IDs and as values the list of IDs of the words in the lexicon that are related to the key. The words not present in the vocabulary are ignored.\n",
    "# This is done to ensure that the IDs of the words in the lexicon are the same as the IDs of the words in the vocabulary.\n",
    "wn_lexicon_with_ids = {}\n",
    "\n",
    "# We iterate through the keys of the lexicon, and replace by the word IDs. Skip if not in the vocabulary.\n",
    "for word in wn_lexicon.keys():\n",
    "    if word not in word_ids:\n",
    "        continue\n",
    "    else:\n",
    "        wn_lexicon_with_ids[word_ids[word]] = [word_ids[w] for w in wn_lexicon[word] if w in word_ids.keys()]\n",
    "        \n",
    "# Do the same for french lexicon:\n",
    "fr_lexicon_with_ids = {}\n",
    "\n",
    "for word in fr_lexicon.keys():\n",
    "    if word not in fr_word_ids:\n",
    "        continue\n",
    "    else:\n",
    "        fr_lexicon_with_ids[fr_word_ids[word]] = [fr_word_ids[w] for w in fr_lexicon[word] if w in fr_word_ids.keys()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of neighbors for 'dog' is 53 after removing words not in the vocabulary.\n",
      "The number of neighbors for 'chien' is 3 after removing words not in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of neighbors for 'dog' is {len(wn_lexicon_with_ids[word_ids['dog']])} after removing words not in the vocabulary.\")\n",
    "print(f\"The number of neighbors for 'chien' is {len(fr_lexicon_with_ids[fr_word_ids['chien']])} after removing words not in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will not use the antonyms in this assignment, because it seems to lower the performance of the model in our implementation. But it is an option to improve the similarity for downstream task.\n"
     ]
    }
   ],
   "source": [
    "# Create a new dictionary to store antonyms from WordNet:\n",
    "antonyms = {}\n",
    "\n",
    "# Get the antonyms from WordNet and store them in the dictionary\n",
    "for synset in wn.all_synsets():\n",
    "    for lemma in synset.lemmas():\n",
    "        for antonym in lemma.antonyms():\n",
    "            word = lemma.name()\n",
    "            antonym_word = antonym.name()\n",
    "            if word in word_ids and antonym_word in word_ids:\n",
    "                antonyms[word_ids[word]] = word_ids[antonym_word]\n",
    "print(\"We will not use the antonyms in this assignment, because it seems to lower the performance of the model in our implementation. But it is an option to improve the similarity for downstream task.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3975, 3003, 4884, 2322, 627, 119836, 4063, 1674, 36729, 114002, 20247, 105881, 24026, 19728, 14792, 4720, 1549, 22356, 264, 3558, 958, 18001, 9908, 69568, 25413, 1665, 46170, 42209, 17848, 29516, 105980, 25816, 1946, 6108, 19728, 3445, 7012, 17797, 7859, 43169, 6315, 38974, 52234, 67662, 60178, 9589, 34750, 72559, 36217, 38782, 50014, 60125, 47212]\n",
      "wrong\n",
      "below\n"
     ]
    }
   ],
   "source": [
    "# Test new lexicons:\n",
    "print(wn_lexicon_with_ids.get(word_ids['dog'])) # dog\n",
    "print(word_indices[antonyms.get(word_ids['right'])])\n",
    "print(word_indices[antonyms.get(word_ids['above'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embedding matrix:\n",
    "embedding_matrix = np.zeros((len(word_indices), len(vectors['the'])))\n",
    "fr_embedding_matrix = np.zeros((len(fr_word_indices), len(fr_vectors['le'])))\n",
    "\n",
    "# Fill the embedding matrix with the word vectors from the word2vec model\n",
    "for i in range(len(word_indices)):\n",
    "    embedding_matrix[i] = vectors[word_indices[i]]   \n",
    "    \n",
    "for i in range(len(fr_word_indices)):\n",
    "    fr_embedding_matrix[i] = fr_vectors[fr_word_indices[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neighbor matrix from the new lexicon:\n",
    "# The value for each word is the sum of the embeddings of the words in the lexicon that are related to the word\n",
    "neighbor_matrix = np.zeros((len(word_indices), len(vectors['the']))) # It's a matrix of size (vocab_size, 250)\n",
    "fr_neighbor_matrix = np.zeros((len(fr_word_indices), len(fr_vectors['le']))) # It's a matrix of size (vocab_size, 100)\n",
    "\n",
    "# Fill the neighbor matrix:\n",
    "# For each word in the lexicon, we sum the embeddings of the words in the lexicon that are related to the word\n",
    "for key in wn_lexicon_with_ids.keys():\n",
    "    neighbor_matrix[key] = np.sum(embedding_matrix[wn_lexicon_with_ids[key]], axis = 0)\n",
    "    \n",
    "for key in fr_lexicon_with_ids.keys():\n",
    "    fr_neighbor_matrix[key] = np.sum(fr_embedding_matrix[fr_lexicon_with_ids[key]], axis = 0)\n",
    "\n",
    "# Create an array of number of neighbors for each word:\n",
    "num_neighbors = np.zeros(len(word_indices))\n",
    "for key in wn_lexicon_with_ids.keys():\n",
    "    num_neighbors[key] = len(wn_lexicon_with_ids[key])\n",
    "num_neighbors[num_neighbors == 0] = 1e-6 # To avoid division by 0\n",
    "\n",
    "fr_num_neighbors = np.zeros(len(fr_word_indices))\n",
    "for key in fr_lexicon_with_ids.keys():\n",
    "    fr_num_neighbors[key] = len(fr_lexicon_with_ids[key])\n",
    "fr_num_neighbors[fr_num_neighbors == 0] = 1e-6 # To avoid division by 0\n",
    "\n",
    "# Divide each row of the neighbor matrix by the number of neighbors for the corresponding word\n",
    "neighbor_matrix = neighbor_matrix / num_neighbors[:, None]\n",
    "fr_neighbor_matrix = fr_neighbor_matrix / fr_num_neighbors[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3975, 3003, 4884, 2322, 627, 119836, 4063, 1674, 36729, 114002, 20247, 105881, 24026, 19728, 14792, 4720, 1549, 22356, 264, 3558, 958, 18001, 9908, 69568, 25413, 1665, 46170, 42209, 17848, 29516, 105980, 25816, 1946, 6108, 19728, 3445, 7012, 17797, 7859, 43169, 6315, 38974, 52234, 67662, 60178, 9589, 34750, 72559, 36217, 38782, 50014, 60125, 47212]\n",
      "(53, 250)\n",
      "(250,)\n",
      "53.0\n",
      "\n",
      "\n",
      "[2, 15232, 40253]\n",
      "(3, 100)\n",
      "(100,)\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "# Test the neighbor matrix:\n",
    "print(wn_lexicon_with_ids[word_ids['dog']]) \n",
    "print(embedding_matrix[wn_lexicon_with_ids[word_ids['dog']]].shape) \n",
    "print(neighbor_matrix[word_ids['dog']].shape)\n",
    "print(num_neighbors[word_ids['dog']]) \n",
    "print('\\n')\n",
    "print(fr_lexicon_with_ids[fr_word_ids['chien']]) \n",
    "print(fr_embedding_matrix[fr_lexicon_with_ids[fr_word_ids['chien']]].shape)\n",
    "print(fr_neighbor_matrix[fr_word_ids['chien']].shape)\n",
    "print(fr_num_neighbors[fr_word_ids['chien']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an antonym matrix:\n",
    "antonym_matrix = np.zeros((len(word_indices), 250))\n",
    "\n",
    "# Fill the antonym matrix. This is assuming that each word has only one antonym:\n",
    "#for key in antonyms.keys():\n",
    "    #antonym_matrix[key] = embedding_matrix[antonyms[key]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the retrofitting algorithm, using recommended hyperparameters:\n",
    "def retrofit_matrix(embedding_matrix, neighbor_matrix, alpha = 1, beta = 1, nb_iters = 10):\n",
    "    new_embedding_matrix = embedding_matrix\n",
    "    for i in np.arange(nb_iters):\n",
    "        new_embedding_matrix = (alpha * new_embedding_matrix  + beta * neighbor_matrix) / (alpha + beta) # If we want to use the antonym matrix, we can subtract from the neighbor matrix\n",
    "        # Implement a stopping criterion: If the norm of the difference between the new and the old embedding matrix is less than 1e-2, stop the algorithm to save computation time.\n",
    "        if np.linalg.norm(new_embedding_matrix - embedding_matrix) < 1e-2:\n",
    "            break\n",
    "    return new_embedding_matrix\n",
    "\n",
    "# Run the algorithm on the embedding matrix using recommended hyperparameters:\n",
    "retrofitted_matrix = retrofit_matrix(embedding_matrix, neighbor_matrix, alpha = 1, beta = 1, nb_iters = 10)\n",
    "fr_retrofitted_matrix = retrofit_matrix(fr_embedding_matrix, fr_neighbor_matrix, alpha = 1, beta = 1, nb_iters = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'diff_vec = np.zeros(250)\\nfor word in word_ids.keys():\\n    if word in wn_lexicon.keys():\\n        diff_vec += (retrofitted_matrix[word_ids[word]] - embedding_matrix[word_ids[word]])\\ndiff_vec = diff_vec / len(word_ids.keys())\\n\\n# Apply the difference vector on the words which were not present in the lexicon:\\nfor word in word_ids.keys():\\n    if word not in wn_lexicon.keys():\\n        retrofitted_matrix[word_ids[word]] = embedding_matrix[word_ids[word]] + diff_vec'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also improve the algorithm further by calculating a general sense vector for all retrofitted vectors, and apply this sense vector on the words which were not present in the lexicon. This is done with the following code:\n",
    "# Calculate the difference vector:\n",
    "\"\"\"diff_vec = np.zeros(250)\n",
    "for word in word_ids.keys():\n",
    "    if word in wn_lexicon.keys():\n",
    "        diff_vec += (retrofitted_matrix[word_ids[word]] - embedding_matrix[word_ids[word]])\n",
    "diff_vec = diff_vec / len(word_ids.keys())\n",
    "\n",
    "# Apply the difference vector on the words which were not present in the lexicon:\n",
    "for word in word_ids.keys():\n",
    "    if word not in wn_lexicon.keys():\n",
    "        retrofitted_matrix[word_ids[word]] = embedding_matrix[word_ids[word]] + diff_vec\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluations.\n",
    "We will evaluate the new retrofitted word embeddings on a lexical similarity task, and a sentiment analysis task.\n",
    "## 4.1. Lexical similarity task.\n",
    "We will use the WS353 and RG65 dataset to evaluate the performance of the retrofitting algorithm on a lexical similarity task. The WS353 dataset contains 353 pairs of words, RG65 contains 65 pairs, and each pair is assigned a similarity score by human annotators. The similarity score ranges from 0 to 10, with 0 being not similar at all, and 10 being very similar. We will use the cosine similarity between the word embeddings of each pair of words to calculate the similarity score, and then compare it with the human similarity score to see how well the retrofitting algorithm performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the WordSim353 and RG65 datasets as pandas dataframes:\n",
    "# It is done with pandas for ease of manipulation, and also ease of calculation of the correlation between the human scores and the cosine similarities\n",
    "# Because the formats of two datasets are different, I read them separately\n",
    "def read_ws353():\n",
    "    with open('ws353.txt', 'r') as f:\n",
    "        data = f.read()\n",
    "    lines = data.split('\\n')\n",
    "    lines = lines[0:-1]\n",
    "    word1 = []\n",
    "    word2 = []\n",
    "    human_scores = []\n",
    "    for line in lines:\n",
    "        tokens = line.split('\\t')\n",
    "        word1.append(tokens[0])\n",
    "        word2.append(tokens[1])\n",
    "        human_scores.append(float(tokens[2]))\n",
    "    df = pd.DataFrame({'word1': word1, 'word2': word2, 'score': human_scores})\n",
    "\n",
    "    return df\n",
    "\n",
    "ws353_data = read_ws353()\n",
    "\n",
    "def read_rg65():\n",
    "    with open('rg65_french.txt', 'r', encoding = 'utf-8') as f:\n",
    "        data = f.read()\n",
    "    lines = data.split('\\n')\n",
    "    lines = lines[0:-1]\n",
    "    word1 = []\n",
    "    word2 = []\n",
    "    human_scores = []\n",
    "    for line in lines:\n",
    "        tokens = line.split(' ')\n",
    "        word1.append(tokens[0])\n",
    "        word2.append(tokens[1])\n",
    "        human_scores.append(float(tokens[2]))\n",
    "    df = pd.DataFrame({'word1': word1, 'word2': word2, 'score': human_scores})\n",
    "\n",
    "    return df\n",
    "\n",
    "rg65_data = read_rg65()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WS353 dataset:\n",
      "            word1     word2  score\n",
      "0            love       sex   6.77\n",
      "1           tiger       cat   7.35\n",
      "2           tiger     tiger  10.00\n",
      "3            book     paper   7.46\n",
      "4        computer  keyboard   7.62\n",
      "..            ...       ...    ...\n",
      "348        shower     flood   6.03\n",
      "349       weather  forecast   8.34\n",
      "350      disaster      area   6.25\n",
      "351      governor    office   6.34\n",
      "352  architecture   century   3.78\n",
      "\n",
      "[353 rows x 3 columns]\n",
      "RG65 dataset:\n",
      "         word1      word2  score\n",
      "0        corde    sourire   0.00\n",
      "1         midi    ficelle   0.00\n",
      "2          coq    périple   0.06\n",
      "3        fruit  fournaise   0.11\n",
      "4   autographe     rivage   0.00\n",
      "..         ...        ...    ...\n",
      "60     coussin   oreiller   3.00\n",
      "61   cimetière  cimetière   4.00\n",
      "62  automobile       auto   3.94\n",
      "63       joyau      bijou   3.22\n",
      "64        midi      dîner   2.17\n",
      "\n",
      "[65 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"WS353 dataset:\")\n",
    "print(ws353_data)\n",
    "print(\"RG65 dataset:\")\n",
    "print(rg65_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to compute the cosine similarity between two words:\n",
    "def emb_cosine_similarity(word1, word2, embedding_matrix, word_ids):\n",
    "    if word1 not in word_ids or word2 not in word_ids:\n",
    "        return None\n",
    "    word1_embedding = embedding_matrix[word_ids[word1]]\n",
    "    word2_embedding = embedding_matrix[word_ids[word2]]\n",
    "    return np.dot(word1_embedding, word2_embedding) / (np.linalg.norm(word1_embedding) * np.linalg.norm(word2_embedding))\n",
    "\n",
    "# Calculate the score for each pair of words in the dataset:\n",
    "ws353_data['original_score'] = ws353_data.apply(lambda row: emb_cosine_similarity(row['word1'], row['word2'], embedding_matrix, word_ids), axis = 1)\n",
    "ws353_data['retrofitted_score'] = ws353_data.apply(lambda row: emb_cosine_similarity(row['word1'], row['word2'], retrofitted_matrix, word_ids), axis = 1)\n",
    "\n",
    "# Calculate the score for each pair of words in the French dataset:\n",
    "rg65_data['original_score'] = rg65_data.apply(lambda row: emb_cosine_similarity(row['word1'], row['word2'], fr_embedding_matrix, fr_word_ids), axis = 1)\n",
    "rg65_data['retrofitted_score'] = rg65_data.apply(lambda row: emb_cosine_similarity(row['word1'], row['word2'], fr_retrofitted_matrix, fr_word_ids), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the scores for English original and retrofitted vectors against WS353 scores:\n",
      "                      score  original_score  retrofitted_score\n",
      "score              1.000000        0.690526           0.207655\n",
      "original_score     0.690526        1.000000           0.202783\n",
      "retrofitted_score  0.207655        0.202783           1.000000\n",
      "These are the scores for French original and retrofitted vectors against RG65 scores:\n",
      "                      score  original_score  retrofitted_score\n",
      "score              1.000000        0.704197           0.435315\n",
      "original_score     0.704197        1.000000           0.244096\n",
      "retrofitted_score  0.435315        0.244096           1.000000\n",
      "This is not a good result, because the correlations between the human scores and the retrofitted scores are much lower than the original vectors. \n",
      "We need to find better hyperparameters for the retrofitting algorithm.\n"
     ]
    }
   ],
   "source": [
    "# Because there are some words in the dataset that are not in the vocabulary, we need to remove them from the dataset before calculation of the correlation:\n",
    "ws353_data = ws353_data.dropna()\n",
    "rg65_data = rg65_data.dropna()\n",
    "\n",
    "# Calculate the Spearman correlation between the human scores and the model scores:\n",
    "print(\"These are the scores for English original and retrofitted vectors against WS353 scores:\")\n",
    "print(ws353_data[['score', 'original_score', 'retrofitted_score']].corr(method = 'spearman'))\n",
    "print(\"These are the scores for French original and retrofitted vectors against RG65 scores:\")\n",
    "print(rg65_data[['score', 'original_score', 'retrofitted_score']].corr(method = 'spearman'))\n",
    "print(\"This is not a good result, because the correlations between the human scores and the retrofitted scores are much lower than the original vectors. \\nWe need to find better hyperparameters for the retrofitting algorithm.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of iterations was intentionally kept small due to heavy computation. The jury can modify the range of iterations in the 3 loops, but we recommend keeping as-is to avoid long waiting time. The results for 500 iterations were already included in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters are alpha = 4.1, beta = 1.1, and numIters = 1, with the Spearman correlation score of 0.696914 for the English set.\n"
     ]
    }
   ],
   "source": [
    "# We can use the lexical similarity task to test for better hyperparameters for the English data, using exhaustive search:\n",
    "# We iterate over the hyperparameters alpha, beta and the number of iterations:\n",
    "scores = []\n",
    "best_score = 0\n",
    "for alpha in np.arange(0.1, 5.1, 1):\n",
    "    for beta in np.arange(0.1, 5.1, 1):\n",
    "        for numIters in range(1, 5):\n",
    "            # For each set of hyperparameters, we retrofit the embedding matrix and calculate the correlation between the human scores and the model scores:\n",
    "            retrofitted_matrix = retrofit_matrix(embedding_matrix, neighbor_matrix, alpha = alpha, beta = beta, nb_iters = numIters)\n",
    "            \n",
    "            # Because using emb_cosine_similarity() is too slow, we use the following method to calculate the cosine similarity:\n",
    "            word1_index = ws353_data['word1'].apply(lambda x: word_ids[x])\n",
    "            word2_index = ws353_data['word2'].apply(lambda x: word_ids[x])\n",
    "            word1_emb = retrofitted_matrix[word1_index]\n",
    "            word2_emb = retrofitted_matrix[word2_index]\n",
    "            ws353_data['retrofitted_score'] = np.sum(word1_emb * word2_emb, axis=1) / (np.linalg.norm(word1_emb, axis=1) * np.linalg.norm(word2_emb, axis=1))\n",
    "            \n",
    "            # Calculate the Spearman correlation between the human scores and the retrofitted scores:\n",
    "            spearman_score = ws353_data['score'].corr(ws353_data['retrofitted_score'], method = 'spearman')\n",
    "            \n",
    "            # Store the hyperparameters and the correlation score:\n",
    "            scores.append((alpha, beta, numIters, spearman_score))\n",
    "            \n",
    "            # Update the best hyperparameters and the best score so far:\n",
    "            if spearman_score > best_score:\n",
    "                best_score = spearman_score\n",
    "                best_alpha = alpha\n",
    "                best_beta = beta\n",
    "                best_numIters = numIters\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(f\"The best hyperparameters are alpha = {best_alpha}, beta = {best_beta}, and numIters = {best_numIters}, with the Spearman correlation score of {best_score:0.6f} for the English set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters are alpha = 2.1, beta = 1.1, and numIters = 1, with the Spearman correlation score of 0.778943 for the French set.\n"
     ]
    }
   ],
   "source": [
    "# We can use the lexical similarity task to test for better hyperparameters for the French data, using exhaustive search:\n",
    "# Same method as above, but for the French data:\n",
    "fr_scores = []\n",
    "fr_best_score = 0\n",
    "for alpha in np.arange(0.1, 5.1, 1):\n",
    "    for beta in np.arange(0.1, 5.1, 1):\n",
    "        for numIters in range(1, 5):\n",
    "            # Aply the retrofitting algorithm to the French data:\n",
    "            retrofitted_matrix = retrofit_matrix(fr_embedding_matrix, fr_neighbor_matrix, alpha = alpha, beta = beta, nb_iters = numIters)\n",
    "            \n",
    "            # Calculate the cosine similarity between the words in the dataset:\n",
    "            word1_index = rg65_data['word1'].apply(lambda x: fr_word_ids[x] if x in fr_word_ids else None)\n",
    "            word2_index = rg65_data['word2'].apply(lambda x: fr_word_ids[x] if x in fr_word_ids else None)\n",
    "            word1_emb = retrofitted_matrix[word1_index]\n",
    "            word2_emb = retrofitted_matrix[word2_index]\n",
    "            rg65_data['retrofitted_score'] = np.sum(word1_emb * word2_emb, axis=1) / (np.linalg.norm(word1_emb, axis=1) * np.linalg.norm(word2_emb, axis=1))\n",
    "            \n",
    "            \n",
    "            # Calculate the Spearman correlation between the human scores and the retrofitted scores:\n",
    "            spearman_score = rg65_data['score'].corr(rg65_data['retrofitted_score'], method = 'spearman')\n",
    "            fr_scores.append((alpha, beta, numIters, spearman_score))\n",
    "            if spearman_score > fr_best_score:\n",
    "                fr_best_score = spearman_score\n",
    "                fr_best_alpha = alpha\n",
    "                fr_best_beta = beta\n",
    "                fr_best_numIters = numIters\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(f\"The best hyperparameters are alpha = {fr_best_alpha}, beta = {fr_best_beta}, and numIters = {fr_best_numIters}, with the Spearman correlation score of {fr_best_score:0.6f} for the French set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters are alpha = 4.1, beta = 1.1, and numIters = 1, \n",
      "with the Spearman correlation score of 0.696914 for the English set.\n",
      "The best hyperparameters are alpha = 2.1, beta = 1.1, and numIters = 1, \n",
      "with the Spearman correlation score of 0.778943 for the French set.\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the results:\n",
    "print(f\"The best hyperparameters are alpha = {best_alpha}, beta = {best_beta}, and numIters = {best_numIters}, \\nwith the Spearman correlation score of {best_score:0.6f} for the English set.\")\n",
    "print(f\"The best hyperparameters are alpha = {fr_best_alpha}, beta = {fr_best_beta}, and numIters = {fr_best_numIters}, \\nwith the Spearman correlation score of {fr_best_score:0.6f} for the French set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Spearman correlation score for the English set is 0.696914 \n",
      "with the following hyperparameters: alpha = 4.1, beta = 1.1, numIters = 1.\n",
      "                      score  original_score  retrofitted_score\n",
      "score              1.000000        0.690526           0.696914\n",
      "original_score     0.690526        1.000000           0.990057\n",
      "retrofitted_score  0.696914        0.990057           1.000000\n",
      "The best Spearman correlation score for the French set is 0.778943 \n",
      "with the following hyperparameters: alpha = 2.1, beta = 1.1, numIters = 1.\n",
      "                      score  original_score  retrofitted_score\n",
      "score              1.000000        0.704197           0.778943\n",
      "original_score     0.704197        1.000000           0.925019\n",
      "retrofitted_score  0.778943        0.925019           1.000000\n"
     ]
    }
   ],
   "source": [
    "# Apply the best hyperparameters to the English data:\n",
    "retrofitted_matrix = retrofit_matrix(embedding_matrix, neighbor_matrix, alpha = best_alpha, beta = best_beta, nb_iters = best_numIters)\n",
    "word1_index = ws353_data['word1'].apply(lambda x: word_ids[x])\n",
    "word2_index = ws353_data['word2'].apply(lambda x: word_ids[x])\n",
    "word1_emb = retrofitted_matrix[word1_index]\n",
    "word2_emb = retrofitted_matrix[word2_index]\n",
    "ws353_data['retrofitted_score'] = np.sum(word1_emb * word2_emb, axis=1) / (np.linalg.norm(word1_emb, axis=1) * np.linalg.norm(word2_emb, axis=1))\n",
    "spearman_score = ws353_data['score'].corr(ws353_data['retrofitted_score'], method = 'spearman')\n",
    "print(f\"The best Spearman correlation score for the English set is {spearman_score:0.6f} \\nwith the following hyperparameters: alpha = {best_alpha}, beta = {best_beta}, numIters = {best_numIters}.\")\n",
    "print(ws353_data[['score', 'original_score', 'retrofitted_score']].corr(method = 'spearman'))\n",
    "\n",
    "# Apply the best hyperparameters to the French data:\n",
    "fr_retrofitted_matrix = retrofit_matrix(fr_embedding_matrix, fr_neighbor_matrix, alpha = fr_best_alpha, beta = fr_best_beta, nb_iters = fr_best_numIters)\n",
    "word1_index = rg65_data['word1'].apply(lambda x: fr_word_ids[x] if x in fr_word_ids else None)\n",
    "word2_index = rg65_data['word2'].apply(lambda x: fr_word_ids[x] if x in fr_word_ids else None)\n",
    "word1_emb = fr_retrofitted_matrix[word1_index]\n",
    "word2_emb = fr_retrofitted_matrix[word2_index]\n",
    "rg65_data['retrofitted_score'] = np.sum(word1_emb * word2_emb, axis=1) / (np.linalg.norm(word1_emb, axis=1) * np.linalg.norm(word2_emb, axis=1))\n",
    "fr_spearman_score = rg65_data['score'].corr(rg65_data['retrofitted_score'], method = 'spearman')\n",
    "print(f\"The best Spearman correlation score for the French set is {fr_spearman_score:0.6f} \\nwith the following hyperparameters: alpha = {fr_best_alpha}, beta = {fr_best_beta}, numIters = {fr_best_numIters}.\")\n",
    "print(rg65_data[['score', 'original_score', 'retrofitted_score']].corr(method = 'spearman'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing down the vectors in retrofitted_vectors.txt\n",
      "\n",
      "Writing down the vectors in fr_retrofitted_vectors.txt\n"
     ]
    }
   ],
   "source": [
    "# We can also print the output of the retrofitting algorithm to text files for other tasks:\n",
    "# We choose not to run it due to large output filesize.\n",
    "# Convert the retrofitted matrix to a dictionary:\n",
    "def convert_matrix_to_dict(matrix, word_ids):\n",
    "    word_vectors = {}\n",
    "    for word, index in word_ids.items():\n",
    "        word_vectors[word] = matrix[index]\n",
    "    return word_vectors\n",
    "\n",
    "retrofitted_vectors = convert_matrix_to_dict(retrofitted_matrix, word_ids)\n",
    "fr_retrofitted_vectors = convert_matrix_to_dict(fr_retrofitted_matrix, fr_word_ids)\n",
    "\n",
    "''' Write word vectors to file '''\n",
    "def print_word_vecs(wordVectors, outFileName):\n",
    "  sys.stderr.write('\\nWriting down the vectors in '+outFileName+'\\n')\n",
    "  outFile = open(outFileName, 'w', encoding = 'utf-8')  \n",
    "  for word, values in wordVectors.items():\n",
    "    outFile.write(word+' ')\n",
    "    for val in wordVectors[word]:\n",
    "      outFile.write('%.6f' %(val)+' ')\n",
    "    outFile.write('\\n')      \n",
    "  outFile.close()\n",
    "  \n",
    "#print_word_vecs(retrofitted_vectors, 'retrofitted_vectors.txt')\n",
    "#print_word_vecs(fr_retrofitted_vectors, 'fr_retrofitted_vectors.txt')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Sentiment analysis task.\n",
    "We will use the Stanford Sentiment Treebank dataset to evaluate the performance of the retrofitting algorithm on a sentiment analysis task. The Stanford Sentiment Treebank dataset contains 11,855 sentences, and each sentence is assigned a sentiment score by human annotators. The sentiment score ranges between -1 and 1, with -1 as negative and 1 as positive. \n",
    "\n",
    "The two word embeddings will be used in a frozen way. We will train 2 neural network using the sentences vectorized by the two embeddings as input, and the sentiment scores as output. Then we will compare the predicted sentiment scores with original embeddings with the predicted sentiment scores with retrofitted embeddings to see how well the retrofitting algorithm performs. For this task, we will use accuracy as a metric to evaluate the performance of the retrofitting algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset from Stanford Sentiment Treebank:\n",
    "def read_sst(filename):\n",
    "    # Open the gzip file\n",
    "    with gzip.open(filename, 'rt', encoding='utf-8') as f:\n",
    "        # Read the data and split it into lines\n",
    "        data = f.read()\n",
    "        lines = data.split('\\n')\n",
    "        lines = lines[0:-1]\n",
    "        \n",
    "        # Initialize lists to store sentences and labels\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        \n",
    "        # Loop over each line\n",
    "        for line in lines:\n",
    "            # Skip the header line\n",
    "            if line.startswith('1') or line.startswith('-1'):\n",
    "                # Split the line into tokens using space as the delimiter\n",
    "                tokens = line.split(' ', 1)\n",
    "            \n",
    "                # Check if the tokens list contains at least two elements\n",
    "                if len(tokens) >= 2:\n",
    "                    # Append the label and sentence to the respective lists\n",
    "                    labels.append(int(tokens[0]))\n",
    "                    sentences.append(tokens[1])\n",
    "        \n",
    "        # Create a DataFrame from the sentences and labels\n",
    "        df = pd.DataFrame({'sentence': sentences, 'label': labels})\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Read the dataset\n",
    "dataset = read_sst('stanford_sentiment_analysis.tar.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to preprocess the sentences:\n",
    "def preprocess_sentence(sentence):\n",
    "    # Remove the HTML tags\n",
    "    sentence = re.sub(r'<[^>]+>', '', sentence)\n",
    "    \n",
    "    # Remove any non-alphanumeric characters (except spaces)\n",
    "    sentence = re.sub(r'[^a-zA-Z0-9 ]', '', sentence)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # Return the preprocessed sentence\n",
    "    return sentence\n",
    "\n",
    "# Preprocess the sentences\n",
    "dataset['sentence'] = dataset['sentence'].apply(preprocess_sentence)\n",
    "\n",
    "# Create a function to tokenize the sentences:\n",
    "def tokenize_sentence(sentence):\n",
    "    # Split the sentence into tokens\n",
    "    tokens = sentence.split(' ')\n",
    "    \n",
    "    # Return the list of tokens\n",
    "    return tokens\n",
    "\n",
    "# Tokenize the sentences\n",
    "dataset['tokens'] = dataset['sentence'].apply(tokenize_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the tokens using the embedding matrix:\n",
    "def vectorize_tokens(tokens, embedding_matrix):\n",
    "    # Initialize the vector\n",
    "    vectors = np.zeros((len(tokens), 250))\n",
    "    \n",
    "    # Loop over the tokens\n",
    "    for i in range(len(tokens)):\n",
    "        # Check if the token is in the vocabulary\n",
    "        if tokens[i] in word_ids:\n",
    "            # Get the index of the token in the vocabulary\n",
    "            index = word_ids[tokens[i]]\n",
    "            \n",
    "            # Get the vector of the token from the embedding matrix\n",
    "            vectors[i] = embedding_matrix[index]\n",
    "    \n",
    "    # Return the vector\n",
    "    return vectors\n",
    "\n",
    "# Vectorize the tokens\n",
    "dataset['original_vectors'] = dataset['tokens'].apply(lambda tokens: vectorize_tokens(tokens, embedding_matrix))\n",
    "dataset['retrofitted_vectors'] = dataset['tokens'].apply(lambda tokens: vectorize_tokens(tokens, retrofitted_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35, 250)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataset['tokens'][0]))\n",
    "dataset['original_vectors'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the vector for each sentence by averaging the vectors of the tokens:\n",
    "def calculate_sentence_vector(vectors):\n",
    "    # Calculate the average of all token vectors\n",
    "    vector = np.mean(vectors, axis = 0)\n",
    "    # Return the vector\n",
    "    return vector\n",
    "\n",
    "# Calculate the sentence vectors\n",
    "dataset['ori_sentence_vector'] = dataset['original_vectors'].apply(calculate_sentence_vector) \n",
    "dataset['retro_sentence_vector'] = dataset['retrofitted_vectors'].apply(calculate_sentence_vector)\n",
    "dataset['label'] = dataset['label'].apply(lambda x: 1 if x == 1 else 0) # Convert the labels to 0 and 1\n",
    "\n",
    "# Dropping the columns that are not needed:\n",
    "dataset = dataset.drop(['sentence', 'tokens', 'original_vectors', 'retrofitted_vectors'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example sets for the original and retrofitted datasets:\n",
    "dataset_original = [(dataset['ori_sentence_vector'][i], dataset['label'][i]) for i in range(len(dataset))]\n",
    "dataset_retrofitted = [(dataset['retro_sentence_vector'][i], dataset['label'][i]) for i in range(len(dataset))]\n",
    "\n",
    "# Split the dataset into train, dev, test sets:\n",
    "from sklearn.model_selection import train_test_split\n",
    "ori_train, ori_temp = train_test_split(dataset_original, test_size = 0.2, random_state = 0)\n",
    "ori_dev, ori_test = train_test_split(ori_temp, test_size = 0.5, random_state = 0)\n",
    "\n",
    "retro_train, retro_temp = train_test_split(dataset_retrofitted, test_size = 0.2, random_state = 0)\n",
    "retro_dev, retro_test = train_test_split(retro_temp, test_size = 0.5, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple one hidden layer neural network:\n",
    "class SentimentAnalysis(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, early_stop = False):\n",
    "        super(SentimentAnalysis, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_dim, hidden_dim) #[250, 100]\n",
    "        self.linear2 = torch.nn.Linear(hidden_dim, output_dim) #[100, 2]\n",
    "        self.sigmoid = torch.nn.Sigmoid() # Sigmoid for binary classification\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.early_stop = early_stop\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x) # [batch_size, 250] . [250, 100] = [batch_size, 100]\n",
    "        out = self.activation(out) # [batch_size, 100]\n",
    "        out = self.linear2(out) # [batch_size, 100] . [100, 2] = [batch_size, 2]\n",
    "        out = self.sigmoid(out) # [batch_size, 2]\n",
    "        return out\n",
    "\n",
    "# Create the model for original embeddings:\n",
    "model_original = SentimentAnalysis(250, 100, 2, False)\n",
    "# Create the model for retrofitted embeddings:\n",
    "model_retrofitted = SentimentAnalysis(250, 100, 2, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer:\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer_original = torch.optim.Adam(model_original.parameters(), lr = 0.001)\n",
    "optimizer_retrofitted = torch.optim.Adam(model_retrofitted.parameters(), lr = 0.001)\n",
    "\n",
    "from random import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the original model...\n",
      "Epoch: 0\n",
      "Loss on training set at epoch 0 : 152.963238\n",
      "Epoch: 1\n",
      "Loss on training set at epoch 1 : 133.407324\n",
      "Epoch: 2\n",
      "Loss on training set at epoch 2 : 127.785907\n",
      "Epoch: 3\n",
      "Loss on training set at epoch 3 : 125.203592\n",
      "Epoch: 4\n",
      "Loss on training set at epoch 4 : 123.884114\n",
      "Epoch: 5\n",
      "Loss on training set at epoch 5 : 123.177697\n",
      "Epoch: 6\n",
      "Loss on training set at epoch 6 : 122.388628\n",
      "Epoch: 7\n",
      "Loss on training set at epoch 7 : 122.023272\n",
      "Epoch: 8\n",
      "Loss on training set at epoch 8 : 121.685067\n",
      "Epoch: 9\n",
      "Loss on training set at epoch 9 : 120.998787\n",
      "Epoch: 10\n",
      "Loss on training set at epoch 10 : 120.634693\n",
      "Epoch: 11\n",
      "Loss on training set at epoch 11 : 120.782156\n",
      "Epoch: 12\n",
      "Loss on training set at epoch 12 : 120.708427\n",
      "Epoch: 13\n",
      "Loss on training set at epoch 13 : 120.022386\n",
      "Epoch: 14\n",
      "Loss on training set at epoch 14 : 120.051519\n",
      "Epoch: 15\n",
      "Loss on training set at epoch 15 : 120.217447\n",
      "Epoch: 16\n",
      "Loss on training set at epoch 16 : 119.535458\n",
      "Epoch: 17\n",
      "Loss on training set at epoch 17 : 119.580836\n",
      "Epoch: 18\n",
      "Loss on training set at epoch 18 : 118.896684\n",
      "Epoch: 19\n",
      "Loss on training set at epoch 19 : 118.874952\n",
      "Epoch: 20\n",
      "Loss on training set at epoch 20 : 119.011008\n",
      "Epoch: 21\n",
      "Loss on training set at epoch 21 : 118.385179\n",
      "Epoch: 22\n",
      "Loss on training set at epoch 22 : 118.609419\n",
      "Epoch: 23\n",
      "Loss on training set at epoch 23 : 118.275800\n",
      "Epoch: 24\n",
      "Loss on training set at epoch 24 : 118.289544\n",
      "Epoch: 25\n",
      "Loss on training set at epoch 25 : 117.880722\n",
      "Epoch: 26\n",
      "Loss on training set at epoch 26 : 117.749887\n",
      "Epoch: 27\n",
      "Loss on training set at epoch 27 : 117.379028\n",
      "Epoch: 28\n",
      "Loss on training set at epoch 28 : 117.233798\n",
      "Epoch: 29\n",
      "Loss on training set at epoch 29 : 117.258344\n",
      "Epoch: 30\n",
      "Loss on training set at epoch 30 : 117.333502\n",
      "Epoch: 31\n",
      "Loss on training set at epoch 31 : 117.173519\n",
      "Epoch: 32\n",
      "Loss on training set at epoch 32 : 116.854946\n",
      "Epoch: 33\n",
      "Loss on training set at epoch 33 : 116.647452\n",
      "Epoch: 34\n",
      "Loss on training set at epoch 34 : 116.686486\n",
      "Epoch: 35\n",
      "Loss on training set at epoch 35 : 116.416975\n",
      "Epoch: 36\n",
      "Loss on training set at epoch 36 : 116.491554\n",
      "Epoch: 37\n",
      "Loss on training set at epoch 37 : 116.179523\n",
      "Epoch: 38\n",
      "Loss on training set at epoch 38 : 115.980226\n",
      "Epoch: 39\n",
      "Loss on training set at epoch 39 : 115.939564\n",
      "Epoch: 40\n",
      "Loss on training set at epoch 40 : 115.879156\n",
      "Epoch: 41\n",
      "Loss on training set at epoch 41 : 115.439404\n",
      "Epoch: 42\n",
      "Loss on training set at epoch 42 : 115.352470\n",
      "Epoch: 43\n",
      "Loss on training set at epoch 43 : 115.370930\n",
      "Epoch: 44\n",
      "Loss on training set at epoch 44 : 115.422247\n",
      "Epoch: 45\n",
      "Loss on training set at epoch 45 : 115.013370\n",
      "Epoch: 46\n",
      "Loss on training set at epoch 46 : 114.688891\n",
      "Epoch: 47\n",
      "Loss on training set at epoch 47 : 114.790239\n",
      "Epoch: 48\n",
      "Loss on training set at epoch 48 : 115.118332\n",
      "Epoch: 49\n",
      "Loss on training set at epoch 49 : 114.683259\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training the original model:\n",
    "train_losses = []\n",
    "print('Training the original model...')\n",
    "for epoch in range(50):\n",
    "    print('Epoch:', epoch)\n",
    "    epoch_loss = 0\n",
    "    shuffle(ori_train)\n",
    "    i = 0\n",
    "    # Process the training data in batches of 32:\n",
    "    while i < len(ori_train):\n",
    "        batch = ori_train[i:i+32]\n",
    "        i += 32\n",
    "        # Get the sentence vectors and labels from the batch:\n",
    "        sentence, label = zip(*batch)\n",
    "        \n",
    "        # Step 1: Process the input data:\n",
    "        sentence = torch.Tensor(sentence) # [batch_size, 250]\n",
    "        label = torch.LongTensor(label) # [batch_size]\n",
    "        \n",
    "        # Step 2: Zero the gradients:\n",
    "        model_original.zero_grad()\n",
    "        \n",
    "        # Step 3: Run the forward propagation:\n",
    "        output = model_original(sentence)\n",
    "        \n",
    "        # Step 4: Compute loss and gradients:\n",
    "        loss = loss_function(output, label)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step 5: Update the weights:\n",
    "        optimizer_original.step()\n",
    "    # end of handling of this batch\n",
    "    print(\"Loss on training set at epoch %d : %f\" %(epoch, epoch_loss))\n",
    "    train_losses.append(epoch_loss)    \n",
    "    # Step 6: Early stopping:\n",
    "    with torch.no_grad():\n",
    "        if model_original.early_stop:\n",
    "            # forward propagation\n",
    "            # We convert the test examples to tensors and do the forward propagation\n",
    "            dev_sentence, dev_label = zip(*ori_dev)\n",
    "            dev_sentence = torch.tensor(dev_sentence, dtype=torch.float32) # [DEV_SIZE, CONTEXT_SIZE]\n",
    "            dev_label = torch.tensor(dev_label, dtype=torch.long)   # [DEV_SIZE]        \n",
    "            output = model_original(dev_sentence)    \n",
    "                \n",
    "            # total loss on the dev set\n",
    "            dev_loss = loss_function(output, dev_label)\n",
    "            print(\"Loss on dev set at epoch %d: %f\\n\" %(epoch, dev_loss))\n",
    "                \n",
    "            # prediction and accuracy on the dev set\n",
    "            pred_labels = torch.argmax(output, dim=1)\n",
    "            accuracy = torch.sum(pred_labels == dev_label).item() / len(dev_label)\n",
    "            print(\"Accuracy on dev, after epoch %d: %3.2f\\n\" % (epoch, accuracy * 100))\n",
    "                \n",
    "            # early stopping\n",
    "            # if first epoch: we record the dev loss, to be used for early stopping\n",
    "            if epoch == 0:\n",
    "                previous_dev_loss = dev_loss\n",
    "            elif dev_loss > previous_dev_loss:\n",
    "                print(\"Loss on dev has increased, we stop training!\")\n",
    "                break\n",
    "            else:\n",
    "                previous_dev_loss = dev_loss\n",
    "# Plot the training losses\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training losses over Epochs\")\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of the original model:\n",
      "Loss on test set after training: 0.513886\n",
      "\n",
      "Accuracy on test set after training: 78.46\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on test set after training:\n",
    "with torch.no_grad():\n",
    "    # forward propagation on test set\n",
    "    test_sentence, test_label = zip(*ori_test)\n",
    "    test_sentence = torch.tensor(test_sentence, dtype=torch.float32) # [DEV_SIZE, CONTEXT_SIZE]\n",
    "    test_label = torch.tensor(test_label, dtype=torch.long)   # [DEV_SIZE]\n",
    "    log_probs = model_original(test_sentence)\n",
    "    \n",
    "    # total loss on the test set\n",
    "    test_loss = loss_function(log_probs, test_label)\n",
    "\n",
    "            \n",
    "    # prediction and accuracy on the dev set\n",
    "    pred_labels = torch.argmax(log_probs, dim=1)\n",
    "    accuracy = torch.sum(pred_labels == test_label).item() / len(test_label)\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"Result of the original model:\")\n",
    "    print(\"Loss on test set after training: %f\\n\" %(test_loss))\n",
    "    print(\"Accuracy on test set after training: %3.2f\\n\" % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the retrofitted model...\n",
      "Epoch: 0\n",
      "Loss on training set at epoch 0 : 154.794712\n",
      "Epoch: 1\n",
      "Loss on training set at epoch 1 : 135.021960\n",
      "Epoch: 2\n",
      "Loss on training set at epoch 2 : 128.396666\n",
      "Epoch: 3\n",
      "Loss on training set at epoch 3 : 125.876333\n",
      "Epoch: 4\n",
      "Loss on training set at epoch 4 : 124.453717\n",
      "Epoch: 5\n",
      "Loss on training set at epoch 5 : 123.602061\n",
      "Epoch: 6\n",
      "Loss on training set at epoch 6 : 122.795972\n",
      "Epoch: 7\n",
      "Loss on training set at epoch 7 : 122.172147\n",
      "Epoch: 8\n",
      "Loss on training set at epoch 8 : 121.914055\n",
      "Epoch: 9\n",
      "Loss on training set at epoch 9 : 121.597004\n",
      "Epoch: 10\n",
      "Loss on training set at epoch 10 : 121.193973\n",
      "Epoch: 11\n",
      "Loss on training set at epoch 11 : 120.587117\n",
      "Epoch: 12\n",
      "Loss on training set at epoch 12 : 120.353635\n",
      "Epoch: 13\n",
      "Loss on training set at epoch 13 : 120.398037\n",
      "Epoch: 14\n",
      "Loss on training set at epoch 14 : 120.545699\n",
      "Epoch: 15\n",
      "Loss on training set at epoch 15 : 120.438198\n",
      "Epoch: 16\n",
      "Loss on training set at epoch 16 : 119.537918\n",
      "Epoch: 17\n",
      "Loss on training set at epoch 17 : 119.385987\n",
      "Epoch: 18\n",
      "Loss on training set at epoch 18 : 119.460342\n",
      "Epoch: 19\n",
      "Loss on training set at epoch 19 : 119.465104\n",
      "Epoch: 20\n",
      "Loss on training set at epoch 20 : 119.119345\n",
      "Epoch: 21\n",
      "Loss on training set at epoch 21 : 118.942724\n",
      "Epoch: 22\n",
      "Loss on training set at epoch 22 : 119.203874\n",
      "Epoch: 23\n",
      "Loss on training set at epoch 23 : 118.520134\n",
      "Epoch: 24\n",
      "Loss on training set at epoch 24 : 119.286592\n",
      "Epoch: 25\n",
      "Loss on training set at epoch 25 : 118.551525\n",
      "Epoch: 26\n",
      "Loss on training set at epoch 26 : 118.494834\n",
      "Epoch: 27\n",
      "Loss on training set at epoch 27 : 118.361628\n",
      "Epoch: 28\n",
      "Loss on training set at epoch 28 : 118.159671\n",
      "Epoch: 29\n",
      "Loss on training set at epoch 29 : 118.057593\n",
      "Epoch: 30\n",
      "Loss on training set at epoch 30 : 117.896182\n",
      "Epoch: 31\n",
      "Loss on training set at epoch 31 : 117.539983\n",
      "Epoch: 32\n",
      "Loss on training set at epoch 32 : 117.890032\n",
      "Epoch: 33\n",
      "Loss on training set at epoch 33 : 117.745805\n",
      "Epoch: 34\n",
      "Loss on training set at epoch 34 : 117.942567\n",
      "Epoch: 35\n",
      "Loss on training set at epoch 35 : 117.216730\n",
      "Epoch: 36\n",
      "Loss on training set at epoch 36 : 117.132385\n",
      "Epoch: 37\n",
      "Loss on training set at epoch 37 : 117.467923\n",
      "Epoch: 38\n",
      "Loss on training set at epoch 38 : 116.973060\n",
      "Epoch: 39\n",
      "Loss on training set at epoch 39 : 117.133682\n",
      "Epoch: 40\n",
      "Loss on training set at epoch 40 : 116.857451\n",
      "Epoch: 41\n",
      "Loss on training set at epoch 41 : 116.423459\n",
      "Epoch: 42\n",
      "Loss on training set at epoch 42 : 116.851057\n",
      "Epoch: 43\n",
      "Loss on training set at epoch 43 : 116.560359\n",
      "Epoch: 44\n",
      "Loss on training set at epoch 44 : 116.703846\n",
      "Epoch: 45\n",
      "Loss on training set at epoch 45 : 116.294090\n",
      "Epoch: 46\n",
      "Loss on training set at epoch 46 : 115.949218\n",
      "Epoch: 47\n",
      "Loss on training set at epoch 47 : 116.107512\n",
      "Epoch: 48\n",
      "Loss on training set at epoch 48 : 115.857892\n",
      "Epoch: 49\n",
      "Loss on training set at epoch 49 : 115.566771\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHHCAYAAABZbpmkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAABVsUlEQVR4nO3deVxU9f4/8NcZBoZtBhgQBpRNUXEl3MirlQY3Qa/mds0yr622uJTWrbyVW7/Srn1tMW9mt7JFs+WqlaWGuaa4i1tqoogbyCbbAMPAfH5/EJMToIAzc2bG1/PxOA+dc84c3nMgefU5n0USQggQERERuSiF3AUQERER2RLDDhEREbk0hh0iIiJyaQw7RERE5NIYdoiIiMilMewQERGRS2PYISIiIpfGsENEREQujWGHiIiIXBrDDpHMHnjgAURFRbXovbNnz4YkSdYtqIlupG66uQwYMABdu3aVuwy6iTHsEDVCkqQmbVu2bJG7VLrJDRgwoNGfz9jYWLnLI5KdUu4CiBzVZ599ZvH6008/RWpqar39nTp1uqGv88EHH8BkMrXovS+99BJeeOGFG/r65BratGmDefPm1dvv5+cnQzVEjoVhh6gR999/v8XrXbt2ITU1td7+PysvL4e3t3eTv467u3uL6gMApVIJpZL/Gbs6k8mEqqoqeHp6NnqOn5/fdX82iW5WfIxFdAPq+iLs378ft99+O7y9vfGvf/0LAPDtt99iyJAhCAsLg0qlQrt27fDKK6+gpqbG4hp/7vty9uxZSJKEN954A0uXLkW7du2gUqnQu3dv7N271+K9DfXZkSQJkydPxpo1a9C1a1eoVCp06dIF69evr1f/li1b0KtXL3h6eqJdu3Z4//33b6gfkF6vxzPPPIPw8HCoVCp07NgRb7zxBoQQFuelpqaif//+8Pf3h6+vLzp27Gi+b3UWLVqELl26wNvbGwEBAejVqxdWrFhhcc7Fixfx0EMPISQkxPw5P/roo3p1NeVaDcnNzcXDDz+MkJAQeHp6Ii4uDp988on5uNFohFarxYMPPljvvSUlJfD09MSzzz5r3mcwGDBr1izExMRApVIhPDwczz33HAwGg8V7676Hy5cvR5cuXaBSqRr8/jVX3ff2xIkTGDNmDDQaDQIDA/HUU0+hsrLS4tzq6mq88sor5p+/qKgo/Otf/6pXKwCsW7cOd9xxB9RqNTQaDXr37t3g/f31118xcOBAeHt7o3Xr1vj3v/9d75yWfq+IroX/S0h0gwoKCpCSkoKxY8fi/vvvR0hICABg2bJl8PX1xfTp0+Hr64tNmzZh5syZKCkpwYIFC6573RUrVqC0tBSPPfYYJEnCv//9b4wcORJnzpy5bmvQL7/8glWrVuHJJ5+EWq3GO++8g1GjRuHcuXMIDAwEABw8eBDJyckIDQ3FnDlzUFNTg7lz56JVq1Ytug9CCAwbNgybN2/Gww8/jFtuuQUbNmzAP//5T1y8eBFvvvkmAODYsWP429/+hu7du2Pu3LlQqVTIyMjAjh07zNf64IMPMHXqVIwePdr8i/jw4cPYvXs37rvvPgDA5cuXceutt5qDQatWrbBu3To8/PDDKCkpwdNPP93kazWkoqICAwYMQEZGBiZPnozo6Gh8/fXXeOCBB1BUVISnnnoK7u7uGDFiBFatWoX3338fHh4e5vevWbMGBoMBY8eOBVDbOjNs2DD88ssvmDhxIjp16oQjR47gzTffxG+//YY1a9ZYfP1Nmzbhq6++wuTJkxEUFHTdzuA1NTXIz8+vt9/Lyws+Pj4W+8aMGYOoqCjMmzcPu3btwjvvvIMrV67g008/NZ/zyCOP4JNPPsHo0aPxzDPPYPfu3Zg3bx6OHz+O1atXm89btmwZHnroIXTp0gUzZsyAv78/Dh48iPXr11vc3ytXriA5ORkjR47EmDFj8M033+D5559Ht27dkJKSckPfK6LrEkTUJJMmTRJ//k/mjjvuEADEkiVL6p1fXl5eb99jjz0mvL29RWVlpXnfhAkTRGRkpPl1ZmamACACAwNFYWGhef+3334rAIjvv//evG/WrFn1agIgPDw8REZGhnnfoUOHBACxaNEi876hQ4cKb29vcfHiRfO+U6dOCaVSWe+aDflz3WvWrBEAxP/7f//P4rzRo0cLSZLM9bz55psCgMjLy2v02nfffbfo0qXLNb/+ww8/LEJDQ0V+fr7F/rFjxwo/Pz/z/W/KtRry1ltvCQDi888/N++rqqoSffv2Fb6+vqKkpEQIIcSGDRvqfV+EEGLw4MGibdu25tefffaZUCgUYvv27RbnLVmyRAAQO3bsMO8DIBQKhTh27FiTaq37OWxoe+yxx8zn1f28DBs2zOL9Tz75pAAgDh06JIQQIj09XQAQjzzyiMV5zz77rAAgNm3aJIQQoqioSKjVapGQkCAqKioszjWZTPXq+/TTT837DAaD0Ol0YtSoUeZ9Lf1eEV0PH2MR3SCVStXgYwwvLy/z30tLS5Gfn4/bbrsN5eXlOHHixHWve8899yAgIMD8+rbbbgMAnDlz5rrvTUpKQrt27cyvu3fvDo1GY35vTU0NNm7ciOHDhyMsLMx8XkxMjPn/spvrxx9/hJubG6ZOnWqx/5lnnoEQAuvWrQMA+Pv7A6h9zNdYx2x/f39cuHCh3mO7OkII/O9//8PQoUMhhEB+fr55GzRoEIqLi3HgwIEmXetan0en0+Hee+8173N3d8fUqVNRVlaGrVu3AgDuvPNOBAUF4csvvzSfd+XKFaSmpuKee+4x7/v666/RqVMnxMbGWtR75513AgA2b95s8fXvuOMOdO7cucn1RkVFITU1td5W18J1tUmTJlm8njJlivkzX/3n9OnTLc575plnAAA//PADgNrHkaWlpXjhhRfq9Sf686NQX19fiz5FHh4e6NOnj8XPc0u/V0TXw7BDdINat25t8fiizrFjxzBixAj4+flBo9GgVatW5n/si4uLr3vdiIgIi9d1wefKlSvNfm/d++vem5ubi4qKCsTExNQ7r6F9TZGVlYWwsDCo1WqL/XWj1bKysgDUhrh+/frhkUceQUhICMaOHYuvvvrKIvg8//zz8PX1RZ8+fdC+fXtMmjTJ4jFXXl4eioqKsHTpUrRq1cpiqwueubm5TbrWtT5P+/btoVBY/jP558+jVCoxatQofPvtt+b+LKtWrYLRaLQIO6dOncKxY8fq1duhQweLeutER0dft8ar+fj4ICkpqd7W0NDz9u3bW7xu164dFAoFzp49a/5sCoWi3s+CTqeDv7+/+bOfPn0aAJo0h06bNm3qBaCrfyaBln+viK6HfXaIbtDVLTh1ioqKcMcdd0Cj0WDu3Llo164dPD09ceDAATz//PNNGmru5ubW4H7xp86+1n6vrXl5eWHbtm3YvHkzfvjhB6xfvx5ffvkl7rzzTvz0009wc3NDp06dcPLkSaxduxbr16/H//73P/znP//BzJkzMWfOHPP9u//++zFhwoQGv0737t0B4LrXsoaxY8fi/fffx7p16zB8+HB89dVXiI2NRVxcnPkck8mEbt26YeHChQ1eIzw83OJ1Qz9XttJYh3RrTljZlJ9Je3yv6ObEsENkA1u2bEFBQQFWrVqF22+/3bw/MzNTxqr+EBwcDE9PT2RkZNQ71tC+poiMjMTGjRtRWlpq0bpT98guMjLSvE+hUCAxMRGJiYlYuHAhXnvtNbz44ovYvHkzkpKSANS2VNxzzz245557UFVVhZEjR+LVV1/FjBkz0KpVK6jVatTU1JjPv5ZrXaux4dyRkZE4fPgwTCaTRetOQ5/n9ttvR2hoKL788kv0798fmzZtwosvvmhxvXbt2uHQoUNITEyUbdbrOqdOnbJoOcrIyIDJZDJ3go6MjITJZMKpU6cs5pG6fPkyioqKzJ+97lHp0aNHW9wi+Gct+V4RXQ8fYxHZQN3/xV79f61VVVX4z3/+I1dJFtzc3JCUlIQ1a9bg0qVL5v0ZGRnmvjXNNXjwYNTU1ODdd9+12P/mm29CkiRzX6DCwsJ6773lllsAwPwYqKCgwOK4h4cHOnfuDCEEjEYj3NzcMGrUKPzvf//D0aNH610vLy/P/PfrXetanycnJ8eiL051dTUWLVoEX19f3HHHHeb9CoUCo0ePxvfff4/PPvsM1dXVFo+wgNoRUBcvXsQHH3xQ72tVVFRAr9c3Wou1LV682OL1okWLAMD8PRo8eDAA4K233rI4r65VasiQIQCAu+66C2q1GvPmzas3dL0lrYgt/V4RXQ9bdohs4C9/+QsCAgIwYcIETJ06FZIk4bPPPnOIx0h1Zs+ejZ9++gn9+vXDE088YQ4qXbt2RXp6erOvN3ToUAwcOBAvvvgizp49i7i4OPz000/49ttv8fTTT5tbAebOnYtt27ZhyJAhiIyMRG5uLv7zn/+gTZs26N+/P4DaX6I6nQ79+vVDSEgIjh8/jnfffRdDhgwxtxrNnz8fmzdvRkJCAh599FF07twZhYWFOHDgADZu3GgOVU25VkMmTpyI999/Hw888AD279+PqKgofPPNN9ixYwfeeuuteu+95557sGjRIsyaNQvdunWrN7P2+PHj8dVXX+Hxxx/H5s2b0a9fP9TU1ODEiRP46quvsGHDBvTq1avZ971OcXExPv/88waP/XmywczMTAwbNgzJyclIS0vD559/jvvuu8/82C0uLg4TJkzA0qVLzY9k9+zZg08++QTDhw/HwIEDAQAajQZvvvkmHnnkEfTu3Rv33XcfAgICcOjQIZSXl1vMSdQULf1eEV2XPIPAiJxPY0PPGxsqu2PHDnHrrbcKLy8vERYWJp577jnzMOXNmzebz2ts6PmCBQvqXROAmDVrlvl1Y0PPJ02aVO+9kZGRYsKECRb7fv75ZxEfHy88PDxEu3btxH//+1/xzDPPCE9Pz0buwh/+XLcQQpSWlopp06aJsLAw4e7uLtq3by8WLFhgMQz5559/FnfffbcICwsTHh4eIiwsTNx7773it99+M5/z/vvvi9tvv10EBgYKlUol2rVrJ/75z3+K4uJii693+fJlMWnSJBEeHi7c3d2FTqcTiYmJYunSpc2+VkMuX74sHnzwQREUFCQ8PDxEt27dxMcff9zguSaTSYSHhzc4/L5OVVWVeP3110WXLl2ESqUSAQEBomfPnmLOnDkW9TT2PWzMtYaeX/3zUffz8uuvv4rRo0cLtVotAgICxOTJk+sNHTcajWLOnDkiOjpauLu7i/DwcDFjxgyLaRPqfPfdd+Ivf/mL8PLyEhqNRvTp00d88cUXFvU19N/Jn3+GbuR7RXQtkhAO9L+aRCS74cOH49ixYzh16pTcpZCVzZ49G3PmzEFeXh6CgoLkLofIbthnh+gmVlFRYfH61KlT+PHHHzFgwAB5CiIisgH22SG6ibVt2xYPPPAA2rZti6ysLLz33nvw8PDAc889J3dpRERWw7BDdBNLTk7GF198gZycHKhUKvTt2xevvfZavUnniIicGfvsEBERkUtjnx0iIiJyaQw7RERE5NLYZwe1a9ZcunQJarVa9mnciYiIqGmEECgtLUVYWFi9RXuvxrAD4NKlS/UW4SMiIiLncP78ebRp06bR4ww7gHka8vPnz0Oj0chcDRERETVFSUkJwsPDr7ucCMMOYH50pdFoGHaIiIiczPW6oLCDMhEREbk0hh0iIiJyaQw7RERE5NIYdoiIiMilMewQERGRS2PYISIiIpfGsENEREQujWGHiIiIXJqsYWfbtm0YOnQowsLCIEkS1qxZY3H8gQcegCRJFltycrLFOVFRUfXOmT9/vh0/BRERETkyWWdQ1uv1iIuLw0MPPYSRI0c2eE5ycjI+/vhj82uVSlXvnLlz5+LRRx81v77etNFERER085A17KSkpCAlJeWa56hUKuh0umueo1arr3sOERER3Zwcvs/Oli1bEBwcjI4dO+KJJ55AQUFBvXPmz5+PwMBAxMfHY8GCBaiurr7mNQ0GA0pKSiw2IiIick0OvRBocnIyRo4ciejoaJw+fRr/+te/kJKSgrS0NLi5uQEApk6dih49ekCr1WLnzp2YMWMGsrOzsXDhwkavO2/ePMyZM8fm9ReXG1FcYUSAjzvUnu42/3pERERUnySEEHIXAdSuWLp69WoMHz680XPOnDmDdu3aYePGjUhMTGzwnI8++giPPfYYysrKGuzfA9S27BgMBvPruiXii4uLrbrq+diladh1phDv3BuPYXFhVrsuERER1f7+9vPzu+7vb4d/jHW1tm3bIigoCBkZGY2ek5CQgOrqapw9e7bRc1QqFTQajcVmC76q2tacssprP1YjIiIi23GqsHPhwgUUFBQgNDS00XPS09OhUCgQHBxsx8oapvasfUpYZjDKXAkREdHNS9Y+O2VlZRatNJmZmUhPT4dWq4VWq8WcOXMwatQo6HQ6nD59Gs899xxiYmIwaNAgAEBaWhp2796NgQMHQq1WIy0tDdOmTcP999+PgIAAuT6Wma/q97DDlh0iIiLZyBp29u3bh4EDB5pfT58+HQAwYcIEvPfeezh8+DA++eQTFBUVISwsDHfddRdeeeUVc18clUqFlStXYvbs2TAYDIiOjsa0adPM15Gb7+8tO6UGhh0iIiK5yBp2BgwYgGv1j96wYcM139+jRw/s2rXL2mVZTV3Ljp5hh4iISDZO1WfH2fzRZ4dhh4iISC4MOzZU17JTyj47REREsmHYsSFzB2W27BAREcmGYceGOBqLiIhIfgw7NuTLPjtERESyY9ixIbbsEBERyY9hx4bMLTtV1TCZHGIJMiIiopsOw44NqX9fG0sIoNxYI3M1RERENyeGHRvydFfATSEB4KMsIiIiuTDs2JAkSVcNP+dioERERHJg2LGxP8IOH2MRERHJgWHHxsxLRvAxFhERkSwYdmyMj7GIiIjkxbBjY3XDz7k+FhERkTwYdmzMh+tjERERyYphx8bUnEWZiIhIVgw7NsaVz4mIiOTFsGNj5j47DDtERESyYNixMS4GSkREJC+GHRszz7PDlh0iIiJZMOzYmO/vi4GyZYeIiEgeDDs25suWHSIiIlkx7NgYR2MRERHJi2HHxthnh4iISF4MOzbG0VhERETyYtixsbrlIqpqTDBU18hcDRER0c2HYcfG6lp2ALbuEBERyYFhx8bcFBK8PdwAsN8OERGRHBh27KCudaeULTtERER2x7BjB5xrh4iISD4MO3ag5ogsIiIi2TDs2EFdy46+imGHiIjI3mQNO9u2bcPQoUMRFhYGSZKwZs0ai+MPPPAAJEmy2JKTky3OKSwsxLhx46DRaODv74+HH34YZWVldvwU18c+O0RERPKRNezo9XrExcVh8eLFjZ6TnJyM7Oxs8/bFF19YHB83bhyOHTuG1NRUrF27Ftu2bcPEiRNtXXqzmBcDZZ8dIiIiu1Ne/xTbSUlJQUpKyjXPUalU0Ol0DR47fvw41q9fj71796JXr14AgEWLFmHw4MF44403EBYWZvWaW8K8ZARbdoiIiOzO4fvsbNmyBcHBwejYsSOeeOIJFBQUmI+lpaXB39/fHHQAICkpCQqFArt37270mgaDASUlJRabLXExUCIiIvk4dNhJTk7Gp59+ip9//hmvv/46tm7dipSUFNTU1C67kJOTg+DgYIv3KJVKaLVa5OTkNHrdefPmwc/Pz7yFh4fb9HP4sM8OERGRbGR9jHU9Y8eONf+9W7du6N69O9q1a4ctW7YgMTGxxdedMWMGpk+fbn5dUlJi08Dzxzw7Rpt9DSIiImqYQ7fs/Fnbtm0RFBSEjIwMAIBOp0Nubq7FOdXV1SgsLGy0nw9Q2w9Io9FYbLak5mMsIiIi2ThV2Llw4QIKCgoQGhoKAOjbty+Kioqwf/9+8zmbNm2CyWRCQkKCXGXW48tJBYmIiGQj62OssrIycysNAGRmZiI9PR1arRZarRZz5szBqFGjoNPpcPr0aTz33HOIiYnBoEGDAACdOnVCcnIyHn30USxZsgRGoxGTJ0/G2LFjHWYkFvDHY6xStuwQERHZnawtO/v27UN8fDzi4+MBANOnT0d8fDxmzpwJNzc3HD58GMOGDUOHDh3w8MMPo2fPnti+fTtUKpX5GsuXL0dsbCwSExMxePBg9O/fH0uXLpXrIzWILTtERETykbVlZ8CAARBCNHp8w4YN172GVqvFihUrrFmW1dXNs6Nnyw4REZHdOVWfHWdV17Kjr6pBjanxcEdERETWx7BjB3V9dgAuBkpERGRvDDt2oFK6wcOt9laz3w4REZF9MezYyR8TCzLsEBER2RPDjp34qNwAcMkIIiIie2PYsRNflTsAtuwQERHZG8OOnag51w4REZEsGHbshIuBEhERyYNhx07q5tphnx0iIiL7YtixE47GIiIikgfDjp3U9dnhkhFERET2xbBjJ+bFQBl2iIiI7Iphx07qHmOxzw4REZF9MezYCVt2iIiI5MGwYye+nGeHiIhIFgw7dsLRWERERPJg2LETzrNDREQkD4YdO1GzZYeIiEgWDDt2cvVCoEIImashIiK6eTDs2Eldn50ak0Cl0SRzNURERDcPhh078XZ3gyTV/r2Ui4ESERHZDcOOnSgUEnw96paMqJG5GiIiopsHw44dmYefc0QWERGR3TDs2JF5+DkfYxEREdkNw44dsWWHiIjI/hh27IjrYxEREdkfw44dMewQERHZH8OOHXHJCCIiIvtj2LEjLgZKRERkfww7dqRWsYMyERGRvTHs2BFbdoiIiOyPYceOrl4MlIiIiOxD1rCzbds2DB06FGFhYZAkCWvWrGn03McffxySJOGtt96y2B8VFQVJkiy2+fPn27bwFuI8O0RERPYna9jR6/WIi4vD4sWLr3ne6tWrsWvXLoSFhTV4fO7cucjOzjZvU6ZMsUW5N0zNoedERER2p5Tzi6ekpCAlJeWa51y8eBFTpkzBhg0bMGTIkAbPUavV0Ol0tijRqthnh4iIyP4cus+OyWTC+PHj8c9//hNdunRp9Lz58+cjMDAQ8fHxWLBgAaqrrx0mDAYDSkpKLDZ74Dw7RERE9idry871vP7661AqlZg6dWqj50ydOhU9evSAVqvFzp07MWPGDGRnZ2PhwoWNvmfevHmYM2eOLUq+pj9mUOZCoERERPbisGFn//79ePvtt3HgwAFIktToedOnTzf/vXv37vDw8MBjjz2GefPmQaVSNfieGTNmWLyvpKQE4eHh1iu+EXVhp9JogrHGBHc3h25YIyIicgkO+9t2+/btyM3NRUREBJRKJZRKJbKysvDMM88gKiqq0fclJCSguroaZ8+ebfQclUoFjUZjsdmDj+qPbKlnvx0iIiK7cNiWnfHjxyMpKcli36BBgzB+/Hg8+OCDjb4vPT0dCoUCwcHBti6x2TyUCqiUChiqTSitrIa/t4fcJREREbk8WcNOWVkZMjIyzK8zMzORnp4OrVaLiIgIBAYGWpzv7u4OnU6Hjh07AgDS0tKwe/duDBw4EGq1GmlpaZg2bRruv/9+BAQE2PWzNJXaUwlDWRVHZBEREdmJrGFn3759GDhwoPl1XT+aCRMmYNmyZdd9v0qlwsqVKzF79mwYDAZER0dj2rRpFv1xHI2vSol8hh0iIiK7kTXsDBgwAEKIJp//5344PXr0wK5du6xclW1xrh0iIiL7ctgOyq7KlyufExER2RXDjp1xMVAiIiL7YtixMzUXAyUiIrIrhh07My8ZwZYdIiIiu2DYsTMf9tkhIiKyK4YdOzM/xuL6WERERHbBsGNnfywGypYdIiIie2DYsTNznx0+xiIiIrILhh0746SCRERE9sWwY2dqdlAmIiKyK4YdO6tr2dGzZYeIiMguGHbsjPPsEBER2RfDjp1d3WenOYugEhERUcsw7NiZ+ve1sYQAyqtqZK6GiIjI9THs2JmnuwJuCgkAR2QRERHZA8OOnUmSBB8PNwCca4eIiMgeGHZkoPasfZTFlh0iIiLbY9iRgS/n2iEiIrIbhh0Z+HIxUCIiIrth2JEB18ciIiKyH4YdGXB9LCIiIvth2JFB3fpYXDKCiIjI9hh2ZMAlI4iIiOyHYUcG5sdY7LNDRERkcww7MjAPPWfLDhERkc0x7MhAzZYdIiIiu2HYkYEP++wQERHZDcOODDiDMhERkf0w7MhAzXl2iIiI7IZhRwa+Ki4ESkREZC8MOzLg0HMiIiL7kTXsbNu2DUOHDkVYWBgkScKaNWsaPffxxx+HJEl46623LPYXFhZi3Lhx0Gg08Pf3x8MPP4yysjLbFn6D6vrsVNWYYKiukbkaIiIi1yZr2NHr9YiLi8PixYuved7q1auxa9cuhIWF1Ts2btw4HDt2DKmpqVi7di22bduGiRMn2qpkq6gLOwCgNzDsEBER2ZLy+qfYTkpKClJSUq55zsWLFzFlyhRs2LABQ4YMsTh2/PhxrF+/Hnv37kWvXr0AAIsWLcLgwYPxxhtvNBiOHIGbQoK3hxvKq2pQVlkNrY+H3CURERG5LIfus2MymTB+/Hj885//RJcuXeodT0tLg7+/vznoAEBSUhIUCgV2795tz1Kb7Y/1sYwyV0JEROTaZG3ZuZ7XX38dSqUSU6dObfB4Tk4OgoODLfYplUpotVrk5OQ0el2DwQCDwWB+XVJSYp2Cm8HXU4ncUgM7KRMREdmYw7bs7N+/H2+//TaWLVsGSZKseu158+bBz8/PvIWHh1v1+k2h5vpYREREduGwYWf79u3Izc1FREQElEollEolsrKy8MwzzyAqKgoAoNPpkJuba/G+6upqFBYWQqfTNXrtGTNmoLi42LydP3/elh+lQT4MO0RERHbhsI+xxo8fj6SkJIt9gwYNwvjx4/Hggw8CAPr27YuioiLs378fPXv2BABs2rQJJpMJCQkJjV5bpVJBpVLZrvgmMPfZ4WMsIiIim5I17JSVlSEjI8P8OjMzE+np6dBqtYiIiEBgYKDF+e7u7tDpdOjYsSMAoFOnTkhOTsajjz6KJUuWwGg0YvLkyRg7dqzDjsSq48slI4iIiOxC1sdY+/btQ3x8POLj4wEA06dPR3x8PGbOnNnkayxfvhyxsbFITEzE4MGD0b9/fyxdutRWJVuNmouBEhER2YWsLTsDBgyAEKLJ5589e7bePq1WixUrVlixKvtgyw4REZF9OGwHZVfHxUCJiIjsg2FHJlwMlIiIyD4YdmTCeXaIiIjsg2FHJn8sF8GwQ0REZEsMOzL54zEW18YiIiKyJYYdmfjyMRYREZFdMOzIxJfz7BAREdkFw45M6h5j6atqUGNq+lxDRERE1DwMOzKpa9kBAH0VW3eIiIhshWFHJiqlAu5uEgA+yiIiIrIlhh2ZSJLETspERER2wLAjI66PRUREZHsMOzIyr4/Fx1hEREQ2w7AjIy4ZQUREZHsMOzLiYqBERES2x7AjI66PRUREZHsMOzJiyw4REZHtMezI6I+h51wMlIiIyFYYdmTEeXaIiIhsj2FHRuY+O3yMRUREZDMMOzLipIJERES2x7AjI/M8O2zZISIishmGHRmxZYeIiMj2WhR2zp8/jwsXLphf79mzB08//TSWLl1qtcJuBuygTEREZHstCjv33XcfNm/eDADIycnBX//6V+zZswcvvvgi5s6da9UCXZmaLTtEREQ216Kwc/ToUfTp0wcA8NVXX6Fr167YuXMnli9fjmXLllmzPpd29UKgQgiZqyEiInJNLQo7RqMRKpUKALBx40YMGzYMABAbG4vs7GzrVefi6vrsVJsEDNUmmashIiJyTS0KO126dMGSJUuwfft2pKamIjk5GQBw6dIlBAYGWrVAV+bt7gZJqv0759ohIiKyjRaFnddffx3vv/8+BgwYgHvvvRdxcXEAgO+++878eIuuT6GQ4OPBfjtERES2pGzJmwYMGID8/HyUlJQgICDAvH/ixInw9va2WnE3A1+VEmWGas61Q0REZCMtatmpqKiAwWAwB52srCy89dZbOHnyJIKDg61aoKur67dTysVAiYiIbKJFYefuu+/Gp59+CgAoKipCQkIC/u///g/Dhw/He++9Z9UCXZ0vZ1EmIiKyqRaFnQMHDuC2224DAHzzzTcICQlBVlYWPv30U7zzzjtNvs62bdswdOhQhIWFQZIkrFmzxuL47NmzERsbCx8fHwQEBCApKQm7d++2OCcqKgqSJFls8+fPb8nHkkXdXDv6KoYdIiIiW2hR2CkvL4darQYA/PTTTxg5ciQUCgVuvfVWZGVlNfk6er0ecXFxWLx4cYPHO3TogHfffRdHjhzBL7/8gqioKNx1113Iy8uzOG/u3LnIzs42b1OmTGnJx5IFW3aIiIhsq0UdlGNiYrBmzRqMGDECGzZswLRp0wAAubm50Gg0Tb5OSkoKUlJSGj1+3333WbxeuHAhPvzwQxw+fBiJiYnm/Wq1GjqdrpmfwjHUhZ1SjsYiIiKyiRa17MycORPPPvssoqKi0KdPH/Tt2xdAbStPfHy8VQusU1VVhaVLl8LPz8881L3O/PnzERgYiPj4eCxYsADV1dcODgaDASUlJRabXMyLgbJlh4iIyCZa1LIzevRo9O/fH9nZ2RbBIzExESNGjLBacQCwdu1ajB07FuXl5QgNDUVqaiqCgoLMx6dOnYoePXpAq9Vi586dmDFjBrKzs7Fw4cJGrzlv3jzMmTPHqnW2VIC3BwAgr9QgcyVERESuSRI3uChT3ernbdq0ubFCJAmrV6/G8OHDLfbr9XpkZ2cjPz8fH3zwATZt2oTdu3c3OsT9o48+wmOPPYaysjLzkhZ/ZjAYYDD8ES5KSkoQHh6O4uLiZj2Gs4a1hy9h8oqDiI/wx+on+9n1axMRETmzkpIS+Pn5Xff3d4seY5lMJsydOxd+fn6IjIxEZGQk/P398corr8Bksu4aTz4+PoiJicGtt96KDz/8EEqlEh9++GGj5yckJKC6uhpnz55t9ByVSgWNRmOxySUm2BcAkJFbxsVAiYiIbKBFj7FefPFFfPjhh5g/fz769attjfjll18we/ZsVFZW4tVXX7VqkVczmUwWrTJ/lp6eDoVC4TSTG0YH+UAh1a6NlVtqQIjGU+6SiIiIXEqLws4nn3yC//73v+bVzgGge/fuaN26NZ588skmh52ysjJkZGSYX2dmZiI9PR1arRaBgYF49dVXMWzYMISGhiI/Px+LFy/GxYsX8fe//x0AkJaWht27d2PgwIFQq9VIS0vDtGnTcP/991ssY+HIVEo3RAb6IDNfj4zcMoYdIiIiK2tR2CksLERsbGy9/bGxsSgsLGzydfbt24eBAweaX0+fPh0AMGHCBCxZsgQnTpzAJ598gvz8fAQGBqJ3797Yvn07unTpAqD2cdTKlSsxe/ZsGAwGREdHY9q0aebrOIuYYF9k5utx6nIp+sUEXf8NRERE1GQtCjtxcXF49913682W/O6776J79+5Nvs6AAQOu2U9l1apV13x/jx49sGvXriZ/PUcVE+yL1F8vIyOvTO5SiIiIXE6Lws6///1vDBkyBBs3bjTPsZOWlobz58/jxx9/tGqBN4P2v3dSPnWZYYeIiMjaWjQa64477sBvv/2GESNGoKioCEVFRRg5ciSOHTuGzz77zNo1urz2wbVLb5xmyw4REZHV3fA8O1c7dOgQevTogZqaGmtd0i6aOk7fVsqrqtF55gYAwMGX/4oAHw+710BERORsbDrPDlmXt4cSrf29AID9doiIiKyMYcdBxLDfDhERkU0w7DiI9lfNpExERETW06zRWCNHjrzm8aKiohup5aZmbtnJLZW5EiIiItfSrLDj5+d33eP/+Mc/bqigm1X7kNqwc5otO0RERFbVrLDz8ccf26qOm15Mq9rh55eKK1FaaYTa013mioiIiFwD++w4CD9vd7RSqwAAp/P0MldDRETkOhh2HEhMK3ZSJiIisjaGHQdS12+HnZSJiIish2HHgdSNyGInZSIiIuth2HEgfww/Z9ghIiKyFoYdB1IXds4XlqPS6FzrixERETkqhh0H0spXBT8vd5gEcIYjsoiIiKyCYceBSJJkbt3hgqBERETWwbDjYMxrZF3miCwiIiJrYNhxMGzZISIisi6GHQdjHpF1mWGHiIjIGhh2HExd2DlboIexxiRzNURERM6PYcfBhPl5wdvDDcYagayCcrnLISIicnoMOw5GoZDQjmtkERERWQ3DjgMyj8jiGllEREQ3jGHHAbULZssOERGRtTDsOKD2XCOLiIjIahh2HJB59fO8MphMQuZqiIiInBvDjgOK0HrDw02BSqMJF4sq5C6HiIjIqTHsOCClmwJtW/kAYL8dIiKiG8Ww46DamfvtcEQWERHRjWDYcVDtOSKLiIjIKhh2HFQMR2QRERFZhaxhZ9u2bRg6dCjCwsIgSRLWrFljcXz27NmIjY2Fj48PAgICkJSUhN27d1ucU1hYiHHjxkGj0cDf3x8PP/wwysqcPyC0D1YDqG3ZEYIjsoiIiFpK1rCj1+sRFxeHxYsXN3i8Q4cOePfdd3HkyBH88ssviIqKwl133YW8vDzzOePGjcOxY8eQmpqKtWvXYtu2bZg4caK9PoLNRAV5QyEBpZXVyC01yF0OERGR05KEgzQbSJKE1atXY/jw4Y2eU1JSAj8/P2zcuBGJiYk4fvw4OnfujL1796JXr14AgPXr12Pw4MG4cOECwsLCmvS1665bXFwMjUZjjY9jFXe+sQVn8vVY/kgC+sUEyV0OERGRQ2nq72+n6bNTVVWFpUuXws/PD3FxcQCAtLQ0+Pv7m4MOACQlJUGhUNR73HU1g8GAkpISi80RmUdkXeaILCIiopZy+LCzdu1a+Pr6wtPTE2+++SZSU1MRFFTbypGTk4Pg4GCL85VKJbRaLXJychq95rx58+Dn52fewsPDbfoZWso8IivP+fsgERERycXhw87AgQORnp6OnTt3Ijk5GWPGjEFubu4NXXPGjBkoLi42b+fPn7dStdZlHpF1mWGHiIiopRw+7Pj4+CAmJga33norPvzwQyiVSnz44YcAAJ1OVy/4VFdXo7CwEDqdrtFrqlQqaDQai80R1Y3IOs2WHSIiohZz+LDzZyaTCQZD7eikvn37oqioCPv37zcf37RpE0wmExISEuQq0WraBdcuGZFfVoUr+iqZqyEiInJOSjm/eFlZGTIyMsyvMzMzkZ6eDq1Wi8DAQLz66qsYNmwYQkNDkZ+fj8WLF+PixYv4+9//DgDo1KkTkpOT8eijj2LJkiUwGo2YPHkyxo4d2+SRWI7M20OJ1v5euFhUgYy8MvT20cpdEhERkdORtWVn3759iI+PR3x8PABg+vTpiI+Px8yZM+Hm5oYTJ05g1KhR6NChA4YOHYqCggJs374dXbp0MV9j+fLliI2NRWJiIgYPHoz+/ftj6dKlcn0kq2O/HSIiohsja8vOgAEDrjk78KpVq657Da1WixUrVlizLIfSPtgXW3/L4xpZRERELeR0fXZuNjFc/ZyIiOiGMOw4uPYhtWHnNFt2iIiIWoRhx8HFtKodfn6puBJlhmqZqyEiInI+DDsOzs/bHa3UKgBs3SEiImoJhh0nENOqrt8Oww4REVFzMew4gbp+OxyRRURE1HwMO06gbkTWyRzHXJ2diIjIkTHsOIEeEQEAgB0ZBcgvM8hcDRERkXNh2HECXVv74ZZwf1TVmPDF7nNyl0NERORUGHacxAN/iQIAfL47C8Yak7zFEBERORGGHScxuFsognxVuFxiwPqjOXKXQ0RE5DQYdpyEh1KB+xIiAACf7DwrbzFEREROhGHHidyfEAGlQsK+rCs4erFY7nKIiIicAsOOEwnWeGJwt1AAbN0hIiJqKoYdJzPh947K3x66hEJ9lbzFEBEROQGGHSfTI8If3Vr7oarahJV7OQydiIjoehh2nIwkSebWnc/TslDNYehERETXxLDjhP7WPRRaHw9cKq5E6q+X5S6HiIjIoTHsOCFPdzfc2yccALCMHZWJiIiuiWHHSd1/ayTcFBJ2ZxbieDYXCCUiImoMw46TCvXzQnIXHQDg07Sz8hZDRETkwBh2nFhdR+XVBy+iqJzD0ImIiBrCsOPEekcFoFOoBpVGE77ce17ucoiIiBwSw44TkyQJD/wlEgDw2a4s1JiEzBURERE5HoYdJ3f3La3h7+2OC1cq8PNxDkMnIiL6M4YdJ+fp7oZ7etcOQ/+EHZWJiIjqYdhxAeNvjYRCAnZkFODU5VK5yyEiInIoDDsuoE2AN5I6hQDgJINERER/xrDjIh7oFwUA+Hr/Bfx6iZMMEhER1WHYcRF92wZiYMdWqKo2YdKKAyitNMpdEhERkUNg2HERkiRh4ZhbEObnicx8PV5YdQRCcCg6ERERw44LCfDxwKL7ekCpkPDD4Wx8vitL7pKIiIhkJ2vY2bZtG4YOHYqwsDBIkoQ1a9aYjxmNRjz//PPo1q0bfHx8EBYWhn/84x+4dOmSxTWioqIgSZLFNn/+fDt/EsfRMzIAzyfHAgBeWXscRy4Uy1wRERGRvGQNO3q9HnFxcVi8eHG9Y+Xl5Thw4ABefvllHDhwAKtWrcLJkycxbNiweufOnTsX2dnZ5m3KlCn2KN9hPXJbNP7aOQRVNbX9d4or2H+HiIhuXko5v3hKSgpSUlIaPObn54fU1FSLfe+++y769OmDc+fOISIiwrxfrVZDp9PZtFZnIkkS3hgdhyGLtuNcYTme++YQltzfE5IkyV0aERGR3TlVn53i4mJIkgR/f3+L/fPnz0dgYCDi4+OxYMECVFdXX/M6BoMBJSUlFpur8fN2x+L7esDdTcKGY5fx8Y6zcpdEREQkC6cJO5WVlXj++edx7733QqPRmPdPnToVK1euxObNm/HYY4/htddew3PPPXfNa82bNw9+fn7mLTw83NblyyIu3B8vDu4EAJi37jjSzxfJWxAREZEMJOEg45MlScLq1asxfPjweseMRiNGjRqFCxcuYMuWLRZh588++ugjPPbYYygrK4NKpWrwHIPBAIPBYH5dUlKC8PBwFBcXX/PazkgIgUkrDuDHIzlo7e+FH6b2h7+3h9xlERER3bCSkhL4+fld9/e3w7fsGI1GjBkzBllZWUhNTb1uGElISEB1dTXOnj3b6DkqlQoajcZic1WSJGH+qO6IDPTGxaIKPPv1Ic6/Q0RENxWHDjt1QefUqVPYuHEjAgMDr/ue9PR0KBQKBAcH26FC56DxrO2/46FUYOPxXHyw/YzcJREREdmNrKOxysrKkJGRYX6dmZmJ9PR0aLVahIaGYvTo0Thw4ADWrl2Lmpoa5OTkAAC0Wi08PDyQlpaG3bt3Y+DAgVCr1UhLS8O0adNw//33IyAgQK6P5ZC6tvbDzL91xktrjmL+uhNQSBIe7h/NEVpEROTyZO2zs2XLFgwcOLDe/gkTJmD27NmIjo5u8H2bN2/GgAEDcODAATz55JM4ceIEDAYDoqOjMX78eEyfPr3R/joNaeozP2cnhMBLa45i+e5zAIBhcWGYP6obvD1kzbxEREQt0tTf3w7TQVlON0vYAWoDzyc7z+L//XAc1SaBWJ0a74/vichAH7lLIyIiahaX6aBM1iVJEh7oF40Vj96KIF8VTuSUYuiiX7DlZK7cpREREdkEw85Nqk+0Fmun9Ed8hD9KKqvx4LK9WLw5gyO1iIjI5TDs3MR0fp5YOfFW3JcQASGABRtO4vHP96O0kmtpERGR62DYucmplG54bUQ3vD6qGzzcFNhw7DKGL96BjNwyuUsjIiKyCoYdAgDc0zsCXz3eF6F+njidp8fwxTvww+FsucsiIiK6YQw7ZHZLuD++n9IfCdFalBmqMWnFAcz89igqjTVyl0ZERNRiDDtkIchXheWPJODJAe0AAJ+mZWH0kp3IKtDLXBkREVHLMOxQPUo3BZ5LjsWyB3sjwNsdRy+W4G/v/MLHWkRE5JQYdqhRAzoG48enbkPvqACU8rEWERE5KYYduqZQPy988eiteIKPtYiIyEkx7NB1Kd0UeD45Fh/zsRYRETkhhh1qsoG/P9bqFfnHY62pXxxEdnGF3KURERE1imGHmiXUzwtfTLwVj9/RDpIEfHfoEu58YysW/XyKfXmIiMghMexQs7m7KfBCSiy+m9QfvSIDUGGswf+l/oakhVux7kg219ciIiKHIgn+ZmryEvFUnxAC3x26hHk/nkBOSSUAoG/bQMwa1hmxOt5LIiKynab+/mbYAcOONZRXVWPJltNYsu0MqqpNUEjAuIRITP9rBwT4eMhdHhERuSCGnWZg2LGe84XleO3H41h3NAcA4Ofljr/3bIOEtoHoHRUAf28GHyIisg6GnWZg2LG+nafzMff7X3Eip9Rif8cQNXpHB6BPdCD6RGmh8/OUqUIiInJ2DDvNwLBjG9U1Jqw/loMdGQXYk1mA03n1JyKM0HqjT7QWfdsGYkDHVgj0VclQKREROSOGnWZg2LGP/DID9p0txJ7MK9hztgC/XiqB6aqfPkmqXXk9MTYYA2OD0TlUA0mS5CuYiIgcGsNOMzDsyKO00oj9WVewJ7MQW3/Lw7FLJRbHdRpPDIwNRmJsMP4SEwhvD6VMlRIRkSNi2GkGhh3HkF1cgc0n8rDpRC52ZOSj4qpJCj2UCvSPCcLIHq2R1CkEnu5uMlZKRESOgGGnGRh2HE+lsQa7zhRg04lc/Hw8FxeL/liSws/LHcPiwjC6Zxt0b+PHR11ERDcphp1mYNhxbEII/Ha5DN8duohVBy4iu7jSfKx9sC9G92yDEfGtEazhyC4iopsJw04zMOw4jxqTwM7T+fhm/wWsP5oDQ7UJAKCQgDs6tMLfe4VjUBcd3BRs7SEicnUMO83AsOOcSiqN+OFwNr7ZfwH7s66Y93cK1eDlIZ3wl5ggGasjIiJbY9hpBoYd53cmrwzf7L+Az3ZlobSyGgCQ1CkY/xrcCW1b+cpcHRER2QLDTjMw7LiOQn0V3t74Gz7ffQ41JgGlQsL4vpF4KrE9l6ogInIxDDvNwLDjejJyy/Daj8ex6UQugNoRXE8ltsf4vpFwd1PIXB0REVkDw04zMOy4ru2n8vDqD8fNa3S1DfLBjMGdkBgbDAU7MRMROTWGnWZg2HFtNSaBL/eex8LUk8gvqwIAaDyViAv3R3xEAOIj/HFLG38E+NzYYy5jjQkXr1Qgq7Ac5wr0OFtQjqyCclQYq9HKV4UQjSdaqWv/DNF4Ivj3v3t5cIJEIqKWYNhpBoadm0NppRHvbTmNZTvPoryqpt7x6CAf3BLuXxt+wv3h7aFEpbEG5VU1qDDWoKKqGhV1r3/fLpdWIuv3UHOxqAI1pub/56T2VCI6yAcv/60zekdprfFRiYhuCk4RdrZt24YFCxZg//79yM7OxurVqzF8+HAAgNFoxEsvvYQff/wRZ86cgZ+fH5KSkjB//nyEhYWZr1FYWIgpU6bg+++/h0KhwKhRo/D222/D17fpI3AYdm4uxhoTTuaU4uD5IqSfK8LB81dwpoEV2VtCpVQgMtAbEVofRAZ6IzLQG74qJfJKDcgtNeBySSVySwy4XFqJyyWVqDSazO/1cFPgtZHdMLpnG6vUQkTk6pr6+1vWlRX1ej3i4uLw0EMPYeTIkRbHysvLceDAAbz88suIi4vDlStX8NRTT2HYsGHYt2+f+bxx48YhOzsbqampMBqNePDBBzFx4kSsWLHC3h+HnIS7mwJdW/uha2s/jL81EgBQXG5E+oUiHDx3BQfPFeHoxWJUmwS83N3g7eEGz9//9PJwg5d77Z/eHm7Q+nggMtAHkVpvRAb6IFitanJfICEESg3VuFxcif/76TesP5aDZ78+hIzcMjw3qCP7FBERWYnDPMaSJMmiZache/fuRZ8+fZCVlYWIiAgcP34cnTt3xt69e9GrVy8AwPr16zF48GBcuHDBogXoWtiyQ3IzmQQWpv6GdzdnAADu6hyCN++5BT4qrvRORNSYpv7+dqoxuMXFxZAkCf7+/gCAtLQ0+Pv7m4MOACQlJUGhUGD37t2NXsdgMKCkpMRiI5KTQiHh2UEd8eY9cfBwU+CnXy9j9JI0XLpqAVQiImoZpwk7lZWVeP7553Hvvfea01tOTg6Cg4MtzlMqldBqtcjJyWn0WvPmzYOfn595Cw8Pt2ntRE01Ir4Nvph4K4J8PXA8uwTD3t2Bg+euXP+NRETUKKcIO0ajEWPGjIEQAu+9994NX2/GjBkoLi42b+fPn7dClUTW0TMyAGsm9UOsTo38MgPuWboL3x26JHdZREROy+HDTl3QycrKQmpqqsUzOZ1Oh9zcXIvzq6urUVhYCJ1O1+g1VSoVNBqNxUbkSNoEeOObJ/6CxNhgVFWbMPWLg1iY+hsM1fWHzBMR0bU5dO/HuqBz6tQpbN68GYGBgRbH+/bti6KiIuzfvx89e/YEAGzatAkmkwkJCQlylExkNb4qJZb+oxdeX38CS7edwTs/n8K7m04hzN8LUYG1Q9ujg3wQGeiDqEBvhGu94en+xwSFhuoaFFcYUVxuRFGFEUXlRhRXGFFUXgVDtQlCCAgBCABCACYhIFD7QgDw9/ZAUqdgRAb6yHQHiIisQ9bRWGVlZcjIqB19Eh8fj4ULF2LgwIHQarUIDQ3F6NGjceDAAaxduxYhISHm92m1Wnh41M52m5KSgsuXL2PJkiXmoee9evVq1tBzjsYiR/fV3vN4bd1xFJUbGz1HkoBQjScEgKJyIyqM1mkF6hSqQXIXHZK76tAhxBeSxCHxROQYnGJSwS1btmDgwIH19k+YMAGzZ89GdHR0g+/bvHkzBgwYAKB2UsHJkydbTCr4zjvvcFJBcjlCCOSVGZBVUI6z+XqcNS9JocfZ/HKUGarrvUchARovd/h7ucPP26P2Ty93eLm7QZJqAxIg1f4d+P3P2ten88qw60yhxazQbYN8MKirDslddOjexs8qwafSWIOM3DJEBHpD4+ne4uvkFFdixqrDOJ2nx8y/dUZS55Drv4mInJpThB1HwbBDzk4IgUJ9FbIKy6FUSPD38oCftzvUKuUNTU54RV+FjccvY/3RHGw/lY+qmj9mfA7z88RfO4egS2s/tA/2RUywL9RNCCvlVdU4kFWEPZkF2J1ZiIPni1BVbUKAtzteGtIZI3u0bnaIWn80By+sOmzR8vX3nm0wc2jnJtVERM6JYacZGHaIrq+00ojNJ/Ow4WgONp/MbXB9sVA/T7QPUaN9sG/tFuKLMH8vHM8uwe7MQuzJLMSRC7WzU19NpVTAUF0bpPrHBOHVEV2b1FeovKoar6z9FV/sqR1R2bW1Br0itfgk7SyEAFr7e2HB37vjL+2CrHAHiMjRMOw0A8MOUfNUGmuw7bc87DxdgFO5pTh1uQy5pYYmvz/MzxMJbQOREK1Fn2gtwrXe+GD7Gby98RQM1SZ4uivwdFIHPNI/Gkq3hgeNHrlQjKdWHsSZfD0kCZh4e1s889eO8FAqsPtMAZ795hDOF9ZOyvhgvyg8nxxr0YGbiJwfw04zMOwQ3bjiciMy8mqDz6nc37fLpcgurkR0kA/6RGmR0LY23LQJ8G7wGpn5ery4+gh2ni4AAHQJ02D+yO7o1sbPfI7JJLB0+xn8308nYawR0Gk8sXBMHP4SY9l6U2aoxqs/HMcXe84BANq28sHCMbfglnB/29wAIrI7hp1mYNghsh1jjQnujbTONEQIga/3X8CrPxxHcYURCgl4uH80pv21A0oqqjH9q3RzGEruosO8kd0Q4OPR6PU2n8jF8/87jNxSA9wUEiYNaIfJd7aHh9Lhpxkjoutg2GkGhh0ix5NXasDctb/i+99nj24T4IUyQzWKyo3wcnfD7GGdMaZXeJM6MxeVV2Hmt8fMM1F3CPFF19Z+8FUp4atSwkelhNpTCR8PJXw9a/epPZXQ+Xmila+Kw+2JHBTDTjMw7BA5rs0ncvHSmqO4+PuiqN3b+OGte25B21ZNn16iztrDl/DSmqPXnK/ozzyUCrTx90LrAC+0CfBCmwDv3//0Qmt/b4RoGIaI5MKw0wwMO0SOTW+oxgfbz8BDqcAj/dve0COovFIDNh6/jOIKI/SGapQZqlFWWQ19VTVKK6vN+0oqqpFbWgnTdf6FVKuU6BSqQeew37dQDdqH+EKlZGdoIltj2GkGhh0iaoixxoSc4kpcuFKBC1fKf/+zAheLav+eXVxpMeliHaVCQkywrzn8tGvli1ZqFYLVKmh9PBodYUZEzcOw0wwMO0TUEsYaE07nleHXSyX49VIJjl0qwa/ZJSiuaPwxmUICtD4qc/hp9fsW6ueJ6CAfRAf5IMzP64YmgyS6WTDsNAPDDhFZixACl4orrwpAxbhwpQJ5ZQYUlBmu+1gMqJ1kMTrIB21b1YaftkG+iG7lg3atfOHndeMzQgshbqifUVV1bcg7nl0CdzcFUrrq2FpFsmDYaQaGHSKyhxpT7bIeuaWVyCs1IK/UgNzf/7xYVIHMfD2yCvQw1jT8z7IkAT0jAjC4WygGdwuFzs+zyV+7vKoaW07m4ccj2dhyMg8KCYgI9EaE1hsRWp/f//RGZKA3Qv08zeGloMyA49mlOJ5dguM5JTieXYqM3FKLGuPa+OH/xsQhJlh9YzeIqJkYdpqBYYeIHEV1jQkXiypwJk+PM/l6nMkrQ2a+Hmfy9MgpqbQ4t2dkXfDRIdTPq961SiuN2HQiF+uO5GDLb7moNJrqndMQN4WE1v5eqDTWNDoztlqlRGyoGidzSlFSWQ0PpQLP3tUBD/dvCzc+giM7YdhpBoYdInIGl4oqsP5oDn48ko19WVcsjvWI8MfgbqG4o0MrHLpQjHVHsust3hqh9UZKVx0GddXBV6XEuYJynCusv1VVW4aiqEBvxOo06BSqQadQNTqFatAmwAuSJCGnuBIvrDqMLSfzANQGsDf+HofooOuvbUZ0oxh2moFhh4icTU5xJdYdzTYHn8b+JW/bygeDu4YiuasOXcI01+2rYzIJ5JYakFWgh9JNgVidGj4q5TXfI4TAV/vO45W1x1FmqIanuwLPJ8diQt8oq3W0Li43YldmAcIDvNE5jP9OUy2GnWZg2CEiZ3a5pBLrjmTjx6M52Hu2EB1D1EjpGoqUbjq0D/a126SHF66U4/n/HcaOjNrlPG5tq8WC0XEI1za8Ftr1XCqqQOqvl/HTrznYdabQPMy/f0wQHrujLfrHBHFCx5scw04zMOwQkaswmYSsw9ZNJoHlu7Pw2o8nUGGsgY+HG55LjsWtbQMR4OOOAG+PRtdKE0Lg5OVS/HSsNuAcvVhicTwq0Bvnr1SYQ0/nUA0eu6MthnQL5WiwmxTDTjMw7BARWVdWgR7//Pow9pwtrHdM46mE1scDAT4e0HrX/unupsCOjHycKyw3nydJQK/IANzVWYe/dg5BVJAPzheW48NfMvHl3vOoMNYAqF037ZH+0RjTOxzeHg0/cjPWmHC+sByZ+Xpk5uthqDahZ2QA4iP8Odu1E2PYaQaGHSIi6zOZBJbtPIuVe88hv6wKV8qrGu1bVMdDqcBtMUG4q0sIEjuFIMhX1eB5V/RV+GxXFpbtPItCfRUAwN/bHf/oG4WEaC3OFuiRmVcbbM7k63GusLzB2a493RXoFalF33aB+Eu7QHRr7cdWIifCsNMMDDtERLZXYxIoqTCiQF8bfAr1Vbiir0JheRVKK6vRvbUfbu/Q6rodoq9WaazB1/sv4INtZyxahRri5e5WO0t1Kx9IAHadKUR+meXQel+VEgnRdeEnCJ1C1TfcL6iiqgZKN6nRx3fUcgw7zcCwQ0Tk3GpMAuuP5mDZzkzkl1WZl96onYHaB21b+dZboV4IgYzcMuw8XYCdp/Ox60xhvaU+YnVqjEuIwN3xraHxbPrs1TUmge2n8vDFnnPYeDwXKqUCt7YNRP+YINzWPggxduw47soYdpqBYYeIiGpMAsezS5D2e/jZeboAht/nHPJyd8Pdt4ThvoQIdG/j3+g1coor8fW+81i59zwuFlU0ep5O44n+7WuDT7+YIIvHdSaTwOXSSvM8SOd/n/8oq7AcQgB/6x6K4fGtG33EdzNh2GkGhh0iIvqz4nIjVh28gOW7zyEjt8y8v2trDcYlRGJYXBh8VErUmAS2/paLFbvPY/PJXHPfII2nEiN7tMHYPuGoMQn8ciof20/lY8/ZwnoTN3YK1UCnUdWGmysV9Y7/mVIh4c7YYIzpFY4BHVvdtP2MGHaagWGHiIgaI4TA3rNXsHx3FtYdyTHPSu2rUuLO2GDsO1uIS8V/LOXRJ0qLexPCkdI1FJ7u9Ud6VRprsPdsIX45lY9tp/JxPLuk3jl1S3ZEaL2vWsPMG4X6Kny9/wIOnS8yn9tKrcLI+Nb4e682N936ZAw7zcCwQ0RETVGor8L/9l/Aij3nkJmvN+/393bHqB5tcG+f8GYHjrxSA3aezofeUNPgYqwN+e1yKb7edx6rDlxEwe+j0QAgPsIff+8ZjpSuOgT4eDT/AzoZhp1mYNghIqLmEEIg7XQBtvyWhy5hGgzqomuwFcfWjDUmbD6Ri6/2XbB4hOamkHBrWy2Su+hwVxcdQjSedq/NHhh2moFhh4iInF1uaSXWHLyI1Qcv1Xs01iPCH8lddUjuEoqIwPrLd9SYBC4VVZgnXczM1yOrQA+tjwq9owLQKyoA7Vo53ggyhp1mYNghIiJXklWgx4ZjOVh/NAcHzhVZHOscqkFSp2AYakzmiRezGljt/s/8vd3RKzIAPSO16B0VgK6t/WRpzboaw04zMOwQEZGrulxSiZ+O5WD9McsFVf/Mw02ByEBvRP0+N1FEoDeyiyqx92whDl0oQqXRVO/87m380Cdai/4xQegZFWD3pTcYdpqBYYeIiG4GV/RV2Hj8MnaeLoCfl7vF5Ith/l5wa2QR2apqE45dKsb+rCvYd/YK9mUVIr+syuIcL3c39InW4rb2QbitfSt0CLH9Yy+GnWZg2CEiImo6IQSyCsqx92wh0k4XYHtGPvJKLZfeCFarLCZODFZbv5M0w04zMOwQERG1nBACJy+XmucO2pNZUO+x18IxcRjZo41Vv25Tf3/LOuXitm3bMHToUISFhUGSJKxZs8bi+KpVq3DXXXchMDAQkiQhPT293jUGDBgASZIstscff9w+H4CIiIggSRJidRo8cltbfPpQH6TPvAvLH0nA43e0Q9fWtSEkLtxftvqavrSsDej1esTFxeGhhx7CyJEjGzzev39/jBkzBo8++mij13n00Ucxd+5c82tv7/rD6oiIiMg+PN3d0C+m9vEVEItCfRUCvJu+kKq1yRp2UlJSkJKS0ujx8ePHAwDOnj17zet4e3tDp9NZszQiIiKyEq3Mszm7xMphy5cvR1BQELp27YoZM2agvLxc7pKIiIjIQcjasmMN9913HyIjIxEWFobDhw/j+eefx8mTJ7Fq1apG32MwGGAw/NFrvKSk/iJsRERE5BqcPuxMnDjR/Pdu3bohNDQUiYmJOH36NNq1a9fge+bNm4c5c+bYq0QiIiKSkUs8xrpaQkICACAjI6PRc2bMmIHi4mLzdv78eXuVR0RERHbm9C07f1Y3PD00NLTRc1QqFVQqlZ0qIiIiIjnJGnbKysosWmAyMzORnp4OrVaLiIgIFBYW4ty5c7h06RIA4OTJkwAAnU4HnU6H06dPY8WKFRg8eDACAwNx+PBhTJs2Dbfffju6d+8uy2ciIiIixyLrDMpbtmzBwIED6+2fMGECli1bhmXLluHBBx+sd3zWrFmYPXs2zp8/j/vvvx9Hjx6FXq9HeHg4RowYgZdeeqlZMyFzBmUiIiLnw+UimoFhh4iIyPk4xXIRRERERLbGsENEREQujWGHiIiIXBrDDhEREbk0hh0iIiJyaS43qWBL1A1I4xpZREREzqPu9/b1BpYz7AAoLS0FAISHh8tcCRERETVXaWkp/Pz8Gj3OeXYAmEwmXLp0CWq1GpIkWe26JSUlCA8Px/nz5zl/jx3wftsX77d98X7bF++3fbX0fgshUFpairCwMCgUjffMYcsOAIVCgTZt2tjs+hqNhv+x2BHvt33xftsX77d98X7bV0vu97VadOqwgzIRERG5NIYdIiIicmkMOzakUqkwa9YsqFQquUu5KfB+2xfvt33xftsX77d92fp+s4MyERERuTS27BAREZFLY9ghIiIil8awQ0RERC6NYYeIiIhcGsOODS1evBhRUVHw9PREQkIC9uzZI3dJLmHbtm0YOnQowsLCIEkS1qxZY3FcCIGZM2ciNDQUXl5eSEpKwqlTp+Qp1gXMmzcPvXv3hlqtRnBwMIYPH46TJ09anFNZWYlJkyYhMDAQvr6+GDVqFC5fvixTxc7tvffeQ/fu3c2Tq/Xt2xfr1q0zH+e9tp358+dDkiQ8/fTT5n2839Y1e/ZsSJJkscXGxpqP2+p+M+zYyJdffonp06dj1qxZOHDgAOLi4jBo0CDk5ubKXZrT0+v1iIuLw+LFixs8/u9//xvvvPMOlixZgt27d8PHxweDBg1CZWWlnSt1DVu3bsWkSZOwa9cupKamwmg04q677oJerzefM23aNHz//ff4+uuvsXXrVly6dAkjR46UsWrn1aZNG8yfPx/79+/Hvn37cOedd+Luu+/GsWPHAPBe28revXvx/vvvo3v37hb7eb+tr0uXLsjOzjZvv/zyi/mYze63IJvo06ePmDRpkvl1TU2NCAsLE/PmzZOxKtcDQKxevdr82mQyCZ1OJxYsWGDeV1RUJFQqlfjiiy9kqND15ObmCgBi69atQoja++vu7i6+/vpr8znHjx8XAERaWppcZbqUgIAA8d///pf32kZKS0tF+/btRWpqqrjjjjvEU089JYTgz7YtzJo1S8TFxTV4zJb3my07NlBVVYX9+/cjKSnJvE+hUCApKQlpaWkyVub6MjMzkZOTY3Hv/fz8kJCQwHtvJcXFxQAArVYLANi/fz+MRqPFPY+NjUVERATv+Q2qqanBypUrodfr0bdvX95rG5k0aRKGDBlicV8B/mzbyqlTpxAWFoa2bdti3LhxOHfuHADb3m8uBGoD+fn5qKmpQUhIiMX+kJAQnDhxQqaqbg45OTkA0OC9rztGLWcymfD000+jX79+6Nq1K4Dae+7h4QF/f3+Lc3nPW+7IkSPo27cvKisr4evri9WrV6Nz585IT0/nvbaylStX4sCBA9i7d2+9Y/zZtr6EhAQsW7YMHTt2RHZ2NubMmYPbbrsNR48eten9ZtghoiabNGkSjh49avGMnayvY8eOSE9PR3FxMb755htMmDABW7dulbssl3P+/Hk89dRTSE1Nhaenp9zl3BRSUlLMf+/evTsSEhIQGRmJr776Cl5eXjb7unyMZQNBQUFwc3Or14P88uXL0Ol0MlV1c6i7v7z31jd58mSsXbsWmzdvRps2bcz7dTodqqqqUFRUZHE+73nLeXh4ICYmBj179sS8efMQFxeHt99+m/fayvbv34/c3Fz06NEDSqUSSqUSW7duxTvvvAOlUomQkBDebxvz9/dHhw4dkJGRYdOfb4YdG/Dw8EDPnj3x888/m/eZTCb8/PPP6Nu3r4yVub7o6GjodDqLe19SUoLdu3fz3reQEAKTJ0/G6tWrsWnTJkRHR1sc79mzJ9zd3S3u+cmTJ3Hu3DnecysxmUwwGAy811aWmJiII0eOID093bz16tUL48aNM/+d99u2ysrKcPr0aYSGhtr25/uGujdTo1auXClUKpVYtmyZ+PXXX8XEiROFv7+/yMnJkbs0p1daWioOHjwoDh48KACIhQsXioMHD4qsrCwhhBDz588X/v7+4ttvvxWHDx8Wd999t4iOjhYVFRUyV+6cnnjiCeHn5ye2bNkisrOzzVt5ebn5nMcff1xERESITZs2iX379om+ffuKvn37yli183rhhRfE1q1bRWZmpjh8+LB44YUXhCRJ4qeffhJC8F7b2tWjsYTg/ba2Z555RmzZskVkZmaKHTt2iKSkJBEUFCRyc3OFELa73ww7NrRo0SIREREhPDw8RJ8+fcSuXbvkLsklbN68WQCot02YMEEIUTv8/OWXXxYhISFCpVKJxMREcfLkSXmLdmIN3WsA4uOPPzafU1FRIZ588kkREBAgvL29xYgRI0R2drZ8RTuxhx56SERGRgoPDw/RqlUrkZiYaA46QvBe29qfww7vt3Xdc889IjQ0VHh4eIjWrVuLe+65R2RkZJiP2+p+S0IIcWNtQ0RERESOi312iIiIyKUx7BAREZFLY9ghIiIil8awQ0RERC6NYYeIiIhcGsMOERERuTSGHSIiInJpDDtERA2QJAlr1qyRuwwisgKGHSJyOA888AAkSaq3JScny10aETkhpdwFEBE1JDk5GR9//LHFPpVKJVM1ROTM2LJDRA5JpVJBp9NZbAEBAQBqHzG99957SElJgZeXF9q2bYtvvvnG4v1HjhzBnXfeCS8vLwQGBmLixIkoKyuzOOejjz5Cly5doFKpEBoaismTJ1scz8/Px4gRI+Dt7Y327dvju+++s+2HJiKbYNghIqf08ssvY9SoUTh06BDGjRuHsWPH4vjx4wAAvV6PQYMGISAgAHv37sXXX3+NjRs3WoSZ9957D5MmTcLEiRNx5MgRfPfdd4iJibH4GnPmzMGYMWNw+PBhDB48GOPGjUNhYaFdPycRWcENLyVKRGRlEyZMEG5ubsLHx8die/XVV4UQtSuxP/744xbvSUhIEE888YQQQoilS5eKgIAAUVZWZj7+ww8/CIVCIXJycoQQQoSFhYkXX3yx0RoAiJdeesn8uqysTAAQ69ats9rnJCL7YJ8dInJIAwcOxHvvvWexT6vVmv/et29fi2N9+/ZFeno6AOD48eOIi4uDj4+P+Xi/fv1gMplw8uRJSJKES5cuITEx8Zo1dO/e3fx3Hx8faDQa5ObmtvQjEZFMGHaIyCH5+PjUe6xkLV5eXk06z93d3eK1JEkwmUy2KImIbIh9dojIKe3atave606dOgEAOnXqhEOHDkGv15uP79ixAwqFAh07doRarUZUVBR+/vlnu9ZMRPJgyw4ROSSDwYCcnByLfUqlEkFBQQCAr7/+Gr169UL//v2xfPly7NmzBx9++CEAYNy4cZg1axYmTJiA2bNnIy8vD1OmTMH48eMREhICAJg9ezYef/xxBAcHIyUlBaWlpdixYwemTJli3w9KRDbHsENEDmn9+vUIDQ212NexY0ecOHECQO1IqZUrV+LJJ59EaGgovvjiC3Tu3BkA4O3tjQ0bNuCpp55C79694e3tjVGjRmHhwoXma02YMAGVlZV488038eyzzyIoKAijR4+23wckIruRhBBC7iKIiJpDkiSsXr0aw4cPl7sUInIC7LNDRERELo1hh4iIiFwa++wQkdPh03ciag627BAREZFLY9ghIiIil8awQ0RERC6NYYeIiIhcGsMOERERuTSGHSIiInJpDDtERETk0hh2iIiIyKUx7BAREZFL+/9OcGvUcdFYWAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training the retrofitted model:\n",
    "train_losses = []\n",
    "print('Training the retrofitted model...')\n",
    "\n",
    "for epoch in range(50):\n",
    "    print('Epoch:', epoch)\n",
    "    epoch_loss = 0\n",
    "    shuffle(retro_train)\n",
    "    i = 0\n",
    "    # Process the training data in batches of 32:\n",
    "    while i < len(retro_train):\n",
    "        batch = retro_train[i:i+32]\n",
    "        i += 32\n",
    "        # Get the sentence vectors and labels from the batch:\n",
    "        sentence, label = zip(*batch)\n",
    "        \n",
    "        # Step 1: Process the input data:\n",
    "        sentence = torch.Tensor(sentence) # [batch_size, 250]\n",
    "        label = torch.LongTensor(label) # [batch_size]\n",
    "        \n",
    "        # Step 2: Zero the gradients:\n",
    "        model_retrofitted.zero_grad()\n",
    "        \n",
    "        # Step 3: Run the forward propagation:\n",
    "        output = model_retrofitted(sentence)\n",
    "        \n",
    "        # Step 4: Compute loss and gradients:\n",
    "        loss = loss_function(output, label)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step 5: Update the weights:\n",
    "        optimizer_retrofitted.step()\n",
    "    # end of handling of this batch\n",
    "    print(\"Loss on training set at epoch %d : %f\" %(epoch, epoch_loss))\n",
    "    train_losses.append(epoch_loss)    \n",
    "        # Step 6: Early stopping:\n",
    "    with torch.no_grad():\n",
    "        if model_retrofitted.early_stop:\n",
    "            # forward propagation\n",
    "            # We convert the test examples to tensors and do the forward propagation\n",
    "            dev_sentence, dev_label = zip(*retro_dev)\n",
    "            dev_sentence = torch.tensor(dev_sentence, dtype=torch.float32) # [DEV_SIZE, CONTEXT_SIZE]\n",
    "            dev_label = torch.tensor(dev_label, dtype=torch.long)   # [DEV_SIZE]        \n",
    "            output = model_retrofitted(dev_sentence)    \n",
    "                \n",
    "            # total loss on the dev set\n",
    "            dev_loss = loss_function(output, dev_label)\n",
    "            print(\"Loss on dev set at epoch %d: %f\\n\" %(epoch, dev_loss))\n",
    "                \n",
    "            # prediction and accuracy on the dev set\n",
    "            pred_labels = torch.argmax(output, dim=1)\n",
    "            accuracy = torch.sum(pred_labels == dev_label).item() / len(dev_label)\n",
    "            print(\"Accuracy on dev, after epoch %d: %3.2f\\n\" % (epoch, accuracy * 100))\n",
    "                \n",
    "            # early stopping\n",
    "            # if first epoch: we record the dev loss, to be used for early stopping\n",
    "            if epoch == 0:\n",
    "                previous_dev_loss = dev_loss\n",
    "            elif dev_loss > previous_dev_loss:\n",
    "                print(\"Loss on dev has increased, we stop training!\")\n",
    "                break\n",
    "            else:\n",
    "                previous_dev_loss = dev_loss\n",
    "# Plot the training losses\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training losses over Epochs\")\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of the retrofitted model:\n",
      "Loss on test set after training: 0.518729\n",
      "\n",
      "Accuracy on test after training: 77.42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on test set after training:\n",
    "with torch.no_grad():\n",
    "    # forward propagation on test set\n",
    "    test_sentence, test_label = zip(*retro_test)\n",
    "    test_sentence = torch.tensor(test_sentence, dtype=torch.float32) # [DEV_SIZE, CONTEXT_SIZE]\n",
    "    test_label = torch.tensor(test_label, dtype=torch.long)   # [DEV_SIZE]\n",
    "    log_probs = model_retrofitted(test_sentence)\n",
    "    \n",
    "    # total loss on the test set\n",
    "    test_loss = loss_function(log_probs, test_label)\n",
    "            \n",
    "    # prediction and accuracy on the dev set\n",
    "    pred_labels = torch.argmax(log_probs, dim=1)\n",
    "    accuracy = torch.sum(pred_labels == test_label).item() / len(test_label)\n",
    "    \n",
    "    # Print the results:\n",
    "    print(\"Result of the retrofitted model:\")\n",
    "    print(\"Loss on test set after training: %f\\n\" %(test_loss))\n",
    "    print(\"Accuracy on test after training: %3.2f\\n\" % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
