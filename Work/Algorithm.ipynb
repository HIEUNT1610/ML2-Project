{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work on this notebook to implement the algorithm.\n",
    "Please check regularly for updates.\n",
    "\n",
    "Main tasks:\n",
    "1. Write the code for the algorithm\n",
    "2. Find better $\\alpha$ and $\\beta$ values for the model.\n",
    "3. Find stopping criteria for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries:\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "import math\n",
    "import re\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "import gzip\n",
    "from nltk.corpus import wordnet \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy corpus of 10 words:\n",
    "toy_corpus = ['frog', 'toad', 'extrinsic', 'cat', 'intrinsic', 'dog', 'mission', 'true', 'false', 'incorrect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Load the Google News word2vec model:\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the toy corpus:\n",
    "vec_toy_corpus = [wv[word] for word in toy_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between frog and frog is 1.0000\n",
      "Similarity between frog and toad is 0.7050\n",
      "Similarity between frog and extrinsic is 0.0780\n",
      "Similarity between frog and cat is 0.4789\n",
      "Similarity between frog and intrinsic is 0.0972\n",
      "Similarity between frog and dog is 0.3550\n",
      "Similarity between frog and mission is 0.0185\n",
      "Similarity between frog and true is 0.1128\n",
      "Similarity between frog and false is -0.0008\n",
      "Similarity between frog and incorrect is 0.0589\n",
      "Similarity between toad and frog is 0.7050\n",
      "Similarity between toad and toad is 1.0000\n",
      "Similarity between toad and extrinsic is 0.0638\n",
      "Similarity between toad and cat is 0.4725\n",
      "Similarity between toad and intrinsic is 0.0609\n",
      "Similarity between toad and dog is 0.3837\n",
      "Similarity between toad and mission is 0.0607\n",
      "Similarity between toad and true is 0.0903\n",
      "Similarity between toad and false is 0.0358\n",
      "Similarity between toad and incorrect is 0.0119\n",
      "Similarity between extrinsic and frog is 0.0780\n",
      "Similarity between extrinsic and toad is 0.0638\n",
      "Similarity between extrinsic and extrinsic is 1.0000\n",
      "Similarity between extrinsic and cat is -0.0275\n",
      "Similarity between extrinsic and intrinsic is 0.4950\n",
      "Similarity between extrinsic and dog is -0.0656\n",
      "Similarity between extrinsic and mission is -0.0576\n",
      "Similarity between extrinsic and true is 0.1345\n",
      "Similarity between extrinsic and false is 0.1512\n",
      "Similarity between extrinsic and incorrect is 0.1686\n",
      "Similarity between cat and frog is 0.4789\n",
      "Similarity between cat and toad is 0.4725\n",
      "Similarity between cat and extrinsic is -0.0275\n",
      "Similarity between cat and cat is 1.0000\n",
      "Similarity between cat and intrinsic is 0.0047\n",
      "Similarity between cat and dog is 0.7609\n",
      "Similarity between cat and mission is 0.0152\n",
      "Similarity between cat and true is 0.0769\n",
      "Similarity between cat and false is 0.0476\n",
      "Similarity between cat and incorrect is 0.0909\n",
      "Similarity between intrinsic and frog is 0.0972\n",
      "Similarity between intrinsic and toad is 0.0609\n",
      "Similarity between intrinsic and extrinsic is 0.4950\n",
      "Similarity between intrinsic and cat is 0.0047\n",
      "Similarity between intrinsic and intrinsic is 1.0000\n",
      "Similarity between intrinsic and dog is 0.0080\n",
      "Similarity between intrinsic and mission is 0.0624\n",
      "Similarity between intrinsic and true is 0.3059\n",
      "Similarity between intrinsic and false is 0.0939\n",
      "Similarity between intrinsic and incorrect is 0.1198\n",
      "Similarity between dog and frog is 0.3550\n",
      "Similarity between dog and toad is 0.3837\n",
      "Similarity between dog and extrinsic is -0.0656\n",
      "Similarity between dog and cat is 0.7609\n",
      "Similarity between dog and intrinsic is 0.0080\n",
      "Similarity between dog and dog is 1.0000\n",
      "Similarity between dog and mission is 0.0146\n",
      "Similarity between dog and true is 0.0663\n",
      "Similarity between dog and false is 0.0469\n",
      "Similarity between dog and incorrect is 0.0495\n",
      "Similarity between mission and frog is 0.0185\n",
      "Similarity between mission and toad is 0.0607\n",
      "Similarity between mission and extrinsic is -0.0576\n",
      "Similarity between mission and cat is 0.0152\n",
      "Similarity between mission and intrinsic is 0.0624\n",
      "Similarity between mission and dog is 0.0146\n",
      "Similarity between mission and mission is 1.0000\n",
      "Similarity between mission and true is 0.1568\n",
      "Similarity between mission and false is 0.0452\n",
      "Similarity between mission and incorrect is 0.0439\n",
      "Similarity between true and frog is 0.1128\n",
      "Similarity between true and toad is 0.0903\n",
      "Similarity between true and extrinsic is 0.1345\n",
      "Similarity between true and cat is 0.0769\n",
      "Similarity between true and intrinsic is 0.3059\n",
      "Similarity between true and dog is 0.0663\n",
      "Similarity between true and mission is 0.1568\n",
      "Similarity between true and true is 1.0000\n",
      "Similarity between true and false is 0.3709\n",
      "Similarity between true and incorrect is 0.2458\n",
      "Similarity between false and frog is -0.0008\n",
      "Similarity between false and toad is 0.0358\n",
      "Similarity between false and extrinsic is 0.1512\n",
      "Similarity between false and cat is 0.0476\n",
      "Similarity between false and intrinsic is 0.0939\n",
      "Similarity between false and dog is 0.0469\n",
      "Similarity between false and mission is 0.0452\n",
      "Similarity between false and true is 0.3709\n",
      "Similarity between false and false is 1.0000\n",
      "Similarity between false and incorrect is 0.5383\n",
      "Similarity between incorrect and frog is 0.0589\n",
      "Similarity between incorrect and toad is 0.0119\n",
      "Similarity between incorrect and extrinsic is 0.1686\n",
      "Similarity between incorrect and cat is 0.0909\n",
      "Similarity between incorrect and intrinsic is 0.1198\n",
      "Similarity between incorrect and dog is 0.0495\n",
      "Similarity between incorrect and mission is 0.0439\n",
      "Similarity between incorrect and true is 0.2458\n",
      "Similarity between incorrect and false is 0.5383\n",
      "Similarity between incorrect and incorrect is 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Compare vectors using cosine similarity:\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Compute the cosine similarity matrix:\n",
    "similarity_matrix = np.zeros((len(vec_toy_corpus), len(vec_toy_corpus)))\n",
    "for i in range(len(vec_toy_corpus)):\n",
    "    for j in range(len(vec_toy_corpus)):\n",
    "        similarity_matrix[i][j] = cosine_similarity(vec_toy_corpus[i], vec_toy_corpus[j])\n",
    "\n",
    "# Print the cosine similarity matrix:\n",
    "for i in range(len(vec_toy_corpus)):\n",
    "    for j in range(len(vec_toy_corpus)):\n",
    "        print(f'Similarity between {toy_corpus[i]} and {toy_corpus[j]} is {similarity_matrix[i][j]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We implement the retrofitting algorithm of Faruqui et al. (2015):\n",
    "# Preprocessing the data:\n",
    "isNumber = re.compile(r'\\d+.*')\n",
    "def norm_word(word):\n",
    "  if isNumber.search(word.lower()):\n",
    "    return '---num---'\n",
    "  elif re.sub(r'\\W+', '', word) == '':\n",
    "    return '---punc---'\n",
    "  else:\n",
    "    return word.lower()\n",
    "\n",
    "# Read all the word vectors and normalize them:\n",
    "def read_word_vecs(filename):\n",
    "  wordVectors = {}\n",
    "  if filename.endswith('.gz'): fileObject = gzip.open(filename, 'r')\n",
    "  else: fileObject = open(filename, 'r')\n",
    "  \n",
    "  for line in fileObject:\n",
    "    line = line.strip().lower()\n",
    "    word = line.split()[0]\n",
    "    wordVectors[word] = np.zeros(len(line.split())-1, dtype=float)\n",
    "    for index, vecVal in enumerate(line.split()[1:]):\n",
    "      wordVectors[word][index] = float(vecVal)\n",
    "    ''' normalize weight vector '''\n",
    "    wordVectors[word] /= math.sqrt((wordVectors[word]**2).sum() + 1e-6)\n",
    "    \n",
    "  sys.stderr.write(\"Vectors read from: \"+filename+\" \\n\")\n",
    "  return wordVectors\n",
    "\n",
    "# Read lexicon as a dictionary:\n",
    "def read_lexicon(filename):\n",
    "  lexicon = {}\n",
    "  if filename.endswith('.gz'): fileObject = gzip.open(filename, 'r')\n",
    "  else: fileObject = open(filename, 'r')\n",
    "  \n",
    "  for line in fileObject:\n",
    "    words = line.lower().strip().split()\n",
    "    lexicon[norm_word(words[0])] = [norm_word(word) for word in words[1:]]\n",
    "  return lexicon\n",
    "\n",
    "# Retrofit word vectors to a lexicon:\n",
    "def retrofit(wordVecs, lexicon, numIters): # wordVecs is a dictionary\n",
    "  newWordVecs = deepcopy(wordVecs)\n",
    "  wvVocab = set(newWordVecs.keys())\n",
    "  loopVocab = wvVocab.intersection(set(lexicon.keys()))\n",
    "  for it in range(numIters):\n",
    "    # loop through every node also in ontology (else just use data estimate)\n",
    "    for word in loopVocab:\n",
    "      wordNeighbours = set(lexicon[word]).intersection(wvVocab)\n",
    "      numNeighbours = len(wordNeighbours)\n",
    "      #no neighbours, pass - use data estimate\n",
    "      if numNeighbours == 0:\n",
    "        continue\n",
    "      # the weight of the data estimate if the number of neighbours\n",
    "      newVec = numNeighbours * wordVecs[word]\n",
    "      # loop over neighbours and add to new vector (currently with weight 1)\n",
    "      for ppWord in wordNeighbours:\n",
    "        newVec += newWordVecs[ppWord]\n",
    "      newWordVecs[word] = newVec/(2*numNeighbours)\n",
    "  return newWordVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['frog', 'toad', 'extrinsic', 'cat', 'intrinsic', 'dog', 'mission', 'true', 'false', 'incorrect'])\n"
     ]
    }
   ],
   "source": [
    "# Transform the toy corpus into a wordVecs dictionary:\n",
    "toy_wordVecs = {}\n",
    "for i in range(len(vec_toy_corpus)):\n",
    "    toy_wordVecs[toy_corpus[i]] = vec_toy_corpus[i]\n",
    "print(toy_wordVecs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot use a string pattern on a bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Read the lexicon. We can use NLTK WordNet.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m lexicon \u001b[39m=\u001b[39m read_lexicon(\u001b[39m'\u001b[39;49m\u001b[39mppdb-2.0-xl-lexical.gz\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m \u001b[39m#newWordVecs = retrofit(toy_wordVecs, lexicon, 10)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[49], line 38\u001b[0m, in \u001b[0;36mread_lexicon\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m fileObject:\n\u001b[0;32m     37\u001b[0m   words \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit()\n\u001b[1;32m---> 38\u001b[0m   lexicon[norm_word(words[\u001b[39m0\u001b[39m])] \u001b[39m=\u001b[39m [norm_word(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words[\u001b[39m1\u001b[39m:]]\n\u001b[0;32m     39\u001b[0m \u001b[39mreturn\u001b[39;00m lexicon\n",
      "Cell \u001b[1;32mIn[49], line 38\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m fileObject:\n\u001b[0;32m     37\u001b[0m   words \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit()\n\u001b[1;32m---> 38\u001b[0m   lexicon[norm_word(words[\u001b[39m0\u001b[39m])] \u001b[39m=\u001b[39m [norm_word(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words[\u001b[39m1\u001b[39m:]]\n\u001b[0;32m     39\u001b[0m \u001b[39mreturn\u001b[39;00m lexicon\n",
      "Cell \u001b[1;32mIn[49], line 5\u001b[0m, in \u001b[0;36mnorm_word\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnorm_word\u001b[39m(word):\n\u001b[1;32m----> 5\u001b[0m   \u001b[39mif\u001b[39;00m isNumber\u001b[39m.\u001b[39;49msearch(word\u001b[39m.\u001b[39;49mlower()):\n\u001b[0;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m---num---\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      7\u001b[0m   \u001b[39melif\u001b[39;00m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mW+\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, word) \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot use a string pattern on a bytes-like object"
     ]
    }
   ],
   "source": [
    "# Read the lexicon. We can use NLTK WordNet.\n",
    "lexicon = read_lexicon('ppdb-2.0-xl-lexical.gz') # I am not sure how to read the lexicon files.\n",
    "#newWordVecs = retrofit(toy_wordVecs, lexicon, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
